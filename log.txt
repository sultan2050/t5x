2023-10-07 02:00:47.284870: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-10-07 02:00:47.284942: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-10-07 02:00:47.284977: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-10-07 02:00:48.363233: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
I1007 02:00:51.207494 140317719510080 resource_reader.py:50] system_path_file_exists:t5_1_1_base.gin
E1007 02:00:51.207925 140317719510080 resource_reader.py:55] Path not found: t5_1_1_base.gin
I1007 02:01:04.800925 140317719510080 gin_utils.py:85] Gin Configuration:
I1007 02:01:04.805167 140317719510080 gin_utils.py:87] from __gin__ import dynamic_registration
I1007 02:01:04.805263 140317719510080 gin_utils.py:87] import __main__ as train_script
I1007 02:01:04.805304 140317719510080 gin_utils.py:87] import seqio
I1007 02:01:04.805337 140317719510080 gin_utils.py:87] import t5.data.mixtures
I1007 02:01:04.805372 140317719510080 gin_utils.py:87] from t5x import adafactor
I1007 02:01:04.805431 140317719510080 gin_utils.py:87] from t5x.examples.t5 import network
I1007 02:01:04.805464 140317719510080 gin_utils.py:87] from t5x import gin_utils
I1007 02:01:04.805492 140317719510080 gin_utils.py:87] from t5x import models
I1007 02:01:04.805518 140317719510080 gin_utils.py:87] from t5x import partitioning
I1007 02:01:04.805542 140317719510080 gin_utils.py:87] from t5x import trainer
I1007 02:01:04.805565 140317719510080 gin_utils.py:87] from t5x import utils
I1007 02:01:04.805588 140317719510080 gin_utils.py:87] import tasks
I1007 02:01:04.805610 140317719510080 gin_utils.py:87] 
I1007 02:01:04.805638 140317719510080 gin_utils.py:87] # Macros:
I1007 02:01:04.805660 140317719510080 gin_utils.py:87] # ==============================================================================
I1007 02:01:04.805683 140317719510080 gin_utils.py:87] BATCH_SIZE = 256
I1007 02:01:04.805706 140317719510080 gin_utils.py:87] DROPOUT_RATE = 0.0
I1007 02:01:04.805728 140317719510080 gin_utils.py:87] LABEL_SMOOTHING = 0.0
I1007 02:01:04.805750 140317719510080 gin_utils.py:87] LOSS_NORMALIZING_FACTOR = None
I1007 02:01:04.805772 140317719510080 gin_utils.py:87] MIXTURE_OR_TASK_MODULE = None
I1007 02:01:04.805794 140317719510080 gin_utils.py:87] MIXTURE_OR_TASK_NAME = 'arabic_dataset'
I1007 02:01:04.805815 140317719510080 gin_utils.py:87] MODEL = @models.EncoderDecoderModel()
I1007 02:01:04.805845 140317719510080 gin_utils.py:87] MODEL_DIR = 'gs://sultan-t5x/arabict5_tinyy/'
I1007 02:01:04.805869 140317719510080 gin_utils.py:87] OPTIMIZER = @adafactor.Adafactor()
I1007 02:01:04.805891 140317719510080 gin_utils.py:87] RANDOM_SEED = None
I1007 02:01:04.805912 140317719510080 gin_utils.py:87] SHUFFLE_TRAIN_EXAMPLES = True
I1007 02:01:04.805935 140317719510080 gin_utils.py:87] TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}
I1007 02:01:04.805957 140317719510080 gin_utils.py:87] TRAIN_STEPS = 100000
I1007 02:01:04.805979 140317719510080 gin_utils.py:87] USE_CACHED_TASKS = False
I1007 02:01:04.806001 140317719510080 gin_utils.py:87] USE_HARDWARE_RNG = False
I1007 02:01:04.806023 140317719510080 gin_utils.py:87] VOCABULARY = @seqio.SentencePieceVocabulary()
I1007 02:01:04.806046 140317719510080 gin_utils.py:87] Z_LOSS = 0.0001
I1007 02:01:04.806068 140317719510080 gin_utils.py:87] 
I1007 02:01:04.806090 140317719510080 gin_utils.py:87] # Parameters for adafactor.Adafactor:
I1007 02:01:04.806112 140317719510080 gin_utils.py:87] # ==============================================================================
I1007 02:01:04.806133 140317719510080 gin_utils.py:87] adafactor.Adafactor.decay_rate = 0.8
I1007 02:01:04.806155 140317719510080 gin_utils.py:87] adafactor.Adafactor.logical_factor_rules = \
I1007 02:01:04.806188 140317719510080 gin_utils.py:87]     @adafactor.standard_logical_factor_rules()
I1007 02:01:04.806210 140317719510080 gin_utils.py:87] adafactor.Adafactor.step_offset = 0
I1007 02:01:04.806232 140317719510080 gin_utils.py:87] 
I1007 02:01:04.806254 140317719510080 gin_utils.py:87] # Parameters for utils.CheckpointConfig:
I1007 02:01:04.806276 140317719510080 gin_utils.py:87] # ==============================================================================
I1007 02:01:04.806298 140317719510080 gin_utils.py:87] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I1007 02:01:04.806319 140317719510080 gin_utils.py:87] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I1007 02:01:04.806341 140317719510080 gin_utils.py:87] 
I1007 02:01:04.806363 140317719510080 gin_utils.py:87] # Parameters for utils.create_learning_rate_scheduler:
I1007 02:01:04.806385 140317719510080 gin_utils.py:87] # ==============================================================================
I1007 02:01:04.806406 140317719510080 gin_utils.py:87] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I1007 02:01:04.806428 140317719510080 gin_utils.py:87] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I1007 02:01:04.806450 140317719510080 gin_utils.py:87] utils.create_learning_rate_scheduler.warmup_steps = 10000
I1007 02:01:04.806472 140317719510080 gin_utils.py:87] 
I1007 02:01:04.806494 140317719510080 gin_utils.py:87] # Parameters for train/utils.DatasetConfig:
I1007 02:01:04.806515 140317719510080 gin_utils.py:87] # ==============================================================================
I1007 02:01:04.806537 140317719510080 gin_utils.py:87] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I1007 02:01:04.806560 140317719510080 gin_utils.py:87] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I1007 02:01:04.806583 140317719510080 gin_utils.py:87] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I1007 02:01:04.806605 140317719510080 gin_utils.py:87] train/utils.DatasetConfig.pack = True
I1007 02:01:04.806627 140317719510080 gin_utils.py:87] train/utils.DatasetConfig.seed = None
I1007 02:01:04.806649 140317719510080 gin_utils.py:87] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I1007 02:01:04.806671 140317719510080 gin_utils.py:87] train/utils.DatasetConfig.split = 'train'
I1007 02:01:04.806693 140317719510080 gin_utils.py:87] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I1007 02:01:04.806715 140317719510080 gin_utils.py:87] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I1007 02:01:04.806739 140317719510080 gin_utils.py:87] 
I1007 02:01:04.806761 140317719510080 gin_utils.py:87] # Parameters for train_eval/utils.DatasetConfig:
I1007 02:01:04.806783 140317719510080 gin_utils.py:87] # ==============================================================================
I1007 02:01:04.806862 140317719510080 gin_utils.py:87] train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE
I1007 02:01:04.806912 140317719510080 gin_utils.py:87] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I1007 02:01:04.806937 140317719510080 gin_utils.py:87] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I1007 02:01:04.806961 140317719510080 gin_utils.py:87] train_eval/utils.DatasetConfig.pack = True
I1007 02:01:04.806984 140317719510080 gin_utils.py:87] train_eval/utils.DatasetConfig.seed = 42
I1007 02:01:04.807008 140317719510080 gin_utils.py:87] train_eval/utils.DatasetConfig.shuffle = False
I1007 02:01:04.807032 140317719510080 gin_utils.py:87] train_eval/utils.DatasetConfig.split = 'validation'
I1007 02:01:04.807056 140317719510080 gin_utils.py:87] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I1007 02:01:04.807079 140317719510080 gin_utils.py:87] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I1007 02:01:04.807103 140317719510080 gin_utils.py:87] 
I1007 02:01:04.807133 140317719510080 gin_utils.py:87] # Parameters for models.EncoderDecoderModel:
I1007 02:01:04.807157 140317719510080 gin_utils.py:87] # ==============================================================================
I1007 02:01:04.807181 140317719510080 gin_utils.py:87] models.EncoderDecoderModel.input_vocabulary = %VOCABULARY
I1007 02:01:04.807204 140317719510080 gin_utils.py:87] models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING
I1007 02:01:04.807228 140317719510080 gin_utils.py:87] models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I1007 02:01:04.807251 140317719510080 gin_utils.py:87] models.EncoderDecoderModel.module = @network.Transformer()
I1007 02:01:04.807274 140317719510080 gin_utils.py:87] models.EncoderDecoderModel.optimizer_def = %OPTIMIZER
I1007 02:01:04.807297 140317719510080 gin_utils.py:87] models.EncoderDecoderModel.output_vocabulary = %VOCABULARY
I1007 02:01:04.807321 140317719510080 gin_utils.py:87] models.EncoderDecoderModel.z_loss = %Z_LOSS
I1007 02:01:04.807344 140317719510080 gin_utils.py:87] 
I1007 02:01:04.807367 140317719510080 gin_utils.py:87] # Parameters for partitioning.PjitPartitioner:
I1007 02:01:04.807389 140317719510080 gin_utils.py:87] # ==============================================================================
I1007 02:01:04.807412 140317719510080 gin_utils.py:87] partitioning.PjitPartitioner.logical_axis_rules = \
I1007 02:01:04.807435 140317719510080 gin_utils.py:87]     @partitioning.standard_logical_axis_rules()
I1007 02:01:04.807458 140317719510080 gin_utils.py:87] partitioning.PjitPartitioner.model_parallel_submesh = None
I1007 02:01:04.807481 140317719510080 gin_utils.py:87] partitioning.PjitPartitioner.num_partitions = 1
I1007 02:01:04.807504 140317719510080 gin_utils.py:87] 
I1007 02:01:04.807527 140317719510080 gin_utils.py:87] # Parameters for utils.RestoreCheckpointConfig:
I1007 02:01:04.807549 140317719510080 gin_utils.py:87] # ==============================================================================
I1007 02:01:04.807572 140317719510080 gin_utils.py:87] utils.RestoreCheckpointConfig.path = []
I1007 02:01:04.807595 140317719510080 gin_utils.py:87] 
I1007 02:01:04.807617 140317719510080 gin_utils.py:87] # Parameters for utils.SaveCheckpointConfig:
I1007 02:01:04.807640 140317719510080 gin_utils.py:87] # ==============================================================================
I1007 02:01:04.807663 140317719510080 gin_utils.py:87] utils.SaveCheckpointConfig.dtype = 'float32'
I1007 02:01:04.807685 140317719510080 gin_utils.py:87] utils.SaveCheckpointConfig.keep = None
I1007 02:01:04.807708 140317719510080 gin_utils.py:87] utils.SaveCheckpointConfig.period = 1000
I1007 02:01:04.807731 140317719510080 gin_utils.py:87] utils.SaveCheckpointConfig.save_dataset = False
I1007 02:01:04.807754 140317719510080 gin_utils.py:87] 
I1007 02:01:04.807776 140317719510080 gin_utils.py:87] # Parameters for seqio.SentencePieceVocabulary:
I1007 02:01:04.807799 140317719510080 gin_utils.py:87] # ==============================================================================
I1007 02:01:04.807822 140317719510080 gin_utils.py:87] seqio.SentencePieceVocabulary.extra_ids = 100
I1007 02:01:04.807851 140317719510080 gin_utils.py:87] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I1007 02:01:04.807874 140317719510080 gin_utils.py:87]     'gs://sultan-t5x/spiece.model'
I1007 02:01:04.807897 140317719510080 gin_utils.py:87] 
I1007 02:01:04.807919 140317719510080 gin_utils.py:87] # Parameters for network.T5Config:
I1007 02:01:04.807942 140317719510080 gin_utils.py:87] # ==============================================================================
I1007 02:01:04.807965 140317719510080 gin_utils.py:87] network.T5Config.dropout_rate = %DROPOUT_RATE
I1007 02:01:04.807987 140317719510080 gin_utils.py:87] network.T5Config.dtype = 'bfloat16'
I1007 02:01:04.808010 140317719510080 gin_utils.py:87] network.T5Config.emb_dim = 768
I1007 02:01:04.808033 140317719510080 gin_utils.py:87] network.T5Config.head_dim = 64
I1007 02:01:04.808055 140317719510080 gin_utils.py:87] network.T5Config.logits_via_embedding = False
I1007 02:01:04.808082 140317719510080 gin_utils.py:87] network.T5Config.mlp_activations = ('gelu', 'linear')
I1007 02:01:04.808105 140317719510080 gin_utils.py:87] network.T5Config.mlp_dim = 2048
I1007 02:01:04.808128 140317719510080 gin_utils.py:87] network.T5Config.num_decoder_layers = 12
I1007 02:01:04.808167 140317719510080 gin_utils.py:87] network.T5Config.num_encoder_layers = 12
I1007 02:01:04.808189 140317719510080 gin_utils.py:87] network.T5Config.num_heads = 12
I1007 02:01:04.808210 140317719510080 gin_utils.py:87] network.T5Config.vocab_size = 32128
I1007 02:01:04.808231 140317719510080 gin_utils.py:87] 
I1007 02:01:04.808253 140317719510080 gin_utils.py:87] # Parameters for train_script.train:
I1007 02:01:04.808274 140317719510080 gin_utils.py:87] # ==============================================================================
I1007 02:01:04.808297 140317719510080 gin_utils.py:87] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I1007 02:01:04.808319 140317719510080 gin_utils.py:87] train_script.train.eval_period = 2000
I1007 02:01:04.808341 140317719510080 gin_utils.py:87] train_script.train.eval_steps = 20
I1007 02:01:04.808362 140317719510080 gin_utils.py:87] train_script.train.infer_eval_dataset_cfg = None
I1007 02:01:04.808384 140317719510080 gin_utils.py:87] train_script.train.model = %MODEL
I1007 02:01:04.808405 140317719510080 gin_utils.py:87] train_script.train.model_dir = %MODEL_DIR
I1007 02:01:04.808427 140317719510080 gin_utils.py:87] train_script.train.partitioner = @partitioning.PjitPartitioner()
I1007 02:01:04.808449 140317719510080 gin_utils.py:87] train_script.train.random_seed = %RANDOM_SEED
I1007 02:01:04.808493 140317719510080 gin_utils.py:87] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I1007 02:01:04.808515 140317719510080 gin_utils.py:87] train_script.train.total_steps = %TRAIN_STEPS
I1007 02:01:04.808538 140317719510080 gin_utils.py:87] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I1007 02:01:04.808578 140317719510080 gin_utils.py:87] train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()
I1007 02:01:04.808603 140317719510080 gin_utils.py:87] train_script.train.trainer_cls = @trainer.Trainer
I1007 02:01:04.808628 140317719510080 gin_utils.py:87] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I1007 02:01:04.808669 140317719510080 gin_utils.py:87] 
I1007 02:01:04.808695 140317719510080 gin_utils.py:87] # Parameters for trainer.Trainer:
I1007 02:01:04.808737 140317719510080 gin_utils.py:87] # ==============================================================================
I1007 02:01:04.808762 140317719510080 gin_utils.py:87] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I1007 02:01:04.808788 140317719510080 gin_utils.py:87] trainer.Trainer.num_microbatches = None
I1007 02:01:04.808813 140317719510080 gin_utils.py:87] 
I1007 02:01:04.808844 140317719510080 gin_utils.py:87] # Parameters for network.Transformer:
I1007 02:01:04.808870 140317719510080 gin_utils.py:87] # ==============================================================================
I1007 02:01:04.808896 140317719510080 gin_utils.py:87] network.Transformer.config = @network.T5Config()
I1007 02:01:04.809994 140317719510080 partitioning.py:559] `activation_partitioning_dims` = 1, `parameter_partitioning_dims` = 1
I1007 02:01:07.129195 140317719510080 train.py:196] Process ID: 0
I1007 02:01:07.167000 140317719510080 train.py:250] Using fast RngBitGenerator PRNG for initialization and dropout.
I1007 02:01:07.237880 140317719510080 train.py:256] Random seed not provided, using RNG seed 1696644067
I1007 02:01:07.360049 140317719510080 partitioning.py:151] last device coords : (1, 1, 0, 1)
last local device coords: (1, 1, 0, 1)
W1007 02:01:07.360248 140317719510080 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I1007 02:01:07.360418 140317719510080 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I1007 02:01:07.360481 140317719510080 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]
 [TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1)]
 [TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0)]
 [TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1)]
 [TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0)]
 [TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1)]
 [TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]]
I1007 02:01:07.360689 140317719510080 partitioning.py:257] global_mesh devices shape: (8, 1)
I1007 02:01:08.152340 140317719510080 utils.py:1957] Initializing dataset for task 'arabic_dataset' with a replica batch size of 256 and a seed of 1696644068
I1007 02:01:08.152507 140317719510080 dataset_providers.py:1597] Sharding at the data source: 1 of 1
W1007 02:01:08.152554 140317719510080 dataset_providers.py:450] Using an uncached FunctionDataset for training is not recommended since it often results in insufficient shuffling on restarts, resulting in overfitting. It is highly recommended that you cache this task before training with it or use a data source that supports lower-level shuffling (e.g., FileDataSource).
W1007 02:01:11.283345 140317719510080 preprocessors.py:2398] unsupervised preprocessor got preprocessors=None; no preprocessing will be applied.
W1007 02:01:11.866543 140317719510080 utils.py:899] Features not in `features_length` will be removed during packing: {'inputs_pretokenized', 'targets_pretokenized'}
I1007 02:01:13.120296 140317719510080 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I1007 02:01:13.120432 140317719510080 dataset_providers.py:2438] feature: encoder_input_tokens 	 shape: [256, 512] 	 dtype: int32
I1007 02:01:13.120484 140317719510080 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [256, 114] 	 dtype: int32
I1007 02:01:13.120522 140317719510080 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [256, 114] 	 dtype: int32
I1007 02:01:13.120554 140317719510080 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [256, 114] 	 dtype: int32
I1007 02:01:13.120586 140317719510080 dataset_providers.py:2438] feature: encoder_segment_ids 	 shape: [256, 512] 	 dtype: int32
I1007 02:01:13.120616 140317719510080 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [256, 114] 	 dtype: int32
I1007 02:01:13.120647 140317719510080 dataset_providers.py:2438] feature: encoder_positions 	 shape: [256, 512] 	 dtype: int32
I1007 02:01:13.120680 140317719510080 dataset_providers.py:2438] feature: decoder_positions 	 shape: [256, 114] 	 dtype: int32
I1007 02:01:13.339916 140317719510080 utils.py:2062] Task arabic_dataset has no 'validation' split; skipping training evaluation.
W1007 02:01:13.340071 140317719510080 train.py:324] No train_eval datasets loaded from config `train_eval_dataset_cfg`: DatasetConfig(mixture_or_task_name='arabic_dataset', task_feature_lengths={'inputs': 512, 'targets': 114}, split='validation', batch_size=256, shuffle=False, seed=42, use_cached=False, pack=True, use_custom_packing_ops=False, module=None, use_memory_cache=True, trim_output_features=True)
W1007 02:01:14.940363 140317719510080 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:14.941378 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.942474 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:14.943467 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.944420 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.945502 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.946442 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.947408 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:14.948360 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:14.949808 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:14.950856 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:14.951823 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.952745 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:14.953671 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.954658 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.955598 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.956519 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:14.957456 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.958434 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.959451 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.960678 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.961836 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:14.962822 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:14.963578 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:14.964323 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:14.965063 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.965970 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:14.966965 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.967890 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.968784 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.969781 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:14.971109 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.972165 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.973063 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.974059 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.974976 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:14.975931 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:14.976663 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:14.977392 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:14.978129 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.979101 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:14.980188 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.981379 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.982370 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.983722 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:14.984708 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.985630 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.986534 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.987457 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.988426 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:14.989329 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:14.990241 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:14.991258 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:14.992131 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.993089 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:14.993995 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.994910 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.995798 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.996758 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:14.997667 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.998579 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:14.999504 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.001320 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.002324 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.003263 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.004002 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.004722 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.005524 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.006426 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.007359 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.008276 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.009235 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.010426 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.011650 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.012588 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.013543 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.014467 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.015380 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.016278 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.017016 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.017800 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.018522 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.019442 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.020648 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.021857 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.022753 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.023672 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.024579 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.025491 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.026455 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.027430 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.028335 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.029247 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.030248 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.031247 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.032124 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.033030 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.033946 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.034934 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.035857 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.036737 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.037652 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.038626 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.039578 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.040774 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.041894 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.042854 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.043580 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.044310 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.045036 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.045922 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.046872 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.047751 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.048634 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.049560 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.051354 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.052341 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.053258 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.054160 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.055132 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.056044 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.056763 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.057488 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.058233 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.059214 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.060352 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.061560 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.062495 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.063488 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.064404 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.065318 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.066234 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.067145 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.068129 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.069052 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.069893 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.070886 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.071870 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.073266 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.074188 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.075187 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.076183 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.077200 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.078132 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.079051 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.080136 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.081408 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.082385 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.083331 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.084095 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.084821 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.085612 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.086542 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.087478 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.088388 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.089366 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.090534 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.091768 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.092706 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.093683 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.094588 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.095592 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.096548 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.097275 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.098064 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.098782 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.099777 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.100980 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.102504 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.103439 140317719510080 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.104367 140317719510080 adafactor.py:358] Since rank of parameter decoder/relpos_bias/rel_embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W1007 02:01:15.105118 140317719510080 adafactor.py:358] Since rank of parameter encoder/encoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.105854 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_0/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.106822 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_0/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.107728 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_0/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.108632 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_0/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.109559 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.110872 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.112028 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.112926 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_0/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.113644 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.114377 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_1/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.115367 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_1/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.116276 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_1/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.117172 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_1/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.118068 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.119045 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.120136 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.121308 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_1/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.122152 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.123057 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_10/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.124053 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_10/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.124981 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_10/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.125907 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_10/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.126830 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.127788 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.128701 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.129672 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_10/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.130630 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.131767 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_11/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.132684 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_11/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.133593 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_11/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.134503 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_11/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.135440 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_11/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.136416 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_11/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.137333 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_11/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.138250 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_11/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.139019 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.139918 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_2/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.141103 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_2/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.142143 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_2/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.143081 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_2/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.144029 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.144954 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.145869 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.146758 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_2/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.147528 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.148336 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_3/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.149256 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_3/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.150387 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_3/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.151597 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_3/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.153003 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.153919 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.154842 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.155760 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_3/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.156534 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.157397 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_4/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.158300 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_4/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.159224 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_4/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.160336 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_4/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.161598 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.162717 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.163762 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.164696 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_4/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.165497 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.166240 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_5/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.167175 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_5/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.168078 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_5/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.168985 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_5/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.170093 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_5/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.171334 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_5/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.172390 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_5/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.173278 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_5/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.174079 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.174822 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_6/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.175730 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_6/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.176642 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_6/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.177554 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_6/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.178564 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.179627 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.180816 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.181936 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_6/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.182735 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.183494 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_7/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.184389 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_7/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.185284 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_7/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.186185 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_7/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.187174 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.188274 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.189248 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.190403 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_7/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.191485 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.192324 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_8/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.193237 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_8/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.194143 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_8/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.195118 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_8/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.196026 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.196915 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.197822 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.198712 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_8/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.199783 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.200751 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_9/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.201896 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_9/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.202787 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_9/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.204188 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_9/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.205105 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.206009 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:15.206955 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:15.207912 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_9/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.208642 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:15.209373 140317719510080 adafactor.py:358] Since rank of parameter encoder/relpos_bias/rel_embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W1007 02:01:15.210340 140317719510080 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
I1007 02:01:15.566226 140317719510080 utils.py:944] Using latest T5X checkpoint.
I1007 02:01:16.044315 140317719510080 utils.py:441] Initializing parameters from specific T5X checkpoint gs://sultan-t5x/arabict5_tinyy/checkpoint_0
I1007 02:01:16.439119 140317719510080 checkpoints.py:1054] Restoring from checkpoint: gs://sultan-t5x/arabict5_tinyy/checkpoint_0/checkpoint
I1007 02:01:16.527050 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/m
I1007 02:01:16.527169 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v
I1007 02:01:16.527203 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_col
I1007 02:01:16.527230 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_row
I1007 02:01:16.527257 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/key/kernel/m
I1007 02:01:16.527282 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/key/kernel/v
I1007 02:01:16.527306 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/key/kernel/v_col
I1007 02:01:16.527335 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/key/kernel/v_row
I1007 02:01:16.527361 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/out/kernel/m
I1007 02:01:16.527385 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/out/kernel/v
I1007 02:01:16.527409 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/out/kernel/v_col
I1007 02:01:16.527432 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/out/kernel/v_row
I1007 02:01:16.527455 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/query/kernel/m
I1007 02:01:16.527478 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/query/kernel/v
I1007 02:01:16.527514 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/query/kernel/v_col
I1007 02:01:16.527538 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/query/kernel/v_row
I1007 02:01:16.527561 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/value/kernel/m
I1007 02:01:16.527585 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/value/kernel/v
I1007 02:01:16.527608 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/value/kernel/v_col
I1007 02:01:16.527631 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/value/kernel/v_row
I1007 02:01:16.527654 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/m
I1007 02:01:16.527678 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v
I1007 02:01:16.527700 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_col
I1007 02:01:16.527724 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_0/kernel/v_row
I1007 02:01:16.527748 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/m
I1007 02:01:16.527771 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v
I1007 02:01:16.527794 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_col
I1007 02:01:16.527817 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi_1/kernel/v_row
I1007 02:01:16.527839 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/m
I1007 02:01:16.527870 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v
I1007 02:01:16.527894 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_col
I1007 02:01:16.527917 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_row
I1007 02:01:16.527939 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_cross_attention_layer_norm/scale/m
I1007 02:01:16.527962 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_cross_attention_layer_norm/scale/v
I1007 02:01:16.527985 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_cross_attention_layer_norm/scale/v_col
I1007 02:01:16.528008 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_cross_attention_layer_norm/scale/v_row
I1007 02:01:16.528031 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/m
I1007 02:01:16.528054 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v
I1007 02:01:16.528077 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_col
I1007 02:01:16.528100 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_row
I1007 02:01:16.528123 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/m
I1007 02:01:16.528146 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v
I1007 02:01:16.528174 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_col
I1007 02:01:16.528198 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_row
I1007 02:01:16.528221 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/m
I1007 02:01:16.528244 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v
I1007 02:01:16.528267 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_col
I1007 02:01:16.528290 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_row
I1007 02:01:16.528313 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/m
I1007 02:01:16.528339 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v
I1007 02:01:16.528363 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_col
I1007 02:01:16.528387 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_row
I1007 02:01:16.528410 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/m
I1007 02:01:16.528432 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v
I1007 02:01:16.528455 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_col
I1007 02:01:16.528478 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_row
I1007 02:01:16.528501 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/m
I1007 02:01:16.528525 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v
I1007 02:01:16.528548 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_col
I1007 02:01:16.528571 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_row
I1007 02:01:16.528594 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/key/kernel/m
I1007 02:01:16.528616 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/key/kernel/v
I1007 02:01:16.528639 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/key/kernel/v_col
I1007 02:01:16.528661 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/key/kernel/v_row
I1007 02:01:16.528684 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/out/kernel/m
I1007 02:01:16.528707 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/out/kernel/v
I1007 02:01:16.528729 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/out/kernel/v_col
I1007 02:01:16.528752 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/out/kernel/v_row
I1007 02:01:16.528780 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/query/kernel/m
I1007 02:01:16.528803 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/query/kernel/v
I1007 02:01:16.528825 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/query/kernel/v_col
I1007 02:01:16.528852 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/query/kernel/v_row
I1007 02:01:16.528876 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/value/kernel/m
I1007 02:01:16.528898 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/value/kernel/v
I1007 02:01:16.528920 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/value/kernel/v_col
I1007 02:01:16.528943 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/value/kernel/v_row
I1007 02:01:16.528965 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/m
I1007 02:01:16.528988 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v
I1007 02:01:16.529011 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_col
I1007 02:01:16.529033 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_0/kernel/v_row
I1007 02:01:16.529056 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/m
I1007 02:01:16.529078 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v
I1007 02:01:16.529101 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_col
I1007 02:01:16.529124 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi_1/kernel/v_row
I1007 02:01:16.529147 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/m
I1007 02:01:16.529169 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v
I1007 02:01:16.529192 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_col
I1007 02:01:16.529214 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_row
I1007 02:01:16.529237 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_cross_attention_layer_norm/scale/m
I1007 02:01:16.529260 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_cross_attention_layer_norm/scale/v
I1007 02:01:16.529283 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_cross_attention_layer_norm/scale/v_col
I1007 02:01:16.529306 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_cross_attention_layer_norm/scale/v_row
I1007 02:01:16.529328 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/m
I1007 02:01:16.529351 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v
I1007 02:01:16.529373 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_col
I1007 02:01:16.529399 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_row
I1007 02:01:16.529422 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/m
I1007 02:01:16.529445 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v
I1007 02:01:16.529468 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_col
I1007 02:01:16.529491 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_row
I1007 02:01:16.529513 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/m
I1007 02:01:16.529536 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v
I1007 02:01:16.529558 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_col
I1007 02:01:16.529581 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_row
I1007 02:01:16.529607 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/m
I1007 02:01:16.529630 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v
I1007 02:01:16.529672 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_col
I1007 02:01:16.529699 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_row
I1007 02:01:16.529726 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/m
I1007 02:01:16.529752 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v
I1007 02:01:16.529779 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_col
I1007 02:01:16.529805 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_row
I1007 02:01:16.529832 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/m
I1007 02:01:16.529863 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v
I1007 02:01:16.529890 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_col
I1007 02:01:16.529917 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_row
I1007 02:01:16.529943 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/key/kernel/m
I1007 02:01:16.529969 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/key/kernel/v
I1007 02:01:16.529995 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/key/kernel/v_col
I1007 02:01:16.530022 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/key/kernel/v_row
I1007 02:01:16.530049 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/out/kernel/m
I1007 02:01:16.530075 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/out/kernel/v
I1007 02:01:16.530106 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/out/kernel/v_col
I1007 02:01:16.530133 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/out/kernel/v_row
I1007 02:01:16.530160 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/query/kernel/m
I1007 02:01:16.530187 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/query/kernel/v
I1007 02:01:16.530213 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/query/kernel/v_col
I1007 02:01:16.530240 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/query/kernel/v_row
I1007 02:01:16.530266 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/value/kernel/m
I1007 02:01:16.530292 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/value/kernel/v
I1007 02:01:16.530318 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/value/kernel/v_col
I1007 02:01:16.530344 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/value/kernel/v_row
I1007 02:01:16.530371 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/m
I1007 02:01:16.530397 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v
I1007 02:01:16.530425 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_col
I1007 02:01:16.530452 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_0/kernel/v_row
I1007 02:01:16.530478 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/m
I1007 02:01:16.530505 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v
I1007 02:01:16.530531 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_col
I1007 02:01:16.530558 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi_1/kernel/v_row
I1007 02:01:16.530584 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/m
I1007 02:01:16.530615 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v
I1007 02:01:16.530642 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_col
I1007 02:01:16.530668 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_row
I1007 02:01:16.530695 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_cross_attention_layer_norm/scale/m
I1007 02:01:16.530722 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_cross_attention_layer_norm/scale/v
I1007 02:01:16.530749 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_cross_attention_layer_norm/scale/v_col
I1007 02:01:16.530776 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_cross_attention_layer_norm/scale/v_row
I1007 02:01:16.530834 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/m
I1007 02:01:16.530872 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v
I1007 02:01:16.530900 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_col
I1007 02:01:16.530928 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_row
I1007 02:01:16.530955 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/m
I1007 02:01:16.530982 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v
I1007 02:01:16.531009 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_col
I1007 02:01:16.531036 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_row
I1007 02:01:16.531063 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/m
I1007 02:01:16.531090 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v
I1007 02:01:16.531117 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_col
I1007 02:01:16.531145 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_row
I1007 02:01:16.531172 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/m
I1007 02:01:16.531200 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v
I1007 02:01:16.531227 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_col
I1007 02:01:16.531254 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_row
I1007 02:01:16.531281 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/m
I1007 02:01:16.531308 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v
I1007 02:01:16.531335 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_col
I1007 02:01:16.531362 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_row
I1007 02:01:16.531389 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/m
I1007 02:01:16.531416 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v
I1007 02:01:16.531443 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_col
I1007 02:01:16.531470 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_row
I1007 02:01:16.531497 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/key/kernel/m
I1007 02:01:16.531525 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/key/kernel/v
I1007 02:01:16.531552 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/key/kernel/v_col
I1007 02:01:16.531584 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/key/kernel/v_row
I1007 02:01:16.531611 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/out/kernel/m
I1007 02:01:16.531639 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/out/kernel/v
I1007 02:01:16.531665 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/out/kernel/v_col
I1007 02:01:16.531692 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/out/kernel/v_row
I1007 02:01:16.531720 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/query/kernel/m
I1007 02:01:16.531747 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/query/kernel/v
I1007 02:01:16.531785 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/query/kernel/v_col
I1007 02:01:16.531807 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/query/kernel/v_row
I1007 02:01:16.531830 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/value/kernel/m
I1007 02:01:16.531857 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/value/kernel/v
I1007 02:01:16.531880 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/value/kernel/v_col
I1007 02:01:16.531902 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/value/kernel/v_row
I1007 02:01:16.531925 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/wi_0/kernel/m
I1007 02:01:16.531948 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/wi_0/kernel/v
I1007 02:01:16.531971 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/wi_0/kernel/v_col
I1007 02:01:16.531994 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/wi_0/kernel/v_row
I1007 02:01:16.532017 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/wi_1/kernel/m
I1007 02:01:16.532039 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/wi_1/kernel/v
I1007 02:01:16.532062 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/wi_1/kernel/v_col
I1007 02:01:16.532085 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/wi_1/kernel/v_row
I1007 02:01:16.532108 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/wo/kernel/m
I1007 02:01:16.532130 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/wo/kernel/v
I1007 02:01:16.532154 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/wo/kernel/v_col
I1007 02:01:16.532176 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/wo/kernel/v_row
I1007 02:01:16.532199 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_cross_attention_layer_norm/scale/m
I1007 02:01:16.532222 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_cross_attention_layer_norm/scale/v
I1007 02:01:16.532248 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_cross_attention_layer_norm/scale/v_col
I1007 02:01:16.532272 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_cross_attention_layer_norm/scale/v_row
I1007 02:01:16.532295 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/m
I1007 02:01:16.532317 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v
I1007 02:01:16.532340 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_col
I1007 02:01:16.532362 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_row
I1007 02:01:16.532386 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/m
I1007 02:01:16.532408 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v
I1007 02:01:16.532431 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_col
I1007 02:01:16.532454 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_row
I1007 02:01:16.532476 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/m
I1007 02:01:16.532500 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v
I1007 02:01:16.532523 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_col
I1007 02:01:16.532546 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_row
I1007 02:01:16.532569 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/m
I1007 02:01:16.532591 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v
I1007 02:01:16.532614 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_col
I1007 02:01:16.532637 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_row
I1007 02:01:16.532660 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/m
I1007 02:01:16.532682 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v
I1007 02:01:16.532705 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_col
I1007 02:01:16.532728 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_row
I1007 02:01:16.532751 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/m
I1007 02:01:16.532774 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v
I1007 02:01:16.532797 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_col
I1007 02:01:16.532820 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_row
I1007 02:01:16.532843 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/key/kernel/m
I1007 02:01:16.532874 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/key/kernel/v
I1007 02:01:16.532897 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/key/kernel/v_col
I1007 02:01:16.532920 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/key/kernel/v_row
I1007 02:01:16.532943 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/out/kernel/m
I1007 02:01:16.532965 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/out/kernel/v
I1007 02:01:16.532988 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/out/kernel/v_col
I1007 02:01:16.533010 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/out/kernel/v_row
I1007 02:01:16.533033 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/query/kernel/m
I1007 02:01:16.533056 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/query/kernel/v
I1007 02:01:16.533078 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/query/kernel/v_col
I1007 02:01:16.533101 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/query/kernel/v_row
I1007 02:01:16.533124 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/value/kernel/m
I1007 02:01:16.533147 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/value/kernel/v
I1007 02:01:16.533169 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/value/kernel/v_col
I1007 02:01:16.533192 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/value/kernel/v_row
I1007 02:01:16.533215 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/m
I1007 02:01:16.533239 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v
I1007 02:01:16.533262 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_col
I1007 02:01:16.533285 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_0/kernel/v_row
I1007 02:01:16.533308 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/m
I1007 02:01:16.533332 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v
I1007 02:01:16.533355 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_col
I1007 02:01:16.533378 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi_1/kernel/v_row
I1007 02:01:16.533401 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/m
I1007 02:01:16.533424 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v
I1007 02:01:16.533447 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_col
I1007 02:01:16.533473 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_row
I1007 02:01:16.533496 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_cross_attention_layer_norm/scale/m
I1007 02:01:16.533519 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_cross_attention_layer_norm/scale/v
I1007 02:01:16.533542 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_cross_attention_layer_norm/scale/v_col
I1007 02:01:16.533565 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_cross_attention_layer_norm/scale/v_row
I1007 02:01:16.533588 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/m
I1007 02:01:16.533611 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v
I1007 02:01:16.533634 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_col
I1007 02:01:16.533656 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_row
I1007 02:01:16.533679 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/m
I1007 02:01:16.533701 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v
I1007 02:01:16.533724 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_col
I1007 02:01:16.533747 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_row
I1007 02:01:16.533769 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/m
I1007 02:01:16.533792 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v
I1007 02:01:16.533814 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_col
I1007 02:01:16.533837 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_row
I1007 02:01:16.533863 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/m
I1007 02:01:16.533886 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v
I1007 02:01:16.533909 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_col
I1007 02:01:16.533931 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_row
I1007 02:01:16.533954 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/m
I1007 02:01:16.533977 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v
I1007 02:01:16.534000 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_col
I1007 02:01:16.534022 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_row
I1007 02:01:16.534044 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/m
I1007 02:01:16.534067 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v
I1007 02:01:16.534093 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_col
I1007 02:01:16.534116 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_row
I1007 02:01:16.534139 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/key/kernel/m
I1007 02:01:16.534161 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/key/kernel/v
I1007 02:01:16.534184 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/key/kernel/v_col
I1007 02:01:16.534206 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/key/kernel/v_row
I1007 02:01:16.534229 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/out/kernel/m
I1007 02:01:16.534251 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/out/kernel/v
I1007 02:01:16.534273 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/out/kernel/v_col
I1007 02:01:16.534296 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/out/kernel/v_row
I1007 02:01:16.534318 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/query/kernel/m
I1007 02:01:16.534340 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/query/kernel/v
I1007 02:01:16.534362 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/query/kernel/v_col
I1007 02:01:16.534385 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/query/kernel/v_row
I1007 02:01:16.534407 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/value/kernel/m
I1007 02:01:16.534430 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/value/kernel/v
I1007 02:01:16.534452 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/value/kernel/v_col
I1007 02:01:16.534475 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/value/kernel/v_row
I1007 02:01:16.534497 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/m
I1007 02:01:16.534519 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v
I1007 02:01:16.534542 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_col
I1007 02:01:16.534565 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_0/kernel/v_row
I1007 02:01:16.534588 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/m
I1007 02:01:16.534610 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v
I1007 02:01:16.534633 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_col
I1007 02:01:16.534656 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi_1/kernel/v_row
I1007 02:01:16.534678 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/m
I1007 02:01:16.534705 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v
I1007 02:01:16.534728 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_col
I1007 02:01:16.534750 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_row
I1007 02:01:16.534773 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_cross_attention_layer_norm/scale/m
I1007 02:01:16.534795 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_cross_attention_layer_norm/scale/v
I1007 02:01:16.534830 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_cross_attention_layer_norm/scale/v_col
I1007 02:01:16.534857 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_cross_attention_layer_norm/scale/v_row
I1007 02:01:16.534880 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/m
I1007 02:01:16.534903 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v
I1007 02:01:16.534925 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_col
I1007 02:01:16.534948 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_row
I1007 02:01:16.534971 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/m
I1007 02:01:16.534994 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v
I1007 02:01:16.535016 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_col
I1007 02:01:16.535038 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_row
I1007 02:01:16.535061 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/m
I1007 02:01:16.535084 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v
I1007 02:01:16.535107 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_col
I1007 02:01:16.535129 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_row
I1007 02:01:16.535152 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/m
I1007 02:01:16.535175 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v
I1007 02:01:16.535197 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_col
I1007 02:01:16.535220 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_row
I1007 02:01:16.535243 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/m
I1007 02:01:16.535266 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v
I1007 02:01:16.535289 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_col
I1007 02:01:16.535312 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_row
I1007 02:01:16.535340 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/m
I1007 02:01:16.535364 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v
I1007 02:01:16.535387 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_col
I1007 02:01:16.535410 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_row
I1007 02:01:16.535434 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/key/kernel/m
I1007 02:01:16.535456 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/key/kernel/v
I1007 02:01:16.535479 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/key/kernel/v_col
I1007 02:01:16.535502 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/key/kernel/v_row
I1007 02:01:16.535525 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/out/kernel/m
I1007 02:01:16.535548 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/out/kernel/v
I1007 02:01:16.535570 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/out/kernel/v_col
I1007 02:01:16.535593 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/out/kernel/v_row
I1007 02:01:16.535616 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/query/kernel/m
I1007 02:01:16.535639 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/query/kernel/v
I1007 02:01:16.535662 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/query/kernel/v_col
I1007 02:01:16.535685 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/query/kernel/v_row
I1007 02:01:16.535708 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/value/kernel/m
I1007 02:01:16.535731 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/value/kernel/v
I1007 02:01:16.535754 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/value/kernel/v_col
I1007 02:01:16.535777 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/value/kernel/v_row
I1007 02:01:16.535800 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/m
I1007 02:01:16.535823 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v
I1007 02:01:16.535851 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_col
I1007 02:01:16.535875 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_0/kernel/v_row
I1007 02:01:16.535898 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/m
I1007 02:01:16.535921 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v
I1007 02:01:16.535947 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_col
I1007 02:01:16.535971 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi_1/kernel/v_row
I1007 02:01:16.535994 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/m
I1007 02:01:16.536017 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v
I1007 02:01:16.536040 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_col
I1007 02:01:16.536063 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_row
I1007 02:01:16.536086 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_cross_attention_layer_norm/scale/m
I1007 02:01:16.536108 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_cross_attention_layer_norm/scale/v
I1007 02:01:16.536132 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_cross_attention_layer_norm/scale/v_col
I1007 02:01:16.536154 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_cross_attention_layer_norm/scale/v_row
I1007 02:01:16.536177 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/m
I1007 02:01:16.536200 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v
I1007 02:01:16.536223 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_col
I1007 02:01:16.536245 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_row
I1007 02:01:16.536267 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/m
I1007 02:01:16.536290 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v
I1007 02:01:16.536313 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_col
I1007 02:01:16.536335 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_row
I1007 02:01:16.536358 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/m
I1007 02:01:16.536380 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v
I1007 02:01:16.536402 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_col
I1007 02:01:16.536424 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_row
I1007 02:01:16.536447 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/m
I1007 02:01:16.536469 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v
I1007 02:01:16.536492 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_col
I1007 02:01:16.536514 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_row
I1007 02:01:16.536537 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/m
I1007 02:01:16.536562 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v
I1007 02:01:16.536586 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_col
I1007 02:01:16.536608 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_row
I1007 02:01:16.536630 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/m
I1007 02:01:16.536653 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v
I1007 02:01:16.536674 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_col
I1007 02:01:16.536697 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_row
I1007 02:01:16.536719 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/key/kernel/m
I1007 02:01:16.536742 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/key/kernel/v
I1007 02:01:16.536765 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/key/kernel/v_col
I1007 02:01:16.536787 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/key/kernel/v_row
I1007 02:01:16.536810 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/out/kernel/m
I1007 02:01:16.536833 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/out/kernel/v
I1007 02:01:16.536859 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/out/kernel/v_col
I1007 02:01:16.536882 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/out/kernel/v_row
I1007 02:01:16.536905 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/query/kernel/m
I1007 02:01:16.536928 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/query/kernel/v
I1007 02:01:16.536951 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/query/kernel/v_col
I1007 02:01:16.536974 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/query/kernel/v_row
I1007 02:01:16.536996 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/value/kernel/m
I1007 02:01:16.537019 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/value/kernel/v
I1007 02:01:16.537042 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/value/kernel/v_col
I1007 02:01:16.537064 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/value/kernel/v_row
I1007 02:01:16.537087 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/wi_0/kernel/m
I1007 02:01:16.537109 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/wi_0/kernel/v
I1007 02:01:16.537132 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/wi_0/kernel/v_col
I1007 02:01:16.537158 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/wi_0/kernel/v_row
I1007 02:01:16.537181 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/wi_1/kernel/m
I1007 02:01:16.537204 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/wi_1/kernel/v
I1007 02:01:16.537226 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/wi_1/kernel/v_col
I1007 02:01:16.537249 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/wi_1/kernel/v_row
I1007 02:01:16.537272 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/wo/kernel/m
I1007 02:01:16.537295 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/wo/kernel/v
I1007 02:01:16.537318 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/wo/kernel/v_col
I1007 02:01:16.537340 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/wo/kernel/v_row
I1007 02:01:16.537363 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_cross_attention_layer_norm/scale/m
I1007 02:01:16.537386 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_cross_attention_layer_norm/scale/v
I1007 02:01:16.537409 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_cross_attention_layer_norm/scale/v_col
I1007 02:01:16.537431 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_cross_attention_layer_norm/scale/v_row
I1007 02:01:16.537454 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/m
I1007 02:01:16.537477 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v
I1007 02:01:16.537499 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_col
I1007 02:01:16.537522 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_row
I1007 02:01:16.537545 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/m
I1007 02:01:16.537567 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v
I1007 02:01:16.537590 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_col
I1007 02:01:16.537613 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_row
I1007 02:01:16.537635 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/m
I1007 02:01:16.537658 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v
I1007 02:01:16.537680 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_col
I1007 02:01:16.537703 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_row
I1007 02:01:16.537725 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/m
I1007 02:01:16.537748 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v
I1007 02:01:16.537770 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_col
I1007 02:01:16.537796 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_row
I1007 02:01:16.537819 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/m
I1007 02:01:16.537842 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v
I1007 02:01:16.537870 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_col
I1007 02:01:16.537893 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_row
I1007 02:01:16.537915 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/m
I1007 02:01:16.537938 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v
I1007 02:01:16.537961 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_col
I1007 02:01:16.537984 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_row
I1007 02:01:16.538007 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/key/kernel/m
I1007 02:01:16.538030 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/key/kernel/v
I1007 02:01:16.538052 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/key/kernel/v_col
I1007 02:01:16.538075 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/key/kernel/v_row
I1007 02:01:16.538098 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/out/kernel/m
I1007 02:01:16.538121 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/out/kernel/v
I1007 02:01:16.538143 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/out/kernel/v_col
I1007 02:01:16.538166 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/out/kernel/v_row
I1007 02:01:16.538189 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/query/kernel/m
I1007 02:01:16.538211 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/query/kernel/v
I1007 02:01:16.538234 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/query/kernel/v_col
I1007 02:01:16.538257 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/query/kernel/v_row
I1007 02:01:16.538280 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/value/kernel/m
I1007 02:01:16.538302 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/value/kernel/v
I1007 02:01:16.538325 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/value/kernel/v_col
I1007 02:01:16.538348 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/value/kernel/v_row
I1007 02:01:16.538371 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/m
I1007 02:01:16.538398 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v
I1007 02:01:16.538422 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_col
I1007 02:01:16.538446 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_0/kernel/v_row
I1007 02:01:16.538469 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/m
I1007 02:01:16.538491 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v
I1007 02:01:16.538514 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_col
I1007 02:01:16.538537 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi_1/kernel/v_row
I1007 02:01:16.538559 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/m
I1007 02:01:16.538581 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v
I1007 02:01:16.538604 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_col
I1007 02:01:16.538626 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_row
I1007 02:01:16.538648 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_cross_attention_layer_norm/scale/m
I1007 02:01:16.538671 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_cross_attention_layer_norm/scale/v
I1007 02:01:16.538693 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_cross_attention_layer_norm/scale/v_col
I1007 02:01:16.538716 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_cross_attention_layer_norm/scale/v_row
I1007 02:01:16.538738 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/m
I1007 02:01:16.538760 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v
I1007 02:01:16.538783 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_col
I1007 02:01:16.538814 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_row
I1007 02:01:16.538837 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/m
I1007 02:01:16.538864 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v
I1007 02:01:16.538887 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_col
I1007 02:01:16.538910 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_row
I1007 02:01:16.538933 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/m
I1007 02:01:16.538955 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v
I1007 02:01:16.538978 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_col
I1007 02:01:16.539001 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_row
I1007 02:01:16.539027 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/m
I1007 02:01:16.539050 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v
I1007 02:01:16.539072 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_col
I1007 02:01:16.539095 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_row
I1007 02:01:16.539118 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/m
I1007 02:01:16.539141 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v
I1007 02:01:16.539164 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_col
I1007 02:01:16.539187 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_row
I1007 02:01:16.539210 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/m
I1007 02:01:16.539232 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v
I1007 02:01:16.539255 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_col
I1007 02:01:16.539278 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_row
I1007 02:01:16.539300 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/key/kernel/m
I1007 02:01:16.539323 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/key/kernel/v
I1007 02:01:16.539345 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/key/kernel/v_col
I1007 02:01:16.539367 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/key/kernel/v_row
I1007 02:01:16.539390 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/out/kernel/m
I1007 02:01:16.539412 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/out/kernel/v
I1007 02:01:16.539434 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/out/kernel/v_col
I1007 02:01:16.539457 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/out/kernel/v_row
I1007 02:01:16.539479 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/query/kernel/m
I1007 02:01:16.539502 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/query/kernel/v
I1007 02:01:16.539524 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/query/kernel/v_col
I1007 02:01:16.539546 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/query/kernel/v_row
I1007 02:01:16.539569 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/value/kernel/m
I1007 02:01:16.539591 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/value/kernel/v
I1007 02:01:16.539621 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/value/kernel/v_col
I1007 02:01:16.539662 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/value/kernel/v_row
I1007 02:01:16.539689 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/m
I1007 02:01:16.539716 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v
I1007 02:01:16.539743 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_col
I1007 02:01:16.539771 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_0/kernel/v_row
I1007 02:01:16.539798 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/m
I1007 02:01:16.539824 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v
I1007 02:01:16.539855 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_col
I1007 02:01:16.539882 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi_1/kernel/v_row
I1007 02:01:16.539909 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/m
I1007 02:01:16.539936 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v
I1007 02:01:16.539963 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_col
I1007 02:01:16.539990 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_row
I1007 02:01:16.540017 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_cross_attention_layer_norm/scale/m
I1007 02:01:16.540043 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_cross_attention_layer_norm/scale/v
I1007 02:01:16.540070 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_cross_attention_layer_norm/scale/v_col
I1007 02:01:16.540097 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_cross_attention_layer_norm/scale/v_row
I1007 02:01:16.540123 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/m
I1007 02:01:16.540150 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v
I1007 02:01:16.540177 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_col
I1007 02:01:16.540204 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_row
I1007 02:01:16.540231 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/m
I1007 02:01:16.540258 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v
I1007 02:01:16.540285 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_col
I1007 02:01:16.540312 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_row
I1007 02:01:16.540338 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/m
I1007 02:01:16.540365 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v
I1007 02:01:16.540396 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_col
I1007 02:01:16.540423 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_row
I1007 02:01:16.540450 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/m
I1007 02:01:16.540477 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v
I1007 02:01:16.540503 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_col
I1007 02:01:16.540531 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_row
I1007 02:01:16.540558 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/m
I1007 02:01:16.540585 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v
I1007 02:01:16.540616 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_col
I1007 02:01:16.540644 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_row
I1007 02:01:16.540674 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/m
I1007 02:01:16.540702 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v
I1007 02:01:16.540729 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_col
I1007 02:01:16.540756 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_row
I1007 02:01:16.540783 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/key/kernel/m
I1007 02:01:16.540810 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/key/kernel/v
I1007 02:01:16.540837 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/key/kernel/v_col
I1007 02:01:16.540868 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/key/kernel/v_row
I1007 02:01:16.540895 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/out/kernel/m
I1007 02:01:16.540923 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/out/kernel/v
I1007 02:01:16.540950 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/out/kernel/v_col
I1007 02:01:16.540977 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/out/kernel/v_row
I1007 02:01:16.541003 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/query/kernel/m
I1007 02:01:16.541030 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/query/kernel/v
I1007 02:01:16.541057 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/query/kernel/v_col
I1007 02:01:16.541083 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/query/kernel/v_row
I1007 02:01:16.541114 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/value/kernel/m
I1007 02:01:16.541141 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/value/kernel/v
I1007 02:01:16.541167 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/value/kernel/v_col
I1007 02:01:16.541194 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/value/kernel/v_row
I1007 02:01:16.541221 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/m
I1007 02:01:16.541248 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v
I1007 02:01:16.541275 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_col
I1007 02:01:16.541302 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_0/kernel/v_row
I1007 02:01:16.541329 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/m
I1007 02:01:16.541356 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v
I1007 02:01:16.541383 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_col
I1007 02:01:16.541410 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi_1/kernel/v_row
I1007 02:01:16.541436 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/m
I1007 02:01:16.541464 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v
I1007 02:01:16.541490 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_col
I1007 02:01:16.541517 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_row
I1007 02:01:16.541544 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_cross_attention_layer_norm/scale/m
I1007 02:01:16.541571 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_cross_attention_layer_norm/scale/v
I1007 02:01:16.541598 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_cross_attention_layer_norm/scale/v_col
I1007 02:01:16.541625 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_cross_attention_layer_norm/scale/v_row
I1007 02:01:16.541652 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/m
I1007 02:01:16.541679 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v
I1007 02:01:16.541706 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_col
I1007 02:01:16.541733 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_row
I1007 02:01:16.541759 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/m
I1007 02:01:16.541795 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v
I1007 02:01:16.541818 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_col
I1007 02:01:16.541841 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_row
I1007 02:01:16.541871 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/m
I1007 02:01:16.541895 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v
I1007 02:01:16.541918 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_col
I1007 02:01:16.541940 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_row
I1007 02:01:16.541964 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/m
I1007 02:01:16.541987 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v
I1007 02:01:16.542010 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_col
I1007 02:01:16.542033 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_row
I1007 02:01:16.542055 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/m
I1007 02:01:16.542078 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v
I1007 02:01:16.542101 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_col
I1007 02:01:16.542124 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_row
I1007 02:01:16.542147 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/m
I1007 02:01:16.542169 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v
I1007 02:01:16.542192 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_col
I1007 02:01:16.542215 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_row
I1007 02:01:16.542238 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/key/kernel/m
I1007 02:01:16.542261 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/key/kernel/v
I1007 02:01:16.542284 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/key/kernel/v_col
I1007 02:01:16.542307 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/key/kernel/v_row
I1007 02:01:16.542330 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/out/kernel/m
I1007 02:01:16.542353 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/out/kernel/v
I1007 02:01:16.542376 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/out/kernel/v_col
I1007 02:01:16.542399 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/out/kernel/v_row
I1007 02:01:16.542422 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/query/kernel/m
I1007 02:01:16.542444 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/query/kernel/v
I1007 02:01:16.542470 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/query/kernel/v_col
I1007 02:01:16.542493 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/query/kernel/v_row
I1007 02:01:16.542516 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/value/kernel/m
I1007 02:01:16.542538 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/value/kernel/v
I1007 02:01:16.542560 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/value/kernel/v_col
I1007 02:01:16.542583 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/value/kernel/v_row
I1007 02:01:16.542606 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/m
I1007 02:01:16.542628 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v
I1007 02:01:16.542651 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_col
I1007 02:01:16.542674 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_0/kernel/v_row
I1007 02:01:16.542697 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/m
I1007 02:01:16.542720 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v
I1007 02:01:16.542743 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_col
I1007 02:01:16.542766 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi_1/kernel/v_row
I1007 02:01:16.542788 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/m
I1007 02:01:16.542820 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v
I1007 02:01:16.542844 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_col
I1007 02:01:16.542871 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_row
I1007 02:01:16.542894 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_cross_attention_layer_norm/scale/m
I1007 02:01:16.542918 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_cross_attention_layer_norm/scale/v
I1007 02:01:16.542941 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_cross_attention_layer_norm/scale/v_col
I1007 02:01:16.542964 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_cross_attention_layer_norm/scale/v_row
I1007 02:01:16.542987 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/m
I1007 02:01:16.543010 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v
I1007 02:01:16.543033 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_col
I1007 02:01:16.543056 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_row
I1007 02:01:16.543078 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/m
I1007 02:01:16.543113 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v
I1007 02:01:16.543136 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_col
I1007 02:01:16.543159 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_row
I1007 02:01:16.543182 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/m
I1007 02:01:16.543205 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v
I1007 02:01:16.543227 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_col
I1007 02:01:16.543250 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_row
I1007 02:01:16.543272 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/m
I1007 02:01:16.543295 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v
I1007 02:01:16.543318 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_col
I1007 02:01:16.543341 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_row
I1007 02:01:16.543363 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/m
I1007 02:01:16.543386 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v
I1007 02:01:16.543409 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_col
I1007 02:01:16.543432 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_row
I1007 02:01:16.543455 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/m
I1007 02:01:16.543477 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v
I1007 02:01:16.543500 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_col
I1007 02:01:16.543523 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_row
I1007 02:01:16.543545 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/m
I1007 02:01:16.543568 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v
I1007 02:01:16.543591 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_col
I1007 02:01:16.543614 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/logits_dense/kernel/v_row
I1007 02:01:16.543637 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/relpos_bias/rel_embedding/m
I1007 02:01:16.543660 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/relpos_bias/rel_embedding/v
I1007 02:01:16.543683 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/relpos_bias/rel_embedding/v_col
I1007 02:01:16.543705 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/decoder/relpos_bias/rel_embedding/v_row
I1007 02:01:16.543728 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/encoder_norm/scale/m
I1007 02:01:16.543755 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/encoder_norm/scale/v
I1007 02:01:16.543778 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/encoder_norm/scale/v_col
I1007 02:01:16.543801 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/encoder_norm/scale/v_row
I1007 02:01:16.543824 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/key/kernel/m
I1007 02:01:16.543850 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/key/kernel/v
I1007 02:01:16.543873 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/key/kernel/v_col
I1007 02:01:16.543896 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/key/kernel/v_row
I1007 02:01:16.543920 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/out/kernel/m
I1007 02:01:16.543943 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/out/kernel/v
I1007 02:01:16.543966 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/out/kernel/v_col
I1007 02:01:16.543989 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/out/kernel/v_row
I1007 02:01:16.544012 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/query/kernel/m
I1007 02:01:16.544034 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/query/kernel/v
I1007 02:01:16.544057 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/query/kernel/v_col
I1007 02:01:16.544079 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/query/kernel/v_row
I1007 02:01:16.544102 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/value/kernel/m
I1007 02:01:16.544125 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/value/kernel/v
I1007 02:01:16.544147 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/value/kernel/v_col
I1007 02:01:16.544170 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/value/kernel/v_row
I1007 02:01:16.544193 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/mlp/wi_0/kernel/m
I1007 02:01:16.544215 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/mlp/wi_0/kernel/v
I1007 02:01:16.544238 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/mlp/wi_0/kernel/v_col
I1007 02:01:16.544261 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/mlp/wi_0/kernel/v_row
I1007 02:01:16.544284 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/mlp/wi_1/kernel/m
I1007 02:01:16.544306 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/mlp/wi_1/kernel/v
I1007 02:01:16.544329 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/mlp/wi_1/kernel/v_col
I1007 02:01:16.544352 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/mlp/wi_1/kernel/v_row
I1007 02:01:16.544374 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/mlp/wo/kernel/m
I1007 02:01:16.544397 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/mlp/wo/kernel/v
I1007 02:01:16.544422 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/mlp/wo/kernel/v_col
I1007 02:01:16.544445 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/mlp/wo/kernel/v_row
I1007 02:01:16.544468 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/pre_attention_layer_norm/scale/m
I1007 02:01:16.544491 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/pre_attention_layer_norm/scale/v
I1007 02:01:16.544513 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/pre_attention_layer_norm/scale/v_col
I1007 02:01:16.544536 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/pre_attention_layer_norm/scale/v_row
I1007 02:01:16.544559 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/pre_mlp_layer_norm/scale/m
I1007 02:01:16.544581 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/pre_mlp_layer_norm/scale/v
I1007 02:01:16.544604 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/pre_mlp_layer_norm/scale/v_col
I1007 02:01:16.544627 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_0/pre_mlp_layer_norm/scale/v_row
I1007 02:01:16.544650 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/attention/key/kernel/m
I1007 02:01:16.544672 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/attention/key/kernel/v
I1007 02:01:16.544696 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/attention/key/kernel/v_col
I1007 02:01:16.544719 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/attention/key/kernel/v_row
I1007 02:01:16.544741 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/attention/out/kernel/m
I1007 02:01:16.544764 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/attention/out/kernel/v
I1007 02:01:16.544787 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/attention/out/kernel/v_col
I1007 02:01:16.544810 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/attention/out/kernel/v_row
I1007 02:01:16.544832 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/attention/query/kernel/m
I1007 02:01:16.544859 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/attention/query/kernel/v
I1007 02:01:16.544882 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/attention/query/kernel/v_col
I1007 02:01:16.544905 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/attention/query/kernel/v_row
I1007 02:01:16.544927 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/attention/value/kernel/m
I1007 02:01:16.544950 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/attention/value/kernel/v
I1007 02:01:16.544972 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/attention/value/kernel/v_col
I1007 02:01:16.544995 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/attention/value/kernel/v_row
I1007 02:01:16.545017 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/mlp/wi_0/kernel/m
I1007 02:01:16.545039 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/mlp/wi_0/kernel/v
I1007 02:01:16.545065 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/mlp/wi_0/kernel/v_col
I1007 02:01:16.545088 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/mlp/wi_0/kernel/v_row
I1007 02:01:16.545111 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/mlp/wi_1/kernel/m
I1007 02:01:16.545134 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/mlp/wi_1/kernel/v
I1007 02:01:16.545156 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/mlp/wi_1/kernel/v_col
I1007 02:01:16.545179 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/mlp/wi_1/kernel/v_row
I1007 02:01:16.545201 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/mlp/wo/kernel/m
I1007 02:01:16.545224 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/mlp/wo/kernel/v
I1007 02:01:16.545247 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/mlp/wo/kernel/v_col
I1007 02:01:16.545269 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/mlp/wo/kernel/v_row
I1007 02:01:16.545291 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/pre_attention_layer_norm/scale/m
I1007 02:01:16.545314 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/pre_attention_layer_norm/scale/v
I1007 02:01:16.545336 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/pre_attention_layer_norm/scale/v_col
I1007 02:01:16.545359 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/pre_attention_layer_norm/scale/v_row
I1007 02:01:16.545381 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/pre_mlp_layer_norm/scale/m
I1007 02:01:16.545404 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/pre_mlp_layer_norm/scale/v
I1007 02:01:16.545427 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/pre_mlp_layer_norm/scale/v_col
I1007 02:01:16.545449 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_1/pre_mlp_layer_norm/scale/v_row
I1007 02:01:16.545472 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/attention/key/kernel/m
I1007 02:01:16.545495 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/attention/key/kernel/v
I1007 02:01:16.545517 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/attention/key/kernel/v_col
I1007 02:01:16.545540 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/attention/key/kernel/v_row
I1007 02:01:16.545563 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/attention/out/kernel/m
I1007 02:01:16.545585 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/attention/out/kernel/v
I1007 02:01:16.545608 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/attention/out/kernel/v_col
I1007 02:01:16.545630 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/attention/out/kernel/v_row
I1007 02:01:16.545653 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/attention/query/kernel/m
I1007 02:01:16.545676 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/attention/query/kernel/v
I1007 02:01:16.545701 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/attention/query/kernel/v_col
I1007 02:01:16.545724 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/attention/query/kernel/v_row
I1007 02:01:16.545747 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/attention/value/kernel/m
I1007 02:01:16.545770 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/attention/value/kernel/v
I1007 02:01:16.545792 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/attention/value/kernel/v_col
I1007 02:01:16.545815 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/attention/value/kernel/v_row
I1007 02:01:16.545838 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/mlp/wi_0/kernel/m
I1007 02:01:16.545865 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/mlp/wi_0/kernel/v
I1007 02:01:16.545887 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/mlp/wi_0/kernel/v_col
I1007 02:01:16.545910 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/mlp/wi_0/kernel/v_row
I1007 02:01:16.545933 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/mlp/wi_1/kernel/m
I1007 02:01:16.545956 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/mlp/wi_1/kernel/v
I1007 02:01:16.545979 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/mlp/wi_1/kernel/v_col
I1007 02:01:16.546001 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/mlp/wi_1/kernel/v_row
I1007 02:01:16.546024 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/mlp/wo/kernel/m
I1007 02:01:16.546047 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/mlp/wo/kernel/v
I1007 02:01:16.546070 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/mlp/wo/kernel/v_col
I1007 02:01:16.546092 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/mlp/wo/kernel/v_row
I1007 02:01:16.546115 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/pre_attention_layer_norm/scale/m
I1007 02:01:16.546138 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/pre_attention_layer_norm/scale/v
I1007 02:01:16.546160 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/pre_attention_layer_norm/scale/v_col
I1007 02:01:16.546183 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/pre_attention_layer_norm/scale/v_row
I1007 02:01:16.546206 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/pre_mlp_layer_norm/scale/m
I1007 02:01:16.546228 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/pre_mlp_layer_norm/scale/v
I1007 02:01:16.546251 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/pre_mlp_layer_norm/scale/v_col
I1007 02:01:16.546273 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_10/pre_mlp_layer_norm/scale/v_row
I1007 02:01:16.546295 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/attention/key/kernel/m
I1007 02:01:16.546318 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/attention/key/kernel/v
I1007 02:01:16.546344 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/attention/key/kernel/v_col
I1007 02:01:16.546367 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/attention/key/kernel/v_row
I1007 02:01:16.546389 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/attention/out/kernel/m
I1007 02:01:16.546412 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/attention/out/kernel/v
I1007 02:01:16.546435 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/attention/out/kernel/v_col
I1007 02:01:16.546457 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/attention/out/kernel/v_row
I1007 02:01:16.546480 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/attention/query/kernel/m
I1007 02:01:16.546502 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/attention/query/kernel/v
I1007 02:01:16.546525 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/attention/query/kernel/v_col
I1007 02:01:16.546548 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/attention/query/kernel/v_row
I1007 02:01:16.546570 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/attention/value/kernel/m
I1007 02:01:16.546592 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/attention/value/kernel/v
I1007 02:01:16.546615 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/attention/value/kernel/v_col
I1007 02:01:16.546637 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/attention/value/kernel/v_row
I1007 02:01:16.546659 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/mlp/wi_0/kernel/m
I1007 02:01:16.546682 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/mlp/wi_0/kernel/v
I1007 02:01:16.546705 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/mlp/wi_0/kernel/v_col
I1007 02:01:16.546727 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/mlp/wi_0/kernel/v_row
I1007 02:01:16.546749 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/mlp/wi_1/kernel/m
I1007 02:01:16.546772 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/mlp/wi_1/kernel/v
I1007 02:01:16.546794 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/mlp/wi_1/kernel/v_col
I1007 02:01:16.546827 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/mlp/wi_1/kernel/v_row
I1007 02:01:16.546852 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/mlp/wo/kernel/m
I1007 02:01:16.546875 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/mlp/wo/kernel/v
I1007 02:01:16.546898 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/mlp/wo/kernel/v_col
I1007 02:01:16.546921 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/mlp/wo/kernel/v_row
I1007 02:01:16.546943 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/pre_attention_layer_norm/scale/m
I1007 02:01:16.546966 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/pre_attention_layer_norm/scale/v
I1007 02:01:16.546993 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/pre_attention_layer_norm/scale/v_col
I1007 02:01:16.547016 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/pre_attention_layer_norm/scale/v_row
I1007 02:01:16.547039 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/pre_mlp_layer_norm/scale/m
I1007 02:01:16.547062 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/pre_mlp_layer_norm/scale/v
I1007 02:01:16.547085 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/pre_mlp_layer_norm/scale/v_col
I1007 02:01:16.547108 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_11/pre_mlp_layer_norm/scale/v_row
I1007 02:01:16.547131 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/attention/key/kernel/m
I1007 02:01:16.547154 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/attention/key/kernel/v
I1007 02:01:16.547177 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/attention/key/kernel/v_col
I1007 02:01:16.547200 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/attention/key/kernel/v_row
I1007 02:01:16.547223 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/attention/out/kernel/m
I1007 02:01:16.547246 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/attention/out/kernel/v
I1007 02:01:16.547269 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/attention/out/kernel/v_col
I1007 02:01:16.547292 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/attention/out/kernel/v_row
I1007 02:01:16.547315 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/attention/query/kernel/m
I1007 02:01:16.547338 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/attention/query/kernel/v
I1007 02:01:16.547360 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/attention/query/kernel/v_col
I1007 02:01:16.547383 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/attention/query/kernel/v_row
I1007 02:01:16.547405 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/attention/value/kernel/m
I1007 02:01:16.547428 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/attention/value/kernel/v
I1007 02:01:16.547451 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/attention/value/kernel/v_col
I1007 02:01:16.547474 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/attention/value/kernel/v_row
I1007 02:01:16.547497 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/mlp/wi_0/kernel/m
I1007 02:01:16.547519 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/mlp/wi_0/kernel/v
I1007 02:01:16.547542 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/mlp/wi_0/kernel/v_col
I1007 02:01:16.547565 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/mlp/wi_0/kernel/v_row
I1007 02:01:16.547588 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/mlp/wi_1/kernel/m
I1007 02:01:16.547611 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/mlp/wi_1/kernel/v
I1007 02:01:16.547637 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/mlp/wi_1/kernel/v_col
I1007 02:01:16.547661 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/mlp/wi_1/kernel/v_row
I1007 02:01:16.547684 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/mlp/wo/kernel/m
I1007 02:01:16.547707 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/mlp/wo/kernel/v
I1007 02:01:16.547730 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/mlp/wo/kernel/v_col
I1007 02:01:16.547752 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/mlp/wo/kernel/v_row
I1007 02:01:16.547775 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/pre_attention_layer_norm/scale/m
I1007 02:01:16.547799 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/pre_attention_layer_norm/scale/v
I1007 02:01:16.547823 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/pre_attention_layer_norm/scale/v_col
I1007 02:01:16.547850 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/pre_attention_layer_norm/scale/v_row
I1007 02:01:16.547874 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/pre_mlp_layer_norm/scale/m
I1007 02:01:16.547897 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/pre_mlp_layer_norm/scale/v
I1007 02:01:16.547921 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/pre_mlp_layer_norm/scale/v_col
I1007 02:01:16.547943 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_2/pre_mlp_layer_norm/scale/v_row
I1007 02:01:16.547966 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/attention/key/kernel/m
I1007 02:01:16.547989 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/attention/key/kernel/v
I1007 02:01:16.548012 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/attention/key/kernel/v_col
I1007 02:01:16.548035 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/attention/key/kernel/v_row
I1007 02:01:16.548058 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/attention/out/kernel/m
I1007 02:01:16.548081 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/attention/out/kernel/v
I1007 02:01:16.548104 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/attention/out/kernel/v_col
I1007 02:01:16.548126 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/attention/out/kernel/v_row
I1007 02:01:16.548149 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/attention/query/kernel/m
I1007 02:01:16.548172 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/attention/query/kernel/v
I1007 02:01:16.548195 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/attention/query/kernel/v_col
I1007 02:01:16.548217 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/attention/query/kernel/v_row
I1007 02:01:16.548240 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/attention/value/kernel/m
I1007 02:01:16.548262 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/attention/value/kernel/v
I1007 02:01:16.548290 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/attention/value/kernel/v_col
I1007 02:01:16.548314 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/attention/value/kernel/v_row
I1007 02:01:16.548336 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/mlp/wi_0/kernel/m
I1007 02:01:16.548359 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/mlp/wi_0/kernel/v
I1007 02:01:16.548382 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/mlp/wi_0/kernel/v_col
I1007 02:01:16.548405 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/mlp/wi_0/kernel/v_row
I1007 02:01:16.548428 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/mlp/wi_1/kernel/m
I1007 02:01:16.548450 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/mlp/wi_1/kernel/v
I1007 02:01:16.548473 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/mlp/wi_1/kernel/v_col
I1007 02:01:16.548496 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/mlp/wi_1/kernel/v_row
I1007 02:01:16.548518 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/mlp/wo/kernel/m
I1007 02:01:16.548541 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/mlp/wo/kernel/v
I1007 02:01:16.548564 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/mlp/wo/kernel/v_col
I1007 02:01:16.548586 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/mlp/wo/kernel/v_row
I1007 02:01:16.548609 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/pre_attention_layer_norm/scale/m
I1007 02:01:16.548632 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/pre_attention_layer_norm/scale/v
I1007 02:01:16.548655 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/pre_attention_layer_norm/scale/v_col
I1007 02:01:16.548678 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/pre_attention_layer_norm/scale/v_row
I1007 02:01:16.548701 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/pre_mlp_layer_norm/scale/m
I1007 02:01:16.548724 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/pre_mlp_layer_norm/scale/v
I1007 02:01:16.548747 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/pre_mlp_layer_norm/scale/v_col
I1007 02:01:16.548770 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_3/pre_mlp_layer_norm/scale/v_row
I1007 02:01:16.548793 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/attention/key/kernel/m
I1007 02:01:16.548816 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/attention/key/kernel/v
I1007 02:01:16.548840 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/attention/key/kernel/v_col
I1007 02:01:16.548866 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/attention/key/kernel/v_row
I1007 02:01:16.548889 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/attention/out/kernel/m
I1007 02:01:16.548913 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/attention/out/kernel/v
I1007 02:01:16.548935 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/attention/out/kernel/v_col
I1007 02:01:16.548962 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/attention/out/kernel/v_row
I1007 02:01:16.548985 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/attention/query/kernel/m
I1007 02:01:16.549008 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/attention/query/kernel/v
I1007 02:01:16.549031 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/attention/query/kernel/v_col
I1007 02:01:16.549054 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/attention/query/kernel/v_row
I1007 02:01:16.549077 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/attention/value/kernel/m
I1007 02:01:16.549099 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/attention/value/kernel/v
I1007 02:01:16.549122 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/attention/value/kernel/v_col
I1007 02:01:16.549144 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/attention/value/kernel/v_row
I1007 02:01:16.549167 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/mlp/wi_0/kernel/m
I1007 02:01:16.549190 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/mlp/wi_0/kernel/v
I1007 02:01:16.549213 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/mlp/wi_0/kernel/v_col
I1007 02:01:16.549236 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/mlp/wi_0/kernel/v_row
I1007 02:01:16.549259 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/mlp/wi_1/kernel/m
I1007 02:01:16.549281 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/mlp/wi_1/kernel/v
I1007 02:01:16.549304 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/mlp/wi_1/kernel/v_col
I1007 02:01:16.549326 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/mlp/wi_1/kernel/v_row
I1007 02:01:16.549349 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/mlp/wo/kernel/m
I1007 02:01:16.549371 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/mlp/wo/kernel/v
I1007 02:01:16.549394 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/mlp/wo/kernel/v_col
I1007 02:01:16.549416 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/mlp/wo/kernel/v_row
I1007 02:01:16.549439 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/pre_attention_layer_norm/scale/m
I1007 02:01:16.549462 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/pre_attention_layer_norm/scale/v
I1007 02:01:16.549484 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/pre_attention_layer_norm/scale/v_col
I1007 02:01:16.549507 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/pre_attention_layer_norm/scale/v_row
I1007 02:01:16.549530 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/pre_mlp_layer_norm/scale/m
I1007 02:01:16.549553 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/pre_mlp_layer_norm/scale/v
I1007 02:01:16.549576 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/pre_mlp_layer_norm/scale/v_col
I1007 02:01:16.549605 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_4/pre_mlp_layer_norm/scale/v_row
I1007 02:01:16.549629 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/attention/key/kernel/m
I1007 02:01:16.549670 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/attention/key/kernel/v
I1007 02:01:16.549697 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/attention/key/kernel/v_col
I1007 02:01:16.549724 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/attention/key/kernel/v_row
I1007 02:01:16.549751 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/attention/out/kernel/m
I1007 02:01:16.549778 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/attention/out/kernel/v
I1007 02:01:16.549806 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/attention/out/kernel/v_col
I1007 02:01:16.549833 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/attention/out/kernel/v_row
I1007 02:01:16.549864 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/attention/query/kernel/m
I1007 02:01:16.549891 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/attention/query/kernel/v
I1007 02:01:16.549918 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/attention/query/kernel/v_col
I1007 02:01:16.549945 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/attention/query/kernel/v_row
I1007 02:01:16.549972 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/attention/value/kernel/m
I1007 02:01:16.549999 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/attention/value/kernel/v
I1007 02:01:16.550025 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/attention/value/kernel/v_col
I1007 02:01:16.550052 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/attention/value/kernel/v_row
I1007 02:01:16.550079 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/mlp/wi_0/kernel/m
I1007 02:01:16.550106 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/mlp/wi_0/kernel/v
I1007 02:01:16.550133 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/mlp/wi_0/kernel/v_col
I1007 02:01:16.550160 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/mlp/wi_0/kernel/v_row
I1007 02:01:16.550187 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/mlp/wi_1/kernel/m
I1007 02:01:16.550213 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/mlp/wi_1/kernel/v
I1007 02:01:16.550240 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/mlp/wi_1/kernel/v_col
I1007 02:01:16.550267 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/mlp/wi_1/kernel/v_row
I1007 02:01:16.550294 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/mlp/wo/kernel/m
I1007 02:01:16.550321 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/mlp/wo/kernel/v
I1007 02:01:16.550348 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/mlp/wo/kernel/v_col
I1007 02:01:16.550379 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/mlp/wo/kernel/v_row
I1007 02:01:16.550407 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/pre_attention_layer_norm/scale/m
I1007 02:01:16.550434 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/pre_attention_layer_norm/scale/v
I1007 02:01:16.550461 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/pre_attention_layer_norm/scale/v_col
I1007 02:01:16.550488 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/pre_attention_layer_norm/scale/v_row
I1007 02:01:16.550515 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/pre_mlp_layer_norm/scale/m
I1007 02:01:16.550542 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/pre_mlp_layer_norm/scale/v
I1007 02:01:16.550569 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/pre_mlp_layer_norm/scale/v_col
I1007 02:01:16.550595 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_5/pre_mlp_layer_norm/scale/v_row
I1007 02:01:16.550627 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/attention/key/kernel/m
I1007 02:01:16.550655 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/attention/key/kernel/v
I1007 02:01:16.550682 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/attention/key/kernel/v_col
I1007 02:01:16.550709 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/attention/key/kernel/v_row
I1007 02:01:16.550735 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/attention/out/kernel/m
I1007 02:01:16.550761 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/attention/out/kernel/v
I1007 02:01:16.550788 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/attention/out/kernel/v_col
I1007 02:01:16.550837 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/attention/out/kernel/v_row
I1007 02:01:16.550869 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/attention/query/kernel/m
I1007 02:01:16.550896 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/attention/query/kernel/v
I1007 02:01:16.550923 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/attention/query/kernel/v_col
I1007 02:01:16.550950 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/attention/query/kernel/v_row
I1007 02:01:16.550977 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/attention/value/kernel/m
I1007 02:01:16.551004 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/attention/value/kernel/v
I1007 02:01:16.551031 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/attention/value/kernel/v_col
I1007 02:01:16.551057 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/attention/value/kernel/v_row
I1007 02:01:16.551084 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/mlp/wi_0/kernel/m
I1007 02:01:16.551111 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/mlp/wi_0/kernel/v
I1007 02:01:16.551138 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/mlp/wi_0/kernel/v_col
I1007 02:01:16.551169 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/mlp/wi_0/kernel/v_row
I1007 02:01:16.551196 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/mlp/wi_1/kernel/m
I1007 02:01:16.551223 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/mlp/wi_1/kernel/v
I1007 02:01:16.551250 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/mlp/wi_1/kernel/v_col
I1007 02:01:16.551276 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/mlp/wi_1/kernel/v_row
I1007 02:01:16.551303 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/mlp/wo/kernel/m
I1007 02:01:16.551330 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/mlp/wo/kernel/v
I1007 02:01:16.551356 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/mlp/wo/kernel/v_col
I1007 02:01:16.551383 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/mlp/wo/kernel/v_row
I1007 02:01:16.551410 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/pre_attention_layer_norm/scale/m
I1007 02:01:16.551437 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/pre_attention_layer_norm/scale/v
I1007 02:01:16.551464 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/pre_attention_layer_norm/scale/v_col
I1007 02:01:16.551491 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/pre_attention_layer_norm/scale/v_row
I1007 02:01:16.551518 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/pre_mlp_layer_norm/scale/m
I1007 02:01:16.551545 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/pre_mlp_layer_norm/scale/v
I1007 02:01:16.551571 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/pre_mlp_layer_norm/scale/v_col
I1007 02:01:16.551598 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_6/pre_mlp_layer_norm/scale/v_row
I1007 02:01:16.551625 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/attention/key/kernel/m
I1007 02:01:16.551652 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/attention/key/kernel/v
I1007 02:01:16.551679 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/attention/key/kernel/v_col
I1007 02:01:16.551706 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/attention/key/kernel/v_row
I1007 02:01:16.551733 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/attention/out/kernel/m
I1007 02:01:16.551759 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/attention/out/kernel/v
I1007 02:01:16.551795 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/attention/out/kernel/v_col
I1007 02:01:16.551818 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/attention/out/kernel/v_row
I1007 02:01:16.551841 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/attention/query/kernel/m
I1007 02:01:16.551868 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/attention/query/kernel/v
I1007 02:01:16.551891 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/attention/query/kernel/v_col
I1007 02:01:16.551917 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/attention/query/kernel/v_row
I1007 02:01:16.551940 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/attention/value/kernel/m
I1007 02:01:16.551963 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/attention/value/kernel/v
I1007 02:01:16.551986 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/attention/value/kernel/v_col
I1007 02:01:16.552008 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/attention/value/kernel/v_row
I1007 02:01:16.552031 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/mlp/wi_0/kernel/m
I1007 02:01:16.552054 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/mlp/wi_0/kernel/v
I1007 02:01:16.552077 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/mlp/wi_0/kernel/v_col
I1007 02:01:16.552100 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/mlp/wi_0/kernel/v_row
I1007 02:01:16.552123 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/mlp/wi_1/kernel/m
I1007 02:01:16.552145 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/mlp/wi_1/kernel/v
I1007 02:01:16.552168 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/mlp/wi_1/kernel/v_col
I1007 02:01:16.552191 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/mlp/wi_1/kernel/v_row
I1007 02:01:16.552213 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/mlp/wo/kernel/m
I1007 02:01:16.552236 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/mlp/wo/kernel/v
I1007 02:01:16.552258 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/mlp/wo/kernel/v_col
I1007 02:01:16.552281 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/mlp/wo/kernel/v_row
I1007 02:01:16.552304 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/pre_attention_layer_norm/scale/m
I1007 02:01:16.552327 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/pre_attention_layer_norm/scale/v
I1007 02:01:16.552350 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/pre_attention_layer_norm/scale/v_col
I1007 02:01:16.552372 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/pre_attention_layer_norm/scale/v_row
I1007 02:01:16.552395 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/pre_mlp_layer_norm/scale/m
I1007 02:01:16.552417 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/pre_mlp_layer_norm/scale/v
I1007 02:01:16.552440 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/pre_mlp_layer_norm/scale/v_col
I1007 02:01:16.552462 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_7/pre_mlp_layer_norm/scale/v_row
I1007 02:01:16.552485 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/attention/key/kernel/m
I1007 02:01:16.552507 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/attention/key/kernel/v
I1007 02:01:16.552530 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/attention/key/kernel/v_col
I1007 02:01:16.552553 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/attention/key/kernel/v_row
I1007 02:01:16.552578 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/attention/out/kernel/m
I1007 02:01:16.552601 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/attention/out/kernel/v
I1007 02:01:16.552624 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/attention/out/kernel/v_col
I1007 02:01:16.552646 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/attention/out/kernel/v_row
I1007 02:01:16.552669 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/attention/query/kernel/m
I1007 02:01:16.552691 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/attention/query/kernel/v
I1007 02:01:16.552713 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/attention/query/kernel/v_col
I1007 02:01:16.552736 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/attention/query/kernel/v_row
I1007 02:01:16.552759 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/attention/value/kernel/m
I1007 02:01:16.552781 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/attention/value/kernel/v
I1007 02:01:16.552804 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/attention/value/kernel/v_col
I1007 02:01:16.552826 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/attention/value/kernel/v_row
I1007 02:01:16.552852 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/mlp/wi_0/kernel/m
I1007 02:01:16.552875 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/mlp/wi_0/kernel/v
I1007 02:01:16.552897 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/mlp/wi_0/kernel/v_col
I1007 02:01:16.552920 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/mlp/wi_0/kernel/v_row
I1007 02:01:16.552942 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/mlp/wi_1/kernel/m
I1007 02:01:16.552964 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/mlp/wi_1/kernel/v
I1007 02:01:16.552987 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/mlp/wi_1/kernel/v_col
I1007 02:01:16.553009 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/mlp/wi_1/kernel/v_row
I1007 02:01:16.553031 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/mlp/wo/kernel/m
I1007 02:01:16.553053 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/mlp/wo/kernel/v
I1007 02:01:16.553076 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/mlp/wo/kernel/v_col
I1007 02:01:16.553098 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/mlp/wo/kernel/v_row
I1007 02:01:16.553120 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/pre_attention_layer_norm/scale/m
I1007 02:01:16.553143 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/pre_attention_layer_norm/scale/v
I1007 02:01:16.553165 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/pre_attention_layer_norm/scale/v_col
I1007 02:01:16.553187 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/pre_attention_layer_norm/scale/v_row
I1007 02:01:16.553216 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/pre_mlp_layer_norm/scale/m
I1007 02:01:16.553239 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/pre_mlp_layer_norm/scale/v
I1007 02:01:16.553262 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/pre_mlp_layer_norm/scale/v_col
I1007 02:01:16.553285 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_8/pre_mlp_layer_norm/scale/v_row
I1007 02:01:16.553308 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/attention/key/kernel/m
I1007 02:01:16.553330 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/attention/key/kernel/v
I1007 02:01:16.553353 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/attention/key/kernel/v_col
I1007 02:01:16.553375 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/attention/key/kernel/v_row
I1007 02:01:16.553398 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/attention/out/kernel/m
I1007 02:01:16.553420 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/attention/out/kernel/v
I1007 02:01:16.553442 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/attention/out/kernel/v_col
I1007 02:01:16.553465 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/attention/out/kernel/v_row
I1007 02:01:16.553487 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/attention/query/kernel/m
I1007 02:01:16.553510 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/attention/query/kernel/v
I1007 02:01:16.553532 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/attention/query/kernel/v_col
I1007 02:01:16.553555 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/attention/query/kernel/v_row
I1007 02:01:16.553577 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/attention/value/kernel/m
I1007 02:01:16.553599 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/attention/value/kernel/v
I1007 02:01:16.553621 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/attention/value/kernel/v_col
I1007 02:01:16.553643 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/attention/value/kernel/v_row
I1007 02:01:16.553666 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/mlp/wi_0/kernel/m
I1007 02:01:16.553689 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/mlp/wi_0/kernel/v
I1007 02:01:16.553712 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/mlp/wi_0/kernel/v_col
I1007 02:01:16.553735 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/mlp/wi_0/kernel/v_row
I1007 02:01:16.553758 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/mlp/wi_1/kernel/m
I1007 02:01:16.553780 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/mlp/wi_1/kernel/v
I1007 02:01:16.553802 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/mlp/wi_1/kernel/v_col
I1007 02:01:16.553825 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/mlp/wi_1/kernel/v_row
I1007 02:01:16.553855 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/mlp/wo/kernel/m
I1007 02:01:16.553879 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/mlp/wo/kernel/v
I1007 02:01:16.553902 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/mlp/wo/kernel/v_col
I1007 02:01:16.553925 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/mlp/wo/kernel/v_row
I1007 02:01:16.553947 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/pre_attention_layer_norm/scale/m
I1007 02:01:16.553970 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/pre_attention_layer_norm/scale/v
I1007 02:01:16.553992 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/pre_attention_layer_norm/scale/v_col
I1007 02:01:16.554015 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/pre_attention_layer_norm/scale/v_row
I1007 02:01:16.554038 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/pre_mlp_layer_norm/scale/m
I1007 02:01:16.554060 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/pre_mlp_layer_norm/scale/v
I1007 02:01:16.554082 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/pre_mlp_layer_norm/scale/v_col
I1007 02:01:16.554105 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/layers_9/pre_mlp_layer_norm/scale/v_row
I1007 02:01:16.554127 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/relpos_bias/rel_embedding/m
I1007 02:01:16.554150 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/relpos_bias/rel_embedding/v
I1007 02:01:16.554172 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/relpos_bias/rel_embedding/v_col
I1007 02:01:16.554195 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/encoder/relpos_bias/rel_embedding/v_row
I1007 02:01:16.554217 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/m
I1007 02:01:16.554239 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v
I1007 02:01:16.554262 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_col
I1007 02:01:16.554284 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/param_states/token_embedder/embedding/v_row
I1007 02:01:16.554307 140317719510080 checkpoints.py:1100] Restoring key from ckpt: state/step
I1007 02:01:16.554330 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/decoder_norm/scale
I1007 02:01:16.554352 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/encoder_decoder_attention/key/kernel
I1007 02:01:16.554375 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/encoder_decoder_attention/out/kernel
I1007 02:01:16.554398 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/encoder_decoder_attention/query/kernel
I1007 02:01:16.554420 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/encoder_decoder_attention/value/kernel
I1007 02:01:16.554443 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_0/kernel
I1007 02:01:16.554465 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wi_1/kernel
I1007 02:01:16.554488 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/mlp/wo/kernel
I1007 02:01:16.554510 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_cross_attention_layer_norm/scale
I1007 02:01:16.554537 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_mlp_layer_norm/scale
I1007 02:01:16.554560 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/pre_self_attention_layer_norm/scale
I1007 02:01:16.554583 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/key/kernel
I1007 02:01:16.554605 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/out/kernel
I1007 02:01:16.554628 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/query/kernel
I1007 02:01:16.554651 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_0/self_attention/value/kernel
I1007 02:01:16.554673 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/encoder_decoder_attention/key/kernel
I1007 02:01:16.554696 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/encoder_decoder_attention/out/kernel
I1007 02:01:16.554719 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/encoder_decoder_attention/query/kernel
I1007 02:01:16.554741 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/encoder_decoder_attention/value/kernel
I1007 02:01:16.554764 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_0/kernel
I1007 02:01:16.554787 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wi_1/kernel
I1007 02:01:16.554819 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/mlp/wo/kernel
I1007 02:01:16.554842 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_cross_attention_layer_norm/scale
I1007 02:01:16.554869 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_mlp_layer_norm/scale
I1007 02:01:16.554892 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/pre_self_attention_layer_norm/scale
I1007 02:01:16.554915 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/key/kernel
I1007 02:01:16.554938 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/out/kernel
I1007 02:01:16.554961 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/query/kernel
I1007 02:01:16.554983 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_1/self_attention/value/kernel
I1007 02:01:16.555006 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/encoder_decoder_attention/key/kernel
I1007 02:01:16.555028 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/encoder_decoder_attention/out/kernel
I1007 02:01:16.555051 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/encoder_decoder_attention/query/kernel
I1007 02:01:16.555073 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/encoder_decoder_attention/value/kernel
I1007 02:01:16.555095 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_0/kernel
I1007 02:01:16.555118 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wi_1/kernel
I1007 02:01:16.555140 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/mlp/wo/kernel
I1007 02:01:16.555163 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_cross_attention_layer_norm/scale
I1007 02:01:16.555186 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_mlp_layer_norm/scale
I1007 02:01:16.555208 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/pre_self_attention_layer_norm/scale
I1007 02:01:16.555235 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/key/kernel
I1007 02:01:16.555258 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/out/kernel
I1007 02:01:16.555280 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/query/kernel
I1007 02:01:16.555303 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_10/self_attention/value/kernel
I1007 02:01:16.555325 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/encoder_decoder_attention/key/kernel
I1007 02:01:16.555348 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/encoder_decoder_attention/out/kernel
I1007 02:01:16.555371 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/encoder_decoder_attention/query/kernel
I1007 02:01:16.555394 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/encoder_decoder_attention/value/kernel
I1007 02:01:16.555416 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/wi_0/kernel
I1007 02:01:16.555438 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/wi_1/kernel
I1007 02:01:16.555461 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/mlp/wo/kernel
I1007 02:01:16.555484 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_cross_attention_layer_norm/scale
I1007 02:01:16.555506 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_mlp_layer_norm/scale
I1007 02:01:16.555528 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/pre_self_attention_layer_norm/scale
I1007 02:01:16.555551 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/key/kernel
I1007 02:01:16.555574 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/out/kernel
I1007 02:01:16.555597 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/query/kernel
I1007 02:01:16.555619 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_11/self_attention/value/kernel
I1007 02:01:16.555642 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/encoder_decoder_attention/key/kernel
I1007 02:01:16.555664 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/encoder_decoder_attention/out/kernel
I1007 02:01:16.555687 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/encoder_decoder_attention/query/kernel
I1007 02:01:16.555709 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/encoder_decoder_attention/value/kernel
I1007 02:01:16.555732 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_0/kernel
I1007 02:01:16.555753 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wi_1/kernel
I1007 02:01:16.555775 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/mlp/wo/kernel
I1007 02:01:16.555798 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_cross_attention_layer_norm/scale
I1007 02:01:16.555820 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_mlp_layer_norm/scale
I1007 02:01:16.555842 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/pre_self_attention_layer_norm/scale
I1007 02:01:16.555870 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/key/kernel
I1007 02:01:16.555896 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/out/kernel
I1007 02:01:16.555919 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/query/kernel
I1007 02:01:16.555941 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_2/self_attention/value/kernel
I1007 02:01:16.555963 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/encoder_decoder_attention/key/kernel
I1007 02:01:16.555986 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/encoder_decoder_attention/out/kernel
I1007 02:01:16.556009 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/encoder_decoder_attention/query/kernel
I1007 02:01:16.556031 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/encoder_decoder_attention/value/kernel
I1007 02:01:16.556054 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_0/kernel
I1007 02:01:16.556076 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wi_1/kernel
I1007 02:01:16.556099 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/mlp/wo/kernel
I1007 02:01:16.556122 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_cross_attention_layer_norm/scale
I1007 02:01:16.556144 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_mlp_layer_norm/scale
I1007 02:01:16.556167 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/pre_self_attention_layer_norm/scale
I1007 02:01:16.556189 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/key/kernel
I1007 02:01:16.556211 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/out/kernel
I1007 02:01:16.556234 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/query/kernel
I1007 02:01:16.556256 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_3/self_attention/value/kernel
I1007 02:01:16.556278 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/encoder_decoder_attention/key/kernel
I1007 02:01:16.556301 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/encoder_decoder_attention/out/kernel
I1007 02:01:16.556323 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/encoder_decoder_attention/query/kernel
I1007 02:01:16.556346 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/encoder_decoder_attention/value/kernel
I1007 02:01:16.556368 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_0/kernel
I1007 02:01:16.556391 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wi_1/kernel
I1007 02:01:16.556413 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/mlp/wo/kernel
I1007 02:01:16.556435 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_cross_attention_layer_norm/scale
I1007 02:01:16.556457 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_mlp_layer_norm/scale
I1007 02:01:16.556480 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/pre_self_attention_layer_norm/scale
I1007 02:01:16.556503 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/key/kernel
I1007 02:01:16.556525 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/out/kernel
I1007 02:01:16.556547 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/query/kernel
I1007 02:01:16.556573 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_4/self_attention/value/kernel
I1007 02:01:16.556596 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/encoder_decoder_attention/key/kernel
I1007 02:01:16.556618 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/encoder_decoder_attention/out/kernel
I1007 02:01:16.556641 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/encoder_decoder_attention/query/kernel
I1007 02:01:16.556663 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/encoder_decoder_attention/value/kernel
I1007 02:01:16.556685 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/wi_0/kernel
I1007 02:01:16.556707 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/wi_1/kernel
I1007 02:01:16.556730 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/mlp/wo/kernel
I1007 02:01:16.556752 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_cross_attention_layer_norm/scale
I1007 02:01:16.556774 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_mlp_layer_norm/scale
I1007 02:01:16.556796 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/pre_self_attention_layer_norm/scale
I1007 02:01:16.556818 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/key/kernel
I1007 02:01:16.556840 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/out/kernel
I1007 02:01:16.556866 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/query/kernel
I1007 02:01:16.556888 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_5/self_attention/value/kernel
I1007 02:01:16.556911 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/encoder_decoder_attention/key/kernel
I1007 02:01:16.556933 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/encoder_decoder_attention/out/kernel
I1007 02:01:16.556956 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/encoder_decoder_attention/query/kernel
I1007 02:01:16.556978 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/encoder_decoder_attention/value/kernel
I1007 02:01:16.557001 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_0/kernel
I1007 02:01:16.557024 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wi_1/kernel
I1007 02:01:16.557046 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/mlp/wo/kernel
I1007 02:01:16.557068 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_cross_attention_layer_norm/scale
I1007 02:01:16.557091 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_mlp_layer_norm/scale
I1007 02:01:16.557114 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/pre_self_attention_layer_norm/scale
I1007 02:01:16.557137 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/key/kernel
I1007 02:01:16.557159 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/out/kernel
I1007 02:01:16.557181 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/query/kernel
I1007 02:01:16.557204 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_6/self_attention/value/kernel
I1007 02:01:16.557226 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/encoder_decoder_attention/key/kernel
I1007 02:01:16.557252 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/encoder_decoder_attention/out/kernel
I1007 02:01:16.557276 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/encoder_decoder_attention/query/kernel
I1007 02:01:16.557298 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/encoder_decoder_attention/value/kernel
I1007 02:01:16.557321 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_0/kernel
I1007 02:01:16.557343 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wi_1/kernel
I1007 02:01:16.557366 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/mlp/wo/kernel
I1007 02:01:16.557389 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_cross_attention_layer_norm/scale
I1007 02:01:16.557411 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_mlp_layer_norm/scale
I1007 02:01:16.557433 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/pre_self_attention_layer_norm/scale
I1007 02:01:16.557455 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/key/kernel
I1007 02:01:16.557478 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/out/kernel
I1007 02:01:16.557501 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/query/kernel
I1007 02:01:16.557523 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_7/self_attention/value/kernel
I1007 02:01:16.557546 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/encoder_decoder_attention/key/kernel
I1007 02:01:16.557568 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/encoder_decoder_attention/out/kernel
I1007 02:01:16.557590 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/encoder_decoder_attention/query/kernel
I1007 02:01:16.557613 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/encoder_decoder_attention/value/kernel
I1007 02:01:16.557635 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_0/kernel
I1007 02:01:16.557657 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wi_1/kernel
I1007 02:01:16.557679 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/mlp/wo/kernel
I1007 02:01:16.557701 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_cross_attention_layer_norm/scale
I1007 02:01:16.557723 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_mlp_layer_norm/scale
I1007 02:01:16.557746 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/pre_self_attention_layer_norm/scale
I1007 02:01:16.557768 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/key/kernel
I1007 02:01:16.557790 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/out/kernel
I1007 02:01:16.557813 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/query/kernel
I1007 02:01:16.557835 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_8/self_attention/value/kernel
I1007 02:01:16.557861 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/encoder_decoder_attention/key/kernel
I1007 02:01:16.557883 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/encoder_decoder_attention/out/kernel
I1007 02:01:16.557906 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/encoder_decoder_attention/query/kernel
I1007 02:01:16.557931 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/encoder_decoder_attention/value/kernel
I1007 02:01:16.557954 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_0/kernel
I1007 02:01:16.557976 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wi_1/kernel
I1007 02:01:16.557999 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/mlp/wo/kernel
I1007 02:01:16.558021 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_cross_attention_layer_norm/scale
I1007 02:01:16.558043 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_mlp_layer_norm/scale
I1007 02:01:16.558066 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/pre_self_attention_layer_norm/scale
I1007 02:01:16.558089 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/key/kernel
I1007 02:01:16.558111 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/out/kernel
I1007 02:01:16.558134 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/query/kernel
I1007 02:01:16.558156 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/layers_9/self_attention/value/kernel
I1007 02:01:16.558179 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/logits_dense/kernel
I1007 02:01:16.558201 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/decoder/relpos_bias/rel_embedding
I1007 02:01:16.558224 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/encoder_norm/scale
I1007 02:01:16.558246 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_0/attention/key/kernel
I1007 02:01:16.558269 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_0/attention/out/kernel
I1007 02:01:16.558291 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_0/attention/query/kernel
I1007 02:01:16.558314 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_0/attention/value/kernel
I1007 02:01:16.558336 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_0/mlp/wi_0/kernel
I1007 02:01:16.558359 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_0/mlp/wi_1/kernel
I1007 02:01:16.558381 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_0/mlp/wo/kernel
I1007 02:01:16.558404 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_0/pre_attention_layer_norm/scale
I1007 02:01:16.558427 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_0/pre_mlp_layer_norm/scale
I1007 02:01:16.558449 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_1/attention/key/kernel
I1007 02:01:16.558472 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_1/attention/out/kernel
I1007 02:01:16.558494 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_1/attention/query/kernel
I1007 02:01:16.558517 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_1/attention/value/kernel
I1007 02:01:16.558540 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_1/mlp/wi_0/kernel
I1007 02:01:16.558562 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_1/mlp/wi_1/kernel
I1007 02:01:16.558585 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_1/mlp/wo/kernel
I1007 02:01:16.558608 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_1/pre_attention_layer_norm/scale
I1007 02:01:16.558635 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_1/pre_mlp_layer_norm/scale
I1007 02:01:16.558659 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_10/attention/key/kernel
I1007 02:01:16.558681 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_10/attention/out/kernel
I1007 02:01:16.558704 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_10/attention/query/kernel
I1007 02:01:16.558727 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_10/attention/value/kernel
I1007 02:01:16.558749 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_10/mlp/wi_0/kernel
I1007 02:01:16.558773 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_10/mlp/wi_1/kernel
I1007 02:01:16.558795 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_10/mlp/wo/kernel
I1007 02:01:16.558831 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_10/pre_attention_layer_norm/scale
I1007 02:01:16.558861 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_10/pre_mlp_layer_norm/scale
I1007 02:01:16.558884 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_11/attention/key/kernel
I1007 02:01:16.558907 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_11/attention/out/kernel
I1007 02:01:16.558954 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_11/attention/query/kernel
I1007 02:01:16.558997 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_11/attention/value/kernel
I1007 02:01:16.559055 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_11/mlp/wi_0/kernel
I1007 02:01:16.559091 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_11/mlp/wi_1/kernel
I1007 02:01:16.559133 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_11/mlp/wo/kernel
I1007 02:01:16.559160 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_11/pre_attention_layer_norm/scale
I1007 02:01:16.559185 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_11/pre_mlp_layer_norm/scale
I1007 02:01:16.559208 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_2/attention/key/kernel
I1007 02:01:16.559232 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_2/attention/out/kernel
I1007 02:01:16.559255 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_2/attention/query/kernel
I1007 02:01:16.559278 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_2/attention/value/kernel
I1007 02:01:16.559301 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_2/mlp/wi_0/kernel
I1007 02:01:16.559324 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_2/mlp/wi_1/kernel
I1007 02:01:16.559347 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_2/mlp/wo/kernel
I1007 02:01:16.559370 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_2/pre_attention_layer_norm/scale
I1007 02:01:16.559394 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_2/pre_mlp_layer_norm/scale
I1007 02:01:16.559417 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_3/attention/key/kernel
I1007 02:01:16.559440 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_3/attention/out/kernel
I1007 02:01:16.559463 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_3/attention/query/kernel
I1007 02:01:16.559486 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_3/attention/value/kernel
I1007 02:01:16.559515 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_3/mlp/wi_0/kernel
I1007 02:01:16.559539 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_3/mlp/wi_1/kernel
I1007 02:01:16.559562 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_3/mlp/wo/kernel
I1007 02:01:16.559585 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_3/pre_attention_layer_norm/scale
I1007 02:01:16.559614 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_3/pre_mlp_layer_norm/scale
I1007 02:01:16.559637 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_4/attention/key/kernel
I1007 02:01:16.559681 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_4/attention/out/kernel
I1007 02:01:16.559708 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_4/attention/query/kernel
I1007 02:01:16.559734 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_4/attention/value/kernel
I1007 02:01:16.559761 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_4/mlp/wi_0/kernel
I1007 02:01:16.559788 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_4/mlp/wi_1/kernel
I1007 02:01:16.559815 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_4/mlp/wo/kernel
I1007 02:01:16.559842 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_4/pre_attention_layer_norm/scale
I1007 02:01:16.559875 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_4/pre_mlp_layer_norm/scale
I1007 02:01:16.559903 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_5/attention/key/kernel
I1007 02:01:16.559930 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_5/attention/out/kernel
I1007 02:01:16.559957 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_5/attention/query/kernel
I1007 02:01:16.559983 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_5/attention/value/kernel
I1007 02:01:16.560010 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_5/mlp/wi_0/kernel
I1007 02:01:16.560038 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_5/mlp/wi_1/kernel
I1007 02:01:16.560069 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_5/mlp/wo/kernel
I1007 02:01:16.560097 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_5/pre_attention_layer_norm/scale
I1007 02:01:16.560124 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_5/pre_mlp_layer_norm/scale
I1007 02:01:16.560152 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_6/attention/key/kernel
I1007 02:01:16.560178 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_6/attention/out/kernel
I1007 02:01:16.560205 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_6/attention/query/kernel
I1007 02:01:16.560232 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_6/attention/value/kernel
I1007 02:01:16.560260 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_6/mlp/wi_0/kernel
I1007 02:01:16.560287 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_6/mlp/wi_1/kernel
I1007 02:01:16.560315 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_6/mlp/wo/kernel
I1007 02:01:16.560342 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_6/pre_attention_layer_norm/scale
I1007 02:01:16.560369 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_6/pre_mlp_layer_norm/scale
I1007 02:01:16.560401 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_7/attention/key/kernel
I1007 02:01:16.560429 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_7/attention/out/kernel
I1007 02:01:16.560456 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_7/attention/query/kernel
I1007 02:01:16.560483 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_7/attention/value/kernel
I1007 02:01:16.560510 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_7/mlp/wi_0/kernel
I1007 02:01:16.560537 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_7/mlp/wi_1/kernel
I1007 02:01:16.560564 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_7/mlp/wo/kernel
I1007 02:01:16.560590 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_7/pre_attention_layer_norm/scale
I1007 02:01:16.560621 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_7/pre_mlp_layer_norm/scale
I1007 02:01:16.560648 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_8/attention/key/kernel
I1007 02:01:16.560675 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_8/attention/out/kernel
I1007 02:01:16.560702 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_8/attention/query/kernel
I1007 02:01:16.560729 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_8/attention/value/kernel
I1007 02:01:16.560756 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_8/mlp/wi_0/kernel
I1007 02:01:16.560783 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_8/mlp/wi_1/kernel
I1007 02:01:16.560810 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_8/mlp/wo/kernel
I1007 02:01:16.560837 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_8/pre_attention_layer_norm/scale
I1007 02:01:16.560870 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_8/pre_mlp_layer_norm/scale
I1007 02:01:16.560897 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_9/attention/key/kernel
I1007 02:01:16.560924 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_9/attention/out/kernel
I1007 02:01:16.560951 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_9/attention/query/kernel
I1007 02:01:16.560978 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_9/attention/value/kernel
I1007 02:01:16.561005 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_9/mlp/wi_0/kernel
I1007 02:01:16.561033 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_9/mlp/wi_1/kernel
I1007 02:01:16.561060 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_9/mlp/wo/kernel
I1007 02:01:16.561087 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_9/pre_attention_layer_norm/scale
I1007 02:01:16.561114 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/layers_9/pre_mlp_layer_norm/scale
I1007 02:01:16.561142 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/encoder/relpos_bias/rel_embedding
I1007 02:01:16.561169 140317719510080 checkpoints.py:1100] Restoring key from ckpt: target/token_embedder/embedding
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1696644076.586918   74716 gcs_resource.cc:99] Using default AdmissionQueue with limit 32
I0000 00:00:1696644076.588440   76299 google_auth_provider.cc:179] Running on GCE, using service account 11821746030-compute@developer.gserviceaccount.com
I1007 02:01:22.794179 140317719510080 train.py:422] Initialize/restore complete (9.45 seconds).
I1007 02:01:22.887289 140317719510080 utils.py:1364] Variable decoder/decoder_norm/scale                                                       size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.887572 140317719510080 utils.py:1364] Variable decoder/layers_0/encoder_decoder_attention/key/kernel                            size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.887639 140317719510080 utils.py:1364] Variable decoder/layers_0/encoder_decoder_attention/out/kernel                            size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.887679 140317719510080 utils.py:1364] Variable decoder/layers_0/encoder_decoder_attention/query/kernel                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.887716 140317719510080 utils.py:1364] Variable decoder/layers_0/encoder_decoder_attention/value/kernel                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.887751 140317719510080 utils.py:1364] Variable decoder/layers_0/mlp/wi_0/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.887785 140317719510080 utils.py:1364] Variable decoder/layers_0/mlp/wi_1/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.887823 140317719510080 utils.py:1364] Variable decoder/layers_0/mlp/wo/kernel                                                   size 1572864      shape (mlp=2048, embed=768)                    partition spec ('model', None)
I1007 02:01:22.887860 140317719510080 utils.py:1364] Variable decoder/layers_0/pre_cross_attention_layer_norm/scale                            size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.887903 140317719510080 utils.py:1364] Variable decoder/layers_0/pre_mlp_layer_norm/scale                                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.887937 140317719510080 utils.py:1364] Variable decoder/layers_0/pre_self_attention_layer_norm/scale                             size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.887969 140317719510080 utils.py:1364] Variable decoder/layers_0/self_attention/key/kernel                                       size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.888002 140317719510080 utils.py:1364] Variable decoder/layers_0/self_attention/out/kernel                                       size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.888035 140317719510080 utils.py:1364] Variable decoder/layers_0/self_attention/query/kernel                                     size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.888068 140317719510080 utils.py:1364] Variable decoder/layers_0/self_attention/value/kernel                                     size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.888100 140317719510080 utils.py:1364] Variable decoder/layers_1/encoder_decoder_attention/key/kernel                            size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.888134 140317719510080 utils.py:1364] Variable decoder/layers_1/encoder_decoder_attention/out/kernel                            size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.888179 140317719510080 utils.py:1364] Variable decoder/layers_1/encoder_decoder_attention/query/kernel                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.888212 140317719510080 utils.py:1364] Variable decoder/layers_1/encoder_decoder_attention/value/kernel                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.888244 140317719510080 utils.py:1364] Variable decoder/layers_1/mlp/wi_0/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.888277 140317719510080 utils.py:1364] Variable decoder/layers_1/mlp/wi_1/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.888309 140317719510080 utils.py:1364] Variable decoder/layers_1/mlp/wo/kernel                                                   size 1572864      shape (mlp=2048, embed=768)                    partition spec ('model', None)
I1007 02:01:22.888340 140317719510080 utils.py:1364] Variable decoder/layers_1/pre_cross_attention_layer_norm/scale                            size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.888372 140317719510080 utils.py:1364] Variable decoder/layers_1/pre_mlp_layer_norm/scale                                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.888404 140317719510080 utils.py:1364] Variable decoder/layers_1/pre_self_attention_layer_norm/scale                             size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.888435 140317719510080 utils.py:1364] Variable decoder/layers_1/self_attention/key/kernel                                       size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.888467 140317719510080 utils.py:1364] Variable decoder/layers_1/self_attention/out/kernel                                       size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.888499 140317719510080 utils.py:1364] Variable decoder/layers_1/self_attention/query/kernel                                     size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.888531 140317719510080 utils.py:1364] Variable decoder/layers_1/self_attention/value/kernel                                     size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.888563 140317719510080 utils.py:1364] Variable decoder/layers_10/encoder_decoder_attention/key/kernel                           size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.888596 140317719510080 utils.py:1364] Variable decoder/layers_10/encoder_decoder_attention/out/kernel                           size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.888628 140317719510080 utils.py:1364] Variable decoder/layers_10/encoder_decoder_attention/query/kernel                         size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.888659 140317719510080 utils.py:1364] Variable decoder/layers_10/encoder_decoder_attention/value/kernel                         size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.888691 140317719510080 utils.py:1364] Variable decoder/layers_10/mlp/wi_0/kernel                                                size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.888732 140317719510080 utils.py:1364] Variable decoder/layers_10/mlp/wi_1/kernel                                                size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.888766 140317719510080 utils.py:1364] Variable decoder/layers_10/mlp/wo/kernel                                                  size 1572864      shape (mlp=2048, embed=768)                    partition spec ('model', None)
I1007 02:01:22.888799 140317719510080 utils.py:1364] Variable decoder/layers_10/pre_cross_attention_layer_norm/scale                           size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.888831 140317719510080 utils.py:1364] Variable decoder/layers_10/pre_mlp_layer_norm/scale                                       size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.888863 140317719510080 utils.py:1364] Variable decoder/layers_10/pre_self_attention_layer_norm/scale                            size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.888900 140317719510080 utils.py:1364] Variable decoder/layers_10/self_attention/key/kernel                                      size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.888934 140317719510080 utils.py:1364] Variable decoder/layers_10/self_attention/out/kernel                                      size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.888966 140317719510080 utils.py:1364] Variable decoder/layers_10/self_attention/query/kernel                                    size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.888998 140317719510080 utils.py:1364] Variable decoder/layers_10/self_attention/value/kernel                                    size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.889030 140317719510080 utils.py:1364] Variable decoder/layers_11/encoder_decoder_attention/key/kernel                           size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.889063 140317719510080 utils.py:1364] Variable decoder/layers_11/encoder_decoder_attention/out/kernel                           size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.889128 140317719510080 utils.py:1364] Variable decoder/layers_11/encoder_decoder_attention/query/kernel                         size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.889162 140317719510080 utils.py:1364] Variable decoder/layers_11/encoder_decoder_attention/value/kernel                         size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.889194 140317719510080 utils.py:1364] Variable decoder/layers_11/mlp/wi_0/kernel                                                size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.889226 140317719510080 utils.py:1364] Variable decoder/layers_11/mlp/wi_1/kernel                                                size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.889258 140317719510080 utils.py:1364] Variable decoder/layers_11/mlp/wo/kernel                                                  size 1572864      shape (mlp=2048, embed=768)                    partition spec ('model', None)
I1007 02:01:22.889291 140317719510080 utils.py:1364] Variable decoder/layers_11/pre_cross_attention_layer_norm/scale                           size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.889323 140317719510080 utils.py:1364] Variable decoder/layers_11/pre_mlp_layer_norm/scale                                       size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.889358 140317719510080 utils.py:1364] Variable decoder/layers_11/pre_self_attention_layer_norm/scale                            size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.889390 140317719510080 utils.py:1364] Variable decoder/layers_11/self_attention/key/kernel                                      size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.889422 140317719510080 utils.py:1364] Variable decoder/layers_11/self_attention/out/kernel                                      size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.889454 140317719510080 utils.py:1364] Variable decoder/layers_11/self_attention/query/kernel                                    size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.889486 140317719510080 utils.py:1364] Variable decoder/layers_11/self_attention/value/kernel                                    size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.889517 140317719510080 utils.py:1364] Variable decoder/layers_2/encoder_decoder_attention/key/kernel                            size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.889549 140317719510080 utils.py:1364] Variable decoder/layers_2/encoder_decoder_attention/out/kernel                            size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.889582 140317719510080 utils.py:1364] Variable decoder/layers_2/encoder_decoder_attention/query/kernel                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.889614 140317719510080 utils.py:1364] Variable decoder/layers_2/encoder_decoder_attention/value/kernel                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.889646 140317719510080 utils.py:1364] Variable decoder/layers_2/mlp/wi_0/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.889678 140317719510080 utils.py:1364] Variable decoder/layers_2/mlp/wi_1/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.889710 140317719510080 utils.py:1364] Variable decoder/layers_2/mlp/wo/kernel                                                   size 1572864      shape (mlp=2048, embed=768)                    partition spec ('model', None)
I1007 02:01:22.889742 140317719510080 utils.py:1364] Variable decoder/layers_2/pre_cross_attention_layer_norm/scale                            size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.889774 140317719510080 utils.py:1364] Variable decoder/layers_2/pre_mlp_layer_norm/scale                                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.889806 140317719510080 utils.py:1364] Variable decoder/layers_2/pre_self_attention_layer_norm/scale                             size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.889837 140317719510080 utils.py:1364] Variable decoder/layers_2/self_attention/key/kernel                                       size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.889870 140317719510080 utils.py:1364] Variable decoder/layers_2/self_attention/out/kernel                                       size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.889913 140317719510080 utils.py:1364] Variable decoder/layers_2/self_attention/query/kernel                                     size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.889947 140317719510080 utils.py:1364] Variable decoder/layers_2/self_attention/value/kernel                                     size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.889979 140317719510080 utils.py:1364] Variable decoder/layers_3/encoder_decoder_attention/key/kernel                            size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.890012 140317719510080 utils.py:1364] Variable decoder/layers_3/encoder_decoder_attention/out/kernel                            size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.890043 140317719510080 utils.py:1364] Variable decoder/layers_3/encoder_decoder_attention/query/kernel                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.890076 140317719510080 utils.py:1364] Variable decoder/layers_3/encoder_decoder_attention/value/kernel                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.890110 140317719510080 utils.py:1364] Variable decoder/layers_3/mlp/wi_0/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.890144 140317719510080 utils.py:1364] Variable decoder/layers_3/mlp/wi_1/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.890198 140317719510080 utils.py:1364] Variable decoder/layers_3/mlp/wo/kernel                                                   size 1572864      shape (mlp=2048, embed=768)                    partition spec ('model', None)
I1007 02:01:22.890235 140317719510080 utils.py:1364] Variable decoder/layers_3/pre_cross_attention_layer_norm/scale                            size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.890273 140317719510080 utils.py:1364] Variable decoder/layers_3/pre_mlp_layer_norm/scale                                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.890310 140317719510080 utils.py:1364] Variable decoder/layers_3/pre_self_attention_layer_norm/scale                             size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.890347 140317719510080 utils.py:1364] Variable decoder/layers_3/self_attention/key/kernel                                       size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.890385 140317719510080 utils.py:1364] Variable decoder/layers_3/self_attention/out/kernel                                       size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.890423 140317719510080 utils.py:1364] Variable decoder/layers_3/self_attention/query/kernel                                     size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.890460 140317719510080 utils.py:1364] Variable decoder/layers_3/self_attention/value/kernel                                     size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.890498 140317719510080 utils.py:1364] Variable decoder/layers_4/encoder_decoder_attention/key/kernel                            size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.890543 140317719510080 utils.py:1364] Variable decoder/layers_4/encoder_decoder_attention/out/kernel                            size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.890582 140317719510080 utils.py:1364] Variable decoder/layers_4/encoder_decoder_attention/query/kernel                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.890619 140317719510080 utils.py:1364] Variable decoder/layers_4/encoder_decoder_attention/value/kernel                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.890657 140317719510080 utils.py:1364] Variable decoder/layers_4/mlp/wi_0/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.890708 140317719510080 utils.py:1364] Variable decoder/layers_4/mlp/wi_1/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.890748 140317719510080 utils.py:1364] Variable decoder/layers_4/mlp/wo/kernel                                                   size 1572864      shape (mlp=2048, embed=768)                    partition spec ('model', None)
I1007 02:01:22.890787 140317719510080 utils.py:1364] Variable decoder/layers_4/pre_cross_attention_layer_norm/scale                            size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.890864 140317719510080 utils.py:1364] Variable decoder/layers_4/pre_mlp_layer_norm/scale                                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.890908 140317719510080 utils.py:1364] Variable decoder/layers_4/pre_self_attention_layer_norm/scale                             size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.890947 140317719510080 utils.py:1364] Variable decoder/layers_4/self_attention/key/kernel                                       size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.890986 140317719510080 utils.py:1364] Variable decoder/layers_4/self_attention/out/kernel                                       size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.891024 140317719510080 utils.py:1364] Variable decoder/layers_4/self_attention/query/kernel                                     size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.891063 140317719510080 utils.py:1364] Variable decoder/layers_4/self_attention/value/kernel                                     size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.891100 140317719510080 utils.py:1364] Variable decoder/layers_5/encoder_decoder_attention/key/kernel                            size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.891145 140317719510080 utils.py:1364] Variable decoder/layers_5/encoder_decoder_attention/out/kernel                            size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.891184 140317719510080 utils.py:1364] Variable decoder/layers_5/encoder_decoder_attention/query/kernel                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.891222 140317719510080 utils.py:1364] Variable decoder/layers_5/encoder_decoder_attention/value/kernel                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.891266 140317719510080 utils.py:1364] Variable decoder/layers_5/mlp/wi_0/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.891306 140317719510080 utils.py:1364] Variable decoder/layers_5/mlp/wi_1/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.891344 140317719510080 utils.py:1364] Variable decoder/layers_5/mlp/wo/kernel                                                   size 1572864      shape (mlp=2048, embed=768)                    partition spec ('model', None)
I1007 02:01:22.891382 140317719510080 utils.py:1364] Variable decoder/layers_5/pre_cross_attention_layer_norm/scale                            size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.891420 140317719510080 utils.py:1364] Variable decoder/layers_5/pre_mlp_layer_norm/scale                                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.891457 140317719510080 utils.py:1364] Variable decoder/layers_5/pre_self_attention_layer_norm/scale                             size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.891494 140317719510080 utils.py:1364] Variable decoder/layers_5/self_attention/key/kernel                                       size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.891532 140317719510080 utils.py:1364] Variable decoder/layers_5/self_attention/out/kernel                                       size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.891570 140317719510080 utils.py:1364] Variable decoder/layers_5/self_attention/query/kernel                                     size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.891607 140317719510080 utils.py:1364] Variable decoder/layers_5/self_attention/value/kernel                                     size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.891646 140317719510080 utils.py:1364] Variable decoder/layers_6/encoder_decoder_attention/key/kernel                            size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.891685 140317719510080 utils.py:1364] Variable decoder/layers_6/encoder_decoder_attention/out/kernel                            size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.891723 140317719510080 utils.py:1364] Variable decoder/layers_6/encoder_decoder_attention/query/kernel                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.891761 140317719510080 utils.py:1364] Variable decoder/layers_6/encoder_decoder_attention/value/kernel                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.891798 140317719510080 utils.py:1364] Variable decoder/layers_6/mlp/wi_0/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.891838 140317719510080 utils.py:1364] Variable decoder/layers_6/mlp/wi_1/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.891876 140317719510080 utils.py:1364] Variable decoder/layers_6/mlp/wo/kernel                                                   size 1572864      shape (mlp=2048, embed=768)                    partition spec ('model', None)
I1007 02:01:22.891920 140317719510080 utils.py:1364] Variable decoder/layers_6/pre_cross_attention_layer_norm/scale                            size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.891964 140317719510080 utils.py:1364] Variable decoder/layers_6/pre_mlp_layer_norm/scale                                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.892002 140317719510080 utils.py:1364] Variable decoder/layers_6/pre_self_attention_layer_norm/scale                             size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.892040 140317719510080 utils.py:1364] Variable decoder/layers_6/self_attention/key/kernel                                       size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.892078 140317719510080 utils.py:1364] Variable decoder/layers_6/self_attention/out/kernel                                       size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.892115 140317719510080 utils.py:1364] Variable decoder/layers_6/self_attention/query/kernel                                     size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.892153 140317719510080 utils.py:1364] Variable decoder/layers_6/self_attention/value/kernel                                     size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.892190 140317719510080 utils.py:1364] Variable decoder/layers_7/encoder_decoder_attention/key/kernel                            size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.892230 140317719510080 utils.py:1364] Variable decoder/layers_7/encoder_decoder_attention/out/kernel                            size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.892268 140317719510080 utils.py:1364] Variable decoder/layers_7/encoder_decoder_attention/query/kernel                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.892315 140317719510080 utils.py:1364] Variable decoder/layers_7/encoder_decoder_attention/value/kernel                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.892347 140317719510080 utils.py:1364] Variable decoder/layers_7/mlp/wi_0/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.892380 140317719510080 utils.py:1364] Variable decoder/layers_7/mlp/wi_1/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.892411 140317719510080 utils.py:1364] Variable decoder/layers_7/mlp/wo/kernel                                                   size 1572864      shape (mlp=2048, embed=768)                    partition spec ('model', None)
I1007 02:01:22.892443 140317719510080 utils.py:1364] Variable decoder/layers_7/pre_cross_attention_layer_norm/scale                            size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.892486 140317719510080 utils.py:1364] Variable decoder/layers_7/pre_mlp_layer_norm/scale                                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.892519 140317719510080 utils.py:1364] Variable decoder/layers_7/pre_self_attention_layer_norm/scale                             size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.892551 140317719510080 utils.py:1364] Variable decoder/layers_7/self_attention/key/kernel                                       size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.892588 140317719510080 utils.py:1364] Variable decoder/layers_7/self_attention/out/kernel                                       size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.892620 140317719510080 utils.py:1364] Variable decoder/layers_7/self_attention/query/kernel                                     size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.892651 140317719510080 utils.py:1364] Variable decoder/layers_7/self_attention/value/kernel                                     size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.892683 140317719510080 utils.py:1364] Variable decoder/layers_8/encoder_decoder_attention/key/kernel                            size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.892716 140317719510080 utils.py:1364] Variable decoder/layers_8/encoder_decoder_attention/out/kernel                            size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.892749 140317719510080 utils.py:1364] Variable decoder/layers_8/encoder_decoder_attention/query/kernel                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.892781 140317719510080 utils.py:1364] Variable decoder/layers_8/encoder_decoder_attention/value/kernel                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.892813 140317719510080 utils.py:1364] Variable decoder/layers_8/mlp/wi_0/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.892845 140317719510080 utils.py:1364] Variable decoder/layers_8/mlp/wi_1/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.892877 140317719510080 utils.py:1364] Variable decoder/layers_8/mlp/wo/kernel                                                   size 1572864      shape (mlp=2048, embed=768)                    partition spec ('model', None)
I1007 02:01:22.892915 140317719510080 utils.py:1364] Variable decoder/layers_8/pre_cross_attention_layer_norm/scale                            size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.892949 140317719510080 utils.py:1364] Variable decoder/layers_8/pre_mlp_layer_norm/scale                                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.892980 140317719510080 utils.py:1364] Variable decoder/layers_8/pre_self_attention_layer_norm/scale                             size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.893012 140317719510080 utils.py:1364] Variable decoder/layers_8/self_attention/key/kernel                                       size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.893044 140317719510080 utils.py:1364] Variable decoder/layers_8/self_attention/out/kernel                                       size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.893076 140317719510080 utils.py:1364] Variable decoder/layers_8/self_attention/query/kernel                                     size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.893109 140317719510080 utils.py:1364] Variable decoder/layers_8/self_attention/value/kernel                                     size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.893145 140317719510080 utils.py:1364] Variable decoder/layers_9/encoder_decoder_attention/key/kernel                            size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.893178 140317719510080 utils.py:1364] Variable decoder/layers_9/encoder_decoder_attention/out/kernel                            size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.893211 140317719510080 utils.py:1364] Variable decoder/layers_9/encoder_decoder_attention/query/kernel                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.893243 140317719510080 utils.py:1364] Variable decoder/layers_9/encoder_decoder_attention/value/kernel                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.893276 140317719510080 utils.py:1364] Variable decoder/layers_9/mlp/wi_0/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.893308 140317719510080 utils.py:1364] Variable decoder/layers_9/mlp/wi_1/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.893341 140317719510080 utils.py:1364] Variable decoder/layers_9/mlp/wo/kernel                                                   size 1572864      shape (mlp=2048, embed=768)                    partition spec ('model', None)
I1007 02:01:22.893373 140317719510080 utils.py:1364] Variable decoder/layers_9/pre_cross_attention_layer_norm/scale                            size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.893404 140317719510080 utils.py:1364] Variable decoder/layers_9/pre_mlp_layer_norm/scale                                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.893436 140317719510080 utils.py:1364] Variable decoder/layers_9/pre_self_attention_layer_norm/scale                             size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.893468 140317719510080 utils.py:1364] Variable decoder/layers_9/self_attention/key/kernel                                       size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.893500 140317719510080 utils.py:1364] Variable decoder/layers_9/self_attention/out/kernel                                       size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.893532 140317719510080 utils.py:1364] Variable decoder/layers_9/self_attention/query/kernel                                     size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.893564 140317719510080 utils.py:1364] Variable decoder/layers_9/self_attention/value/kernel                                     size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.893596 140317719510080 utils.py:1364] Variable decoder/logits_dense/kernel                                                      size 24674304     shape (embed=768, vocab=32128)                 partition spec (None, 'model')
I1007 02:01:22.893629 140317719510080 utils.py:1364] Variable decoder/relpos_bias/rel_embedding                                                size 384          shape (heads=12, relpos_buckets=32)            partition spec ('model', None)
I1007 02:01:22.893664 140317719510080 utils.py:1364] Variable encoder/encoder_norm/scale                                                       size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.893704 140317719510080 utils.py:1364] Variable encoder/layers_0/attention/key/kernel                                            size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.893738 140317719510080 utils.py:1364] Variable encoder/layers_0/attention/out/kernel                                            size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.893771 140317719510080 utils.py:1364] Variable encoder/layers_0/attention/query/kernel                                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.893804 140317719510080 utils.py:1364] Variable encoder/layers_0/attention/value/kernel                                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.893836 140317719510080 utils.py:1364] Variable encoder/layers_0/mlp/wi_0/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.893867 140317719510080 utils.py:1364] Variable encoder/layers_0/mlp/wi_1/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.893905 140317719510080 utils.py:1364] Variable encoder/layers_0/mlp/wo/kernel                                                   size 1572864      shape (mlp=2048, embed=768)                    partition spec ('model', None)
I1007 02:01:22.893939 140317719510080 utils.py:1364] Variable encoder/layers_0/pre_attention_layer_norm/scale                                  size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.893984 140317719510080 utils.py:1364] Variable encoder/layers_0/pre_mlp_layer_norm/scale                                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.894018 140317719510080 utils.py:1364] Variable encoder/layers_1/attention/key/kernel                                            size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.894052 140317719510080 utils.py:1364] Variable encoder/layers_1/attention/out/kernel                                            size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.894085 140317719510080 utils.py:1364] Variable encoder/layers_1/attention/query/kernel                                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.894118 140317719510080 utils.py:1364] Variable encoder/layers_1/attention/value/kernel                                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.894150 140317719510080 utils.py:1364] Variable encoder/layers_1/mlp/wi_0/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.894184 140317719510080 utils.py:1364] Variable encoder/layers_1/mlp/wi_1/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.894216 140317719510080 utils.py:1364] Variable encoder/layers_1/mlp/wo/kernel                                                   size 1572864      shape (mlp=2048, embed=768)                    partition spec ('model', None)
I1007 02:01:22.894249 140317719510080 utils.py:1364] Variable encoder/layers_1/pre_attention_layer_norm/scale                                  size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.894281 140317719510080 utils.py:1364] Variable encoder/layers_1/pre_mlp_layer_norm/scale                                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.894319 140317719510080 utils.py:1364] Variable encoder/layers_10/attention/key/kernel                                           size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.894353 140317719510080 utils.py:1364] Variable encoder/layers_10/attention/out/kernel                                           size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.894386 140317719510080 utils.py:1364] Variable encoder/layers_10/attention/query/kernel                                         size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.894419 140317719510080 utils.py:1364] Variable encoder/layers_10/attention/value/kernel                                         size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.894451 140317719510080 utils.py:1364] Variable encoder/layers_10/mlp/wi_0/kernel                                                size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.894484 140317719510080 utils.py:1364] Variable encoder/layers_10/mlp/wi_1/kernel                                                size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.894516 140317719510080 utils.py:1364] Variable encoder/layers_10/mlp/wo/kernel                                                  size 1572864      shape (mlp=2048, embed=768)                    partition spec ('model', None)
I1007 02:01:22.894548 140317719510080 utils.py:1364] Variable encoder/layers_10/pre_attention_layer_norm/scale                                 size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.894581 140317719510080 utils.py:1364] Variable encoder/layers_10/pre_mlp_layer_norm/scale                                       size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.894613 140317719510080 utils.py:1364] Variable encoder/layers_11/attention/key/kernel                                           size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.894645 140317719510080 utils.py:1364] Variable encoder/layers_11/attention/out/kernel                                           size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.894677 140317719510080 utils.py:1364] Variable encoder/layers_11/attention/query/kernel                                         size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.894710 140317719510080 utils.py:1364] Variable encoder/layers_11/attention/value/kernel                                         size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.894741 140317719510080 utils.py:1364] Variable encoder/layers_11/mlp/wi_0/kernel                                                size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.894773 140317719510080 utils.py:1364] Variable encoder/layers_11/mlp/wi_1/kernel                                                size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.894821 140317719510080 utils.py:1364] Variable encoder/layers_11/mlp/wo/kernel                                                  size 1572864      shape (mlp=2048, embed=768)                    partition spec ('model', None)
I1007 02:01:22.894857 140317719510080 utils.py:1364] Variable encoder/layers_11/pre_attention_layer_norm/scale                                 size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.894898 140317719510080 utils.py:1364] Variable encoder/layers_11/pre_mlp_layer_norm/scale                                       size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.894932 140317719510080 utils.py:1364] Variable encoder/layers_2/attention/key/kernel                                            size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.894965 140317719510080 utils.py:1364] Variable encoder/layers_2/attention/out/kernel                                            size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.894996 140317719510080 utils.py:1364] Variable encoder/layers_2/attention/query/kernel                                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.895028 140317719510080 utils.py:1364] Variable encoder/layers_2/attention/value/kernel                                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.895059 140317719510080 utils.py:1364] Variable encoder/layers_2/mlp/wi_0/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.895092 140317719510080 utils.py:1364] Variable encoder/layers_2/mlp/wi_1/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.895123 140317719510080 utils.py:1364] Variable encoder/layers_2/mlp/wo/kernel                                                   size 1572864      shape (mlp=2048, embed=768)                    partition spec ('model', None)
I1007 02:01:22.895155 140317719510080 utils.py:1364] Variable encoder/layers_2/pre_attention_layer_norm/scale                                  size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.895187 140317719510080 utils.py:1364] Variable encoder/layers_2/pre_mlp_layer_norm/scale                                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.895219 140317719510080 utils.py:1364] Variable encoder/layers_3/attention/key/kernel                                            size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.895251 140317719510080 utils.py:1364] Variable encoder/layers_3/attention/out/kernel                                            size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.895282 140317719510080 utils.py:1364] Variable encoder/layers_3/attention/query/kernel                                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.895314 140317719510080 utils.py:1364] Variable encoder/layers_3/attention/value/kernel                                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.895345 140317719510080 utils.py:1364] Variable encoder/layers_3/mlp/wi_0/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.895376 140317719510080 utils.py:1364] Variable encoder/layers_3/mlp/wi_1/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.895408 140317719510080 utils.py:1364] Variable encoder/layers_3/mlp/wo/kernel                                                   size 1572864      shape (mlp=2048, embed=768)                    partition spec ('model', None)
I1007 02:01:22.895448 140317719510080 utils.py:1364] Variable encoder/layers_3/pre_attention_layer_norm/scale                                  size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.895491 140317719510080 utils.py:1364] Variable encoder/layers_3/pre_mlp_layer_norm/scale                                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.895524 140317719510080 utils.py:1364] Variable encoder/layers_4/attention/key/kernel                                            size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.895557 140317719510080 utils.py:1364] Variable encoder/layers_4/attention/out/kernel                                            size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.895588 140317719510080 utils.py:1364] Variable encoder/layers_4/attention/query/kernel                                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.895620 140317719510080 utils.py:1364] Variable encoder/layers_4/attention/value/kernel                                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.895652 140317719510080 utils.py:1364] Variable encoder/layers_4/mlp/wi_0/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.895685 140317719510080 utils.py:1364] Variable encoder/layers_4/mlp/wi_1/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.895716 140317719510080 utils.py:1364] Variable encoder/layers_4/mlp/wo/kernel                                                   size 1572864      shape (mlp=2048, embed=768)                    partition spec ('model', None)
I1007 02:01:22.895748 140317719510080 utils.py:1364] Variable encoder/layers_4/pre_attention_layer_norm/scale                                  size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.895779 140317719510080 utils.py:1364] Variable encoder/layers_4/pre_mlp_layer_norm/scale                                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.895811 140317719510080 utils.py:1364] Variable encoder/layers_5/attention/key/kernel                                            size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.895843 140317719510080 utils.py:1364] Variable encoder/layers_5/attention/out/kernel                                            size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.895874 140317719510080 utils.py:1364] Variable encoder/layers_5/attention/query/kernel                                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.895912 140317719510080 utils.py:1364] Variable encoder/layers_5/attention/value/kernel                                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.895946 140317719510080 utils.py:1364] Variable encoder/layers_5/mlp/wi_0/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.895978 140317719510080 utils.py:1364] Variable encoder/layers_5/mlp/wi_1/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.896014 140317719510080 utils.py:1364] Variable encoder/layers_5/mlp/wo/kernel                                                   size 1572864      shape (mlp=2048, embed=768)                    partition spec ('model', None)
I1007 02:01:22.896046 140317719510080 utils.py:1364] Variable encoder/layers_5/pre_attention_layer_norm/scale                                  size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.896078 140317719510080 utils.py:1364] Variable encoder/layers_5/pre_mlp_layer_norm/scale                                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.896109 140317719510080 utils.py:1364] Variable encoder/layers_6/attention/key/kernel                                            size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.896141 140317719510080 utils.py:1364] Variable encoder/layers_6/attention/out/kernel                                            size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.896173 140317719510080 utils.py:1364] Variable encoder/layers_6/attention/query/kernel                                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.896204 140317719510080 utils.py:1364] Variable encoder/layers_6/attention/value/kernel                                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.896236 140317719510080 utils.py:1364] Variable encoder/layers_6/mlp/wi_0/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.896269 140317719510080 utils.py:1364] Variable encoder/layers_6/mlp/wi_1/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.896300 140317719510080 utils.py:1364] Variable encoder/layers_6/mlp/wo/kernel                                                   size 1572864      shape (mlp=2048, embed=768)                    partition spec ('model', None)
I1007 02:01:22.896331 140317719510080 utils.py:1364] Variable encoder/layers_6/pre_attention_layer_norm/scale                                  size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.896362 140317719510080 utils.py:1364] Variable encoder/layers_6/pre_mlp_layer_norm/scale                                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.896393 140317719510080 utils.py:1364] Variable encoder/layers_7/attention/key/kernel                                            size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.896425 140317719510080 utils.py:1364] Variable encoder/layers_7/attention/out/kernel                                            size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.896457 140317719510080 utils.py:1364] Variable encoder/layers_7/attention/query/kernel                                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.896489 140317719510080 utils.py:1364] Variable encoder/layers_7/attention/value/kernel                                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.896520 140317719510080 utils.py:1364] Variable encoder/layers_7/mlp/wi_0/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.896552 140317719510080 utils.py:1364] Variable encoder/layers_7/mlp/wi_1/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.896588 140317719510080 utils.py:1364] Variable encoder/layers_7/mlp/wo/kernel                                                   size 1572864      shape (mlp=2048, embed=768)                    partition spec ('model', None)
I1007 02:01:22.896619 140317719510080 utils.py:1364] Variable encoder/layers_7/pre_attention_layer_norm/scale                                  size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.896651 140317719510080 utils.py:1364] Variable encoder/layers_7/pre_mlp_layer_norm/scale                                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.896683 140317719510080 utils.py:1364] Variable encoder/layers_8/attention/key/kernel                                            size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.896715 140317719510080 utils.py:1364] Variable encoder/layers_8/attention/out/kernel                                            size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.896747 140317719510080 utils.py:1364] Variable encoder/layers_8/attention/query/kernel                                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.896779 140317719510080 utils.py:1364] Variable encoder/layers_8/attention/value/kernel                                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.896811 140317719510080 utils.py:1364] Variable encoder/layers_8/mlp/wi_0/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.896843 140317719510080 utils.py:1364] Variable encoder/layers_8/mlp/wi_1/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.896875 140317719510080 utils.py:1364] Variable encoder/layers_8/mlp/wo/kernel                                                   size 1572864      shape (mlp=2048, embed=768)                    partition spec ('model', None)
I1007 02:01:22.896913 140317719510080 utils.py:1364] Variable encoder/layers_8/pre_attention_layer_norm/scale                                  size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.896955 140317719510080 utils.py:1364] Variable encoder/layers_8/pre_mlp_layer_norm/scale                                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.896988 140317719510080 utils.py:1364] Variable encoder/layers_9/attention/key/kernel                                            size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.897021 140317719510080 utils.py:1364] Variable encoder/layers_9/attention/out/kernel                                            size 589824       shape (joined_kv=768, embed=768)               partition spec ('model', None)
I1007 02:01:22.897053 140317719510080 utils.py:1364] Variable encoder/layers_9/attention/query/kernel                                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.897084 140317719510080 utils.py:1364] Variable encoder/layers_9/attention/value/kernel                                          size 589824       shape (embed=768, joined_kv=768)               partition spec (None, 'model')
I1007 02:01:22.897116 140317719510080 utils.py:1364] Variable encoder/layers_9/mlp/wi_0/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.897152 140317719510080 utils.py:1364] Variable encoder/layers_9/mlp/wi_1/kernel                                                 size 1572864      shape (embed=768, mlp=2048)                    partition spec (None, 'model')
I1007 02:01:22.897184 140317719510080 utils.py:1364] Variable encoder/layers_9/mlp/wo/kernel                                                   size 1572864      shape (mlp=2048, embed=768)                    partition spec ('model', None)
I1007 02:01:22.897216 140317719510080 utils.py:1364] Variable encoder/layers_9/pre_attention_layer_norm/scale                                  size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.897351 140317719510080 utils.py:1364] Variable encoder/layers_9/pre_mlp_layer_norm/scale                                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.897389 140317719510080 utils.py:1364] Variable encoder/relpos_bias/rel_embedding                                                size 384          shape (heads=12, relpos_buckets=32)            partition spec ('model', None)
I1007 02:01:22.897423 140317719510080 utils.py:1364] Variable token_embedder/embedding                                                         size 24674304     shape (vocab=32128, embed=768)                 partition spec ('model', None)
I1007 02:01:22.897502 140317719510080 utils.py:1364] Total number of parameters: 247577856
I1007 02:01:22.897773 140317719510080 utils.py:1364] 
I1007 02:01:22.901254 140317719510080 utils.py:1364] Variable param_states/decoder/decoder_norm/scale/m                                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.901348 140317719510080 utils.py:1364] Variable param_states/decoder/decoder_norm/scale/v                                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.901393 140317719510080 utils.py:1364] Variable param_states/decoder/decoder_norm/scale/v_col                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.901431 140317719510080 utils.py:1364] Variable param_states/decoder/decoder_norm/scale/v_row                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.901467 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/encoder_decoder_attention/key/kernel/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.901504 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/encoder_decoder_attention/key/kernel/v             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.901538 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/encoder_decoder_attention/key/kernel/v_col         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.901573 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/encoder_decoder_attention/key/kernel/v_row         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.901606 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/encoder_decoder_attention/out/kernel/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.901640 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/encoder_decoder_attention/out/kernel/v             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.901674 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/encoder_decoder_attention/out/kernel/v_col         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.901716 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/encoder_decoder_attention/out/kernel/v_row         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.901750 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/encoder_decoder_attention/query/kernel/m           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.901784 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/encoder_decoder_attention/query/kernel/v           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.901817 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/encoder_decoder_attention/query/kernel/v_col       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.901849 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/encoder_decoder_attention/query/kernel/v_row       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.901883 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/encoder_decoder_attention/value/kernel/m           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.901926 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/encoder_decoder_attention/value/kernel/v           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.901960 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/encoder_decoder_attention/value/kernel/v_col       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.901993 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/encoder_decoder_attention/value/kernel/v_row       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.902027 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.902060 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.902094 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/mlp/wi_0/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.902128 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/mlp/wi_0/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.902161 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.902195 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.902228 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/mlp/wi_1/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.902261 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/mlp/wi_1/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.902305 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.902337 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.902366 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/mlp/wo/kernel/v_col                                size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.902394 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/mlp/wo/kernel/v_row                                size 768          shape (768,)                                   partition spec None
I1007 02:01:22.902422 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/pre_cross_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.902453 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/pre_cross_attention_layer_norm/scale/v             size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.902494 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/pre_cross_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.902524 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/pre_cross_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.902553 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.902583 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v                         size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.902612 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.902641 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.902669 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I1007 02:01:22.902699 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v              size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.902728 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.902756 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.902784 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.902824 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.902855 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/self_attention/key/kernel/v_col                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.902888 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/self_attention/key/kernel/v_row                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.902923 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.902952 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.902980 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/self_attention/out/kernel/v_col                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.903008 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/self_attention/out/kernel/v_row                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.903036 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.903065 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.903093 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/self_attention/query/kernel/v_col                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.903121 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/self_attention/query/kernel/v_row                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.903149 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.903177 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.903205 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/self_attention/value/kernel/v_col                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.903233 140317719510080 utils.py:1364] Variable param_states/decoder/layers_0/self_attention/value/kernel/v_row                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.903261 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/encoder_decoder_attention/key/kernel/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.903290 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/encoder_decoder_attention/key/kernel/v             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.903318 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/encoder_decoder_attention/key/kernel/v_col         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.903347 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/encoder_decoder_attention/key/kernel/v_row         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.903375 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/encoder_decoder_attention/out/kernel/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.903407 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/encoder_decoder_attention/out/kernel/v             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.903436 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/encoder_decoder_attention/out/kernel/v_col         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.903465 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/encoder_decoder_attention/out/kernel/v_row         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.903492 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/encoder_decoder_attention/query/kernel/m           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.903521 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/encoder_decoder_attention/query/kernel/v           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.903549 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/encoder_decoder_attention/query/kernel/v_col       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.903577 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/encoder_decoder_attention/query/kernel/v_row       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.903605 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/encoder_decoder_attention/value/kernel/m           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.903633 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/encoder_decoder_attention/value/kernel/v           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.903661 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/encoder_decoder_attention/value/kernel/v_col       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.903690 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/encoder_decoder_attention/value/kernel/v_row       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.903718 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.903747 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.903775 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/mlp/wi_0/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.903803 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/mlp/wi_0/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.903831 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.903870 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.903904 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/mlp/wi_1/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.903938 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/mlp/wi_1/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.903966 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.903995 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.904023 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/mlp/wo/kernel/v_col                                size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.904052 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/mlp/wo/kernel/v_row                                size 768          shape (768,)                                   partition spec None
I1007 02:01:22.904080 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/pre_cross_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.904111 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/pre_cross_attention_layer_norm/scale/v             size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.904141 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/pre_cross_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.904169 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/pre_cross_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.904197 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.904227 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v                         size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.904257 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.904285 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.904313 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I1007 02:01:22.904343 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v              size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.904372 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.904399 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.904427 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.904458 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.904486 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/self_attention/key/kernel/v_col                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.904514 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/self_attention/key/kernel/v_row                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.904542 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.904570 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.904598 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/self_attention/out/kernel/v_col                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.904627 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/self_attention/out/kernel/v_row                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.904655 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.904683 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.904711 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/self_attention/query/kernel/v_col                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.904739 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/self_attention/query/kernel/v_row                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.904767 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.904795 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.904823 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/self_attention/value/kernel/v_col                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.904850 140317719510080 utils.py:1364] Variable param_states/decoder/layers_1/self_attention/value/kernel/v_row                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.904878 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/encoder_decoder_attention/key/kernel/m            size 1            shape (1,)                                     partition spec None
I1007 02:01:22.904915 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/encoder_decoder_attention/key/kernel/v            size 1            shape (1,)                                     partition spec None
I1007 02:01:22.904944 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/encoder_decoder_attention/key/kernel/v_col        size 768          shape (768,)                                   partition spec None
I1007 02:01:22.904976 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/encoder_decoder_attention/key/kernel/v_row        size 768          shape (768,)                                   partition spec None
I1007 02:01:22.905005 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/encoder_decoder_attention/out/kernel/m            size 1            shape (1,)                                     partition spec None
I1007 02:01:22.905033 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/encoder_decoder_attention/out/kernel/v            size 1            shape (1,)                                     partition spec None
I1007 02:01:22.905061 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/encoder_decoder_attention/out/kernel/v_col        size 768          shape (768,)                                   partition spec None
I1007 02:01:22.905089 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/encoder_decoder_attention/out/kernel/v_row        size 768          shape (768,)                                   partition spec None
I1007 02:01:22.905117 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/encoder_decoder_attention/query/kernel/m          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.905146 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/encoder_decoder_attention/query/kernel/v          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.905174 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/encoder_decoder_attention/query/kernel/v_col      size 768          shape (768,)                                   partition spec None
I1007 02:01:22.905202 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/encoder_decoder_attention/query/kernel/v_row      size 768          shape (768,)                                   partition spec None
I1007 02:01:22.905239 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/encoder_decoder_attention/value/kernel/m          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.905268 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/encoder_decoder_attention/value/kernel/v          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.905296 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/encoder_decoder_attention/value/kernel/v_col      size 768          shape (768,)                                   partition spec None
I1007 02:01:22.905324 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/encoder_decoder_attention/value/kernel/v_row      size 768          shape (768,)                                   partition spec None
I1007 02:01:22.905351 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I1007 02:01:22.905379 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I1007 02:01:22.905407 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/mlp/wi_0/kernel/v_col                             size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.905436 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/mlp/wi_0/kernel/v_row                             size 768          shape (768,)                                   partition spec None
I1007 02:01:22.905463 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I1007 02:01:22.905494 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I1007 02:01:22.905523 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/mlp/wi_1/kernel/v_col                             size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.905550 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/mlp/wi_1/kernel/v_row                             size 768          shape (768,)                                   partition spec None
I1007 02:01:22.905579 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I1007 02:01:22.905606 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I1007 02:01:22.905635 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/mlp/wo/kernel/v_col                               size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.905663 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/mlp/wo/kernel/v_row                               size 768          shape (768,)                                   partition spec None
I1007 02:01:22.905690 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/pre_cross_attention_layer_norm/scale/m            size 1            shape (1,)                                     partition spec None
I1007 02:01:22.905721 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/pre_cross_attention_layer_norm/scale/v            size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.905750 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/pre_cross_attention_layer_norm/scale/v_col        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.905779 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/pre_cross_attention_layer_norm/scale/v_row        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.905807 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.905836 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.905865 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.905898 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.905927 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.905957 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v             size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.905986 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.906021 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.906049 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I1007 02:01:22.906077 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I1007 02:01:22.906104 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/self_attention/key/kernel/v_col                   size 768          shape (768,)                                   partition spec None
I1007 02:01:22.906132 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/self_attention/key/kernel/v_row                   size 768          shape (768,)                                   partition spec None
I1007 02:01:22.906160 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I1007 02:01:22.906188 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I1007 02:01:22.906216 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/self_attention/out/kernel/v_col                   size 768          shape (768,)                                   partition spec None
I1007 02:01:22.906244 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/self_attention/out/kernel/v_row                   size 768          shape (768,)                                   partition spec None
I1007 02:01:22.906272 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.906300 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.906328 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/self_attention/query/kernel/v_col                 size 768          shape (768,)                                   partition spec None
I1007 02:01:22.906357 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/self_attention/query/kernel/v_row                 size 768          shape (768,)                                   partition spec None
I1007 02:01:22.906384 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.906413 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.906440 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/self_attention/value/kernel/v_col                 size 768          shape (768,)                                   partition spec None
I1007 02:01:22.906469 140317719510080 utils.py:1364] Variable param_states/decoder/layers_10/self_attention/value/kernel/v_row                 size 768          shape (768,)                                   partition spec None
I1007 02:01:22.906497 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/encoder_decoder_attention/key/kernel/m            size 1            shape (1,)                                     partition spec None
I1007 02:01:22.906530 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/encoder_decoder_attention/key/kernel/v            size 1            shape (1,)                                     partition spec None
I1007 02:01:22.906558 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/encoder_decoder_attention/key/kernel/v_col        size 768          shape (768,)                                   partition spec None
I1007 02:01:22.906596 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/encoder_decoder_attention/key/kernel/v_row        size 768          shape (768,)                                   partition spec None
I1007 02:01:22.906625 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/encoder_decoder_attention/out/kernel/m            size 1            shape (1,)                                     partition spec None
I1007 02:01:22.906653 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/encoder_decoder_attention/out/kernel/v            size 1            shape (1,)                                     partition spec None
I1007 02:01:22.906681 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/encoder_decoder_attention/out/kernel/v_col        size 768          shape (768,)                                   partition spec None
I1007 02:01:22.906709 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/encoder_decoder_attention/out/kernel/v_row        size 768          shape (768,)                                   partition spec None
I1007 02:01:22.906737 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/encoder_decoder_attention/query/kernel/m          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.906766 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/encoder_decoder_attention/query/kernel/v          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.906794 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/encoder_decoder_attention/query/kernel/v_col      size 768          shape (768,)                                   partition spec None
I1007 02:01:22.906836 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/encoder_decoder_attention/query/kernel/v_row      size 768          shape (768,)                                   partition spec None
I1007 02:01:22.906864 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/encoder_decoder_attention/value/kernel/m          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.906897 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/encoder_decoder_attention/value/kernel/v          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.906926 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/encoder_decoder_attention/value/kernel/v_col      size 768          shape (768,)                                   partition spec None
I1007 02:01:22.906954 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/encoder_decoder_attention/value/kernel/v_row      size 768          shape (768,)                                   partition spec None
I1007 02:01:22.906982 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I1007 02:01:22.907011 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I1007 02:01:22.907040 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/mlp/wi_0/kernel/v_col                             size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.907072 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/mlp/wi_0/kernel/v_row                             size 768          shape (768,)                                   partition spec None
I1007 02:01:22.907100 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I1007 02:01:22.907128 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I1007 02:01:22.907156 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/mlp/wi_1/kernel/v_col                             size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.907184 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/mlp/wi_1/kernel/v_row                             size 768          shape (768,)                                   partition spec None
I1007 02:01:22.907212 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I1007 02:01:22.907240 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I1007 02:01:22.907268 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/mlp/wo/kernel/v_col                               size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.907296 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/mlp/wo/kernel/v_row                               size 768          shape (768,)                                   partition spec None
I1007 02:01:22.907324 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/pre_cross_attention_layer_norm/scale/m            size 1            shape (1,)                                     partition spec None
I1007 02:01:22.907355 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/pre_cross_attention_layer_norm/scale/v            size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.907384 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/pre_cross_attention_layer_norm/scale/v_col        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.907413 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/pre_cross_attention_layer_norm/scale/v_row        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.907441 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.907471 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.907500 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.907528 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.907555 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.907589 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v             size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.907619 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.907648 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.907676 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I1007 02:01:22.907705 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I1007 02:01:22.907733 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/self_attention/key/kernel/v_col                   size 768          shape (768,)                                   partition spec None
I1007 02:01:22.907761 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/self_attention/key/kernel/v_row                   size 768          shape (768,)                                   partition spec None
I1007 02:01:22.907788 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I1007 02:01:22.907816 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I1007 02:01:22.907844 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/self_attention/out/kernel/v_col                   size 768          shape (768,)                                   partition spec None
I1007 02:01:22.907872 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/self_attention/out/kernel/v_row                   size 768          shape (768,)                                   partition spec None
I1007 02:01:22.907905 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.907934 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.907972 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/self_attention/query/kernel/v_col                 size 768          shape (768,)                                   partition spec None
I1007 02:01:22.908001 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/self_attention/query/kernel/v_row                 size 768          shape (768,)                                   partition spec None
I1007 02:01:22.908029 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.908057 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.908089 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/self_attention/value/kernel/v_col                 size 768          shape (768,)                                   partition spec None
I1007 02:01:22.908118 140317719510080 utils.py:1364] Variable param_states/decoder/layers_11/self_attention/value/kernel/v_row                 size 768          shape (768,)                                   partition spec None
I1007 02:01:22.908145 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/encoder_decoder_attention/key/kernel/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.908174 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/encoder_decoder_attention/key/kernel/v             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.908203 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/encoder_decoder_attention/key/kernel/v_col         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.908231 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/encoder_decoder_attention/key/kernel/v_row         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.908260 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/encoder_decoder_attention/out/kernel/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.908288 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/encoder_decoder_attention/out/kernel/v             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.908316 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/encoder_decoder_attention/out/kernel/v_col         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.908344 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/encoder_decoder_attention/out/kernel/v_row         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.908371 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/encoder_decoder_attention/query/kernel/m           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.908400 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/encoder_decoder_attention/query/kernel/v           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.908427 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/encoder_decoder_attention/query/kernel/v_col       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.908454 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/encoder_decoder_attention/query/kernel/v_row       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.908482 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/encoder_decoder_attention/value/kernel/m           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.908510 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/encoder_decoder_attention/value/kernel/v           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.908537 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/encoder_decoder_attention/value/kernel/v_col       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.908565 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/encoder_decoder_attention/value/kernel/v_row       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.908595 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.908624 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.908652 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/mlp/wi_0/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.908680 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/mlp/wi_0/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.908708 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.908735 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.908763 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/mlp/wi_1/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.908791 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/mlp/wi_1/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.908826 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.908854 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.908882 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/mlp/wo/kernel/v_col                                size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.908916 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/mlp/wo/kernel/v_row                                size 768          shape (768,)                                   partition spec None
I1007 02:01:22.908944 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/pre_cross_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.908975 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/pre_cross_attention_layer_norm/scale/v             size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.909005 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/pre_cross_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.909033 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/pre_cross_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.909060 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.909090 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v                         size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.909124 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.909152 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.909180 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I1007 02:01:22.909209 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v              size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.909239 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.909267 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.909294 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.909332 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.909362 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/self_attention/key/kernel/v_col                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.909391 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/self_attention/key/kernel/v_row                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.909419 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.909447 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.909476 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/self_attention/out/kernel/v_col                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.909504 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/self_attention/out/kernel/v_row                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.909532 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.909559 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.909588 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/self_attention/query/kernel/v_col                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.909616 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/self_attention/query/kernel/v_row                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.909647 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.909676 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.909704 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/self_attention/value/kernel/v_col                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.909731 140317719510080 utils.py:1364] Variable param_states/decoder/layers_2/self_attention/value/kernel/v_row                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.909759 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/encoder_decoder_attention/key/kernel/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.909788 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/encoder_decoder_attention/key/kernel/v             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.909817 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/encoder_decoder_attention/key/kernel/v_col         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.909846 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/encoder_decoder_attention/key/kernel/v_row         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.909874 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/encoder_decoder_attention/out/kernel/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.909906 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/encoder_decoder_attention/out/kernel/v             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.909935 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/encoder_decoder_attention/out/kernel/v_col         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.909963 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/encoder_decoder_attention/out/kernel/v_row         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.909991 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/encoder_decoder_attention/query/kernel/m           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.910019 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/encoder_decoder_attention/query/kernel/v           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.910046 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/encoder_decoder_attention/query/kernel/v_col       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.910073 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/encoder_decoder_attention/query/kernel/v_row       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.910101 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/encoder_decoder_attention/value/kernel/m           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.910134 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/encoder_decoder_attention/value/kernel/v           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.910184 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/encoder_decoder_attention/value/kernel/v_col       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.910218 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/encoder_decoder_attention/value/kernel/v_row       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.910250 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.910284 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.910317 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/mlp/wi_0/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.910350 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/mlp/wi_0/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.910383 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.910416 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.910450 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/mlp/wi_1/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.910483 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/mlp/wi_1/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.910516 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.910549 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.910582 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/mlp/wo/kernel/v_col                                size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.910615 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/mlp/wo/kernel/v_row                                size 768          shape (768,)                                   partition spec None
I1007 02:01:22.910648 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/pre_cross_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.910684 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/pre_cross_attention_layer_norm/scale/v             size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.910719 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/pre_cross_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.910753 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/pre_cross_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.910831 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.910874 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v                         size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.910915 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.910949 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.910982 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I1007 02:01:22.911017 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v              size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.911052 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.911085 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.911121 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.911156 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.911190 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/self_attention/key/kernel/v_col                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.911222 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/self_attention/key/kernel/v_row                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.911255 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.911289 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.911322 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/self_attention/out/kernel/v_col                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.911354 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/self_attention/out/kernel/v_row                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.911386 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.911419 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.911457 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/self_attention/query/kernel/v_col                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.911490 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/self_attention/query/kernel/v_row                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.911523 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.911556 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.911589 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/self_attention/value/kernel/v_col                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.911622 140317719510080 utils.py:1364] Variable param_states/decoder/layers_3/self_attention/value/kernel/v_row                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.911655 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/encoder_decoder_attention/key/kernel/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.911689 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/encoder_decoder_attention/key/kernel/v             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.911723 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/encoder_decoder_attention/key/kernel/v_col         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.911756 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/encoder_decoder_attention/key/kernel/v_row         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.911788 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/encoder_decoder_attention/out/kernel/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.911822 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/encoder_decoder_attention/out/kernel/v             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.911854 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/encoder_decoder_attention/out/kernel/v_col         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.911887 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/encoder_decoder_attention/out/kernel/v_row         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.911926 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/encoder_decoder_attention/query/kernel/m           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.911959 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/encoder_decoder_attention/query/kernel/v           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.911991 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/encoder_decoder_attention/query/kernel/v_col       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.912024 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/encoder_decoder_attention/query/kernel/v_row       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.912061 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/encoder_decoder_attention/value/kernel/m           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.912095 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/encoder_decoder_attention/value/kernel/v           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.912127 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/encoder_decoder_attention/value/kernel/v_col       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.912159 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/encoder_decoder_attention/value/kernel/v_row       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.912192 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.912225 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.912257 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/mlp/wi_0/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.912301 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/mlp/wi_0/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.912328 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.912356 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.912384 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/mlp/wi_1/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.912421 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/mlp/wi_1/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.912450 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.912478 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.912506 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/mlp/wo/kernel/v_col                                size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.912534 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/mlp/wo/kernel/v_row                                size 768          shape (768,)                                   partition spec None
I1007 02:01:22.912561 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/pre_cross_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.912592 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/pre_cross_attention_layer_norm/scale/v             size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.912625 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/pre_cross_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.912654 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/pre_cross_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.912682 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.912712 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v                         size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.912741 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.912768 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.912796 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I1007 02:01:22.912826 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v              size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.912855 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.912883 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.912916 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.912945 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.912973 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/self_attention/key/kernel/v_col                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.913001 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/self_attention/key/kernel/v_row                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.913029 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.913057 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.913084 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/self_attention/out/kernel/v_col                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.913112 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/self_attention/out/kernel/v_row                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.913143 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.913172 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.913200 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/self_attention/query/kernel/v_col                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.913227 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/self_attention/query/kernel/v_row                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.913255 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.913283 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.913310 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/self_attention/value/kernel/v_col                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.913338 140317719510080 utils.py:1364] Variable param_states/decoder/layers_4/self_attention/value/kernel/v_row                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.913365 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/encoder_decoder_attention/key/kernel/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.913393 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/encoder_decoder_attention/key/kernel/v             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.913422 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/encoder_decoder_attention/key/kernel/v_col         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.913450 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/encoder_decoder_attention/key/kernel/v_row         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.913477 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/encoder_decoder_attention/out/kernel/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.913506 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/encoder_decoder_attention/out/kernel/v             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.913533 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/encoder_decoder_attention/out/kernel/v_col         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.913561 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/encoder_decoder_attention/out/kernel/v_row         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.913589 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/encoder_decoder_attention/query/kernel/m           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.913617 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/encoder_decoder_attention/query/kernel/v           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.913649 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/encoder_decoder_attention/query/kernel/v_col       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.913678 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/encoder_decoder_attention/query/kernel/v_row       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.913706 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/encoder_decoder_attention/value/kernel/m           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.913734 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/encoder_decoder_attention/value/kernel/v           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.913770 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/encoder_decoder_attention/value/kernel/v_col       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.913799 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/encoder_decoder_attention/value/kernel/v_row       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.913827 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.913856 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.913884 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/mlp/wi_0/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.913916 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/mlp/wi_0/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.913944 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.913973 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.914001 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/mlp/wi_1/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.914029 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/mlp/wi_1/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.914056 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.914084 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.914112 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/mlp/wo/kernel/v_col                                size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.914140 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/mlp/wo/kernel/v_row                                size 768          shape (768,)                                   partition spec None
I1007 02:01:22.914171 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/pre_cross_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.914202 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/pre_cross_attention_layer_norm/scale/v             size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.914232 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/pre_cross_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.914260 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/pre_cross_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.914288 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.914318 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v                         size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.914347 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.914375 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.914403 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I1007 02:01:22.914433 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v              size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.914462 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.914490 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.914518 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.914546 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.914574 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/self_attention/key/kernel/v_col                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.914602 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/self_attention/key/kernel/v_row                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.914629 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.914657 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.914690 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/self_attention/out/kernel/v_col                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.914718 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/self_attention/out/kernel/v_row                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.914746 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.914774 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.914814 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/self_attention/query/kernel/v_col                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.914844 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/self_attention/query/kernel/v_row                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.914872 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.914906 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.914935 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/self_attention/value/kernel/v_col                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.914963 140317719510080 utils.py:1364] Variable param_states/decoder/layers_5/self_attention/value/kernel/v_row                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.914991 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/encoder_decoder_attention/key/kernel/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.915020 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/encoder_decoder_attention/key/kernel/v             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.915049 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/encoder_decoder_attention/key/kernel/v_col         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.915077 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/encoder_decoder_attention/key/kernel/v_row         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.915105 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/encoder_decoder_attention/out/kernel/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.915148 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/encoder_decoder_attention/out/kernel/v             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.915178 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/encoder_decoder_attention/out/kernel/v_col         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.915205 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/encoder_decoder_attention/out/kernel/v_row         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.915237 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/encoder_decoder_attention/query/kernel/m           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.915266 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/encoder_decoder_attention/query/kernel/v           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.915294 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/encoder_decoder_attention/query/kernel/v_col       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.915322 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/encoder_decoder_attention/query/kernel/v_row       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.915349 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/encoder_decoder_attention/value/kernel/m           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.915377 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/encoder_decoder_attention/value/kernel/v           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.915405 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/encoder_decoder_attention/value/kernel/v_col       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.915432 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/encoder_decoder_attention/value/kernel/v_row       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.915460 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.915488 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.915516 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/mlp/wi_0/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.915544 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/mlp/wi_0/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.915571 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.915599 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.915627 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/mlp/wi_1/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.915654 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/mlp/wi_1/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.915682 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.915710 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.915742 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/mlp/wo/kernel/v_col                                size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.915770 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/mlp/wo/kernel/v_row                                size 768          shape (768,)                                   partition spec None
I1007 02:01:22.915797 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/pre_cross_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.915828 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/pre_cross_attention_layer_norm/scale/v             size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.915858 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/pre_cross_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.915886 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/pre_cross_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.915921 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.915951 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v                         size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.915980 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.916008 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.916036 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I1007 02:01:22.916065 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v              size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.916094 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.916122 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.916150 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.916177 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.916205 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/self_attention/key/kernel/v_col                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.916232 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/self_attention/key/kernel/v_row                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.916263 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.916292 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.916319 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/self_attention/out/kernel/v_col                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.916347 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/self_attention/out/kernel/v_row                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.916374 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.916402 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.916429 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/self_attention/query/kernel/v_col                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.916457 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/self_attention/query/kernel/v_row                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.916493 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.916522 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.916550 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/self_attention/value/kernel/v_col                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.916578 140317719510080 utils.py:1364] Variable param_states/decoder/layers_6/self_attention/value/kernel/v_row                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.916605 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/encoder_decoder_attention/key/kernel/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.916633 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/encoder_decoder_attention/key/kernel/v             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.916662 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/encoder_decoder_attention/key/kernel/v_col         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.916690 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/encoder_decoder_attention/key/kernel/v_row         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.916718 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/encoder_decoder_attention/out/kernel/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.916746 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/encoder_decoder_attention/out/kernel/v             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.916777 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/encoder_decoder_attention/out/kernel/v_col         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.916806 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/encoder_decoder_attention/out/kernel/v_row         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.916834 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/encoder_decoder_attention/query/kernel/m           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.916862 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/encoder_decoder_attention/query/kernel/v           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.916894 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/encoder_decoder_attention/query/kernel/v_col       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.916923 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/encoder_decoder_attention/query/kernel/v_row       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.916951 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/encoder_decoder_attention/value/kernel/m           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.916979 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/encoder_decoder_attention/value/kernel/v           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.917007 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/encoder_decoder_attention/value/kernel/v_col       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.917035 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/encoder_decoder_attention/value/kernel/v_row       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.917063 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.917091 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.917119 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/mlp/wi_0/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.917146 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/mlp/wi_0/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.917174 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.917203 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.917231 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/mlp/wi_1/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.917258 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/mlp/wi_1/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.917290 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.917319 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.917346 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/mlp/wo/kernel/v_col                                size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.917374 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/mlp/wo/kernel/v_row                                size 768          shape (768,)                                   partition spec None
I1007 02:01:22.917402 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/pre_cross_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.917433 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/pre_cross_attention_layer_norm/scale/v             size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.917463 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/pre_cross_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.917493 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/pre_cross_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.917521 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.917552 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v                         size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.917581 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.917610 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.917638 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I1007 02:01:22.917668 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v              size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.917696 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.917725 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.917752 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.917781 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.917813 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/self_attention/key/kernel/v_col                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.917850 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/self_attention/key/kernel/v_row                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.917878 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.917916 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.917944 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/self_attention/out/kernel/v_col                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.917972 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/self_attention/out/kernel/v_row                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.918000 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.918028 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.918057 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/self_attention/query/kernel/v_col                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.918085 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/self_attention/query/kernel/v_row                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.918112 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.918141 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.918169 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/self_attention/value/kernel/v_col                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.918197 140317719510080 utils.py:1364] Variable param_states/decoder/layers_7/self_attention/value/kernel/v_row                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.918225 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/encoder_decoder_attention/key/kernel/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.918254 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/encoder_decoder_attention/key/kernel/v             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.918282 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/encoder_decoder_attention/key/kernel/v_col         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.918310 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/encoder_decoder_attention/key/kernel/v_row         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.918342 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/encoder_decoder_attention/out/kernel/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.918370 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/encoder_decoder_attention/out/kernel/v             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.918398 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/encoder_decoder_attention/out/kernel/v_col         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.918426 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/encoder_decoder_attention/out/kernel/v_row         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.918453 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/encoder_decoder_attention/query/kernel/m           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.918482 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/encoder_decoder_attention/query/kernel/v           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.918511 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/encoder_decoder_attention/query/kernel/v_col       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.918539 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/encoder_decoder_attention/query/kernel/v_row       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.918566 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/encoder_decoder_attention/value/kernel/m           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.918595 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/encoder_decoder_attention/value/kernel/v           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.918622 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/encoder_decoder_attention/value/kernel/v_col       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.918650 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/encoder_decoder_attention/value/kernel/v_row       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.918678 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.918707 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.918735 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/mlp/wi_0/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.918763 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/mlp/wi_0/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.918791 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.918832 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.918865 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/mlp/wi_1/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.918897 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/mlp/wi_1/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.918927 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.918956 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.918985 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/mlp/wo/kernel/v_col                                size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.919013 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/mlp/wo/kernel/v_row                                size 768          shape (768,)                                   partition spec None
I1007 02:01:22.919042 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/pre_cross_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.919074 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/pre_cross_attention_layer_norm/scale/v             size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.919104 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/pre_cross_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.919132 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/pre_cross_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.919160 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.919190 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v                         size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.919229 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.919259 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.919286 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I1007 02:01:22.919317 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v              size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.919346 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.919379 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.919407 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.919435 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.919463 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/self_attention/key/kernel/v_col                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.919491 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/self_attention/key/kernel/v_row                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.919518 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.919546 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.919574 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/self_attention/out/kernel/v_col                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.919602 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/self_attention/out/kernel/v_row                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.919630 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.919657 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.919685 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/self_attention/query/kernel/v_col                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.919713 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/self_attention/query/kernel/v_row                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.919741 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.919770 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.919797 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/self_attention/value/kernel/v_col                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.919825 140317719510080 utils.py:1364] Variable param_states/decoder/layers_8/self_attention/value/kernel/v_row                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.919853 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/encoder_decoder_attention/key/kernel/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.919886 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/encoder_decoder_attention/key/kernel/v             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.919921 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/encoder_decoder_attention/key/kernel/v_col         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.919950 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/encoder_decoder_attention/key/kernel/v_row         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.919978 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/encoder_decoder_attention/out/kernel/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.920006 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/encoder_decoder_attention/out/kernel/v             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.920034 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/encoder_decoder_attention/out/kernel/v_col         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.920062 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/encoder_decoder_attention/out/kernel/v_row         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.920090 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/encoder_decoder_attention/query/kernel/m           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.920121 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/encoder_decoder_attention/query/kernel/v           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.920150 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/encoder_decoder_attention/query/kernel/v_col       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.920178 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/encoder_decoder_attention/query/kernel/v_row       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.920227 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/encoder_decoder_attention/value/kernel/m           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.920261 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/encoder_decoder_attention/value/kernel/v           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.920294 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/encoder_decoder_attention/value/kernel/v_col       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.920327 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/encoder_decoder_attention/value/kernel/v_row       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.920360 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.920393 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.920427 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/mlp/wi_0/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.920464 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/mlp/wi_0/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.920497 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.920531 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.920564 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/mlp/wi_1/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.920598 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/mlp/wi_1/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.920630 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.920674 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.920708 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/mlp/wo/kernel/v_col                                size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.920741 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/mlp/wo/kernel/v_row                                size 768          shape (768,)                                   partition spec None
I1007 02:01:22.920774 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/pre_cross_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.920810 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/pre_cross_attention_layer_norm/scale/v             size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.920845 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/pre_cross_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.920878 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/pre_cross_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.920918 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.920954 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v                         size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.920990 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.921023 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.921056 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I1007 02:01:22.921097 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v              size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.921137 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.921171 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.921205 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.921239 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.921272 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/self_attention/key/kernel/v_col                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.921306 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/self_attention/key/kernel/v_row                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.921338 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.921372 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.921405 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/self_attention/out/kernel/v_col                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.921438 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/self_attention/out/kernel/v_row                    size 768          shape (768,)                                   partition spec None
I1007 02:01:22.921471 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.921505 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.921538 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/self_attention/query/kernel/v_col                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.921571 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/self_attention/query/kernel/v_row                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.921603 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.921637 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I1007 02:01:22.921670 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/self_attention/value/kernel/v_col                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.921708 140317719510080 utils.py:1364] Variable param_states/decoder/layers_9/self_attention/value/kernel/v_row                  size 768          shape (768,)                                   partition spec None
I1007 02:01:22.921741 140317719510080 utils.py:1364] Variable param_states/decoder/logits_dense/kernel/m                                       size 1            shape (1,)                                     partition spec None
I1007 02:01:22.921775 140317719510080 utils.py:1364] Variable param_states/decoder/logits_dense/kernel/v                                       size 1            shape (1,)                                     partition spec None
I1007 02:01:22.921809 140317719510080 utils.py:1364] Variable param_states/decoder/logits_dense/kernel/v_col                                   size 32128        shape (32128,)                                 partition spec None
I1007 02:01:22.921843 140317719510080 utils.py:1364] Variable param_states/decoder/logits_dense/kernel/v_row                                   size 768          shape (768,)                                   partition spec None
I1007 02:01:22.921875 140317719510080 utils.py:1364] Variable param_states/decoder/relpos_bias/rel_embedding/m                                 size 1            shape (1,)                                     partition spec None
I1007 02:01:22.921922 140317719510080 utils.py:1364] Variable param_states/decoder/relpos_bias/rel_embedding/v                                 size 384          shape (heads=12, relpos_buckets=32)            partition spec ('model', None)
I1007 02:01:22.921960 140317719510080 utils.py:1364] Variable param_states/decoder/relpos_bias/rel_embedding/v_col                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.921994 140317719510080 utils.py:1364] Variable param_states/decoder/relpos_bias/rel_embedding/v_row                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.922028 140317719510080 utils.py:1364] Variable param_states/encoder/encoder_norm/scale/m                                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.922066 140317719510080 utils.py:1364] Variable param_states/encoder/encoder_norm/scale/v                                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.922101 140317719510080 utils.py:1364] Variable param_states/encoder/encoder_norm/scale/v_col                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.922135 140317719510080 utils.py:1364] Variable param_states/encoder/encoder_norm/scale/v_row                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.922168 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/attention/key/kernel/m                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.922201 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/attention/key/kernel/v                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.922235 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/attention/key/kernel/v_col                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.922268 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/attention/key/kernel/v_row                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.922320 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/attention/out/kernel/m                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.922353 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/attention/out/kernel/v                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.922381 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/attention/out/kernel/v_col                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.922409 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/attention/out/kernel/v_row                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.922436 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/attention/query/kernel/m                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.922464 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/attention/query/kernel/v                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.922492 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/attention/query/kernel/v_col                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.922520 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/attention/query/kernel/v_row                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.922548 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/attention/value/kernel/m                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.922577 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/attention/value/kernel/v                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.922605 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/attention/value/kernel/v_col                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.922632 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/attention/value/kernel/v_row                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.922661 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.922689 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.922717 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/mlp/wi_0/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.922746 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/mlp/wi_0/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.922774 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.922802 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.922845 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/mlp/wi_1/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.922878 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/mlp/wi_1/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.922911 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.922940 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.922969 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/mlp/wo/kernel/v_col                                size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.922996 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/mlp/wo/kernel/v_row                                size 768          shape (768,)                                   partition spec None
I1007 02:01:22.923024 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/pre_attention_layer_norm/scale/m                   size 1            shape (1,)                                     partition spec None
I1007 02:01:22.923055 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/pre_attention_layer_norm/scale/v                   size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.923084 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/pre_attention_layer_norm/scale/v_col               size 1            shape (1,)                                     partition spec None
I1007 02:01:22.923112 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/pre_attention_layer_norm/scale/v_row               size 1            shape (1,)                                     partition spec None
I1007 02:01:22.923141 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.923171 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/pre_mlp_layer_norm/scale/v                         size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.923200 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.923228 140317719510080 utils.py:1364] Variable param_states/encoder/layers_0/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.923256 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/attention/key/kernel/m                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.923285 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/attention/key/kernel/v                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.923314 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/attention/key/kernel/v_col                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.923342 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/attention/key/kernel/v_row                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.923371 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/attention/out/kernel/m                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.923406 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/attention/out/kernel/v                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.923434 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/attention/out/kernel/v_col                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.923462 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/attention/out/kernel/v_row                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.923491 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/attention/query/kernel/m                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.923520 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/attention/query/kernel/v                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.923548 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/attention/query/kernel/v_col                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.923576 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/attention/query/kernel/v_row                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.923604 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/attention/value/kernel/m                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.923633 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/attention/value/kernel/v                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.923662 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/attention/value/kernel/v_col                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.923700 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/attention/value/kernel/v_row                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.923729 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.923758 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.923786 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/mlp/wi_0/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.923814 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/mlp/wi_0/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.923842 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.923870 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.923904 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/mlp/wi_1/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.923937 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/mlp/wi_1/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.923965 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.923994 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.924023 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/mlp/wo/kernel/v_col                                size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.924051 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/mlp/wo/kernel/v_row                                size 768          shape (768,)                                   partition spec None
I1007 02:01:22.924079 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/pre_attention_layer_norm/scale/m                   size 1            shape (1,)                                     partition spec None
I1007 02:01:22.924109 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/pre_attention_layer_norm/scale/v                   size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.924138 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/pre_attention_layer_norm/scale/v_col               size 1            shape (1,)                                     partition spec None
I1007 02:01:22.924167 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/pre_attention_layer_norm/scale/v_row               size 1            shape (1,)                                     partition spec None
I1007 02:01:22.924195 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.924225 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/pre_mlp_layer_norm/scale/v                         size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.924254 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.924282 140317719510080 utils.py:1364] Variable param_states/encoder/layers_1/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.924310 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/attention/key/kernel/m                            size 1            shape (1,)                                     partition spec None
I1007 02:01:22.924339 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/attention/key/kernel/v                            size 1            shape (1,)                                     partition spec None
I1007 02:01:22.924367 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/attention/key/kernel/v_col                        size 768          shape (768,)                                   partition spec None
I1007 02:01:22.924394 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/attention/key/kernel/v_row                        size 768          shape (768,)                                   partition spec None
I1007 02:01:22.924422 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/attention/out/kernel/m                            size 1            shape (1,)                                     partition spec None
I1007 02:01:22.924455 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/attention/out/kernel/v                            size 1            shape (1,)                                     partition spec None
I1007 02:01:22.924483 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/attention/out/kernel/v_col                        size 768          shape (768,)                                   partition spec None
I1007 02:01:22.924511 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/attention/out/kernel/v_row                        size 768          shape (768,)                                   partition spec None
I1007 02:01:22.924539 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/attention/query/kernel/m                          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.924567 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/attention/query/kernel/v                          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.924595 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/attention/query/kernel/v_col                      size 768          shape (768,)                                   partition spec None
I1007 02:01:22.924623 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/attention/query/kernel/v_row                      size 768          shape (768,)                                   partition spec None
I1007 02:01:22.924650 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/attention/value/kernel/m                          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.924678 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/attention/value/kernel/v                          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.924706 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/attention/value/kernel/v_col                      size 768          shape (768,)                                   partition spec None
I1007 02:01:22.924734 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/attention/value/kernel/v_row                      size 768          shape (768,)                                   partition spec None
I1007 02:01:22.924761 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I1007 02:01:22.924789 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I1007 02:01:22.924818 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/mlp/wi_0/kernel/v_col                             size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.924845 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/mlp/wi_0/kernel/v_row                             size 768          shape (768,)                                   partition spec None
I1007 02:01:22.924873 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I1007 02:01:22.924907 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I1007 02:01:22.924936 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/mlp/wi_1/kernel/v_col                             size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.924967 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/mlp/wi_1/kernel/v_row                             size 768          shape (768,)                                   partition spec None
I1007 02:01:22.924995 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I1007 02:01:22.925024 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I1007 02:01:22.925061 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/mlp/wo/kernel/v_col                               size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.925090 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/mlp/wo/kernel/v_row                               size 768          shape (768,)                                   partition spec None
I1007 02:01:22.925117 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/pre_attention_layer_norm/scale/m                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.925148 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/pre_attention_layer_norm/scale/v                  size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.925178 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/pre_attention_layer_norm/scale/v_col              size 1            shape (1,)                                     partition spec None
I1007 02:01:22.925207 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/pre_attention_layer_norm/scale/v_row              size 1            shape (1,)                                     partition spec None
I1007 02:01:22.925235 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.925265 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/pre_mlp_layer_norm/scale/v                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.925294 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.925323 140317719510080 utils.py:1364] Variable param_states/encoder/layers_10/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.925351 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/attention/key/kernel/m                            size 1            shape (1,)                                     partition spec None
I1007 02:01:22.925380 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/attention/key/kernel/v                            size 1            shape (1,)                                     partition spec None
I1007 02:01:22.925409 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/attention/key/kernel/v_col                        size 768          shape (768,)                                   partition spec None
I1007 02:01:22.925437 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/attention/key/kernel/v_row                        size 768          shape (768,)                                   partition spec None
I1007 02:01:22.925465 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/attention/out/kernel/m                            size 1            shape (1,)                                     partition spec None
I1007 02:01:22.925498 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/attention/out/kernel/v                            size 1            shape (1,)                                     partition spec None
I1007 02:01:22.925526 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/attention/out/kernel/v_col                        size 768          shape (768,)                                   partition spec None
I1007 02:01:22.925554 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/attention/out/kernel/v_row                        size 768          shape (768,)                                   partition spec None
I1007 02:01:22.925582 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/attention/query/kernel/m                          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.925611 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/attention/query/kernel/v                          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.925639 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/attention/query/kernel/v_col                      size 768          shape (768,)                                   partition spec None
I1007 02:01:22.925667 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/attention/query/kernel/v_row                      size 768          shape (768,)                                   partition spec None
I1007 02:01:22.925694 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/attention/value/kernel/m                          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.925723 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/attention/value/kernel/v                          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.925751 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/attention/value/kernel/v_col                      size 768          shape (768,)                                   partition spec None
I1007 02:01:22.925779 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/attention/value/kernel/v_row                      size 768          shape (768,)                                   partition spec None
I1007 02:01:22.925807 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I1007 02:01:22.925836 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I1007 02:01:22.925864 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/mlp/wi_0/kernel/v_col                             size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.925896 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/mlp/wi_0/kernel/v_row                             size 768          shape (768,)                                   partition spec None
I1007 02:01:22.925925 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I1007 02:01:22.925954 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I1007 02:01:22.925981 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/mlp/wi_1/kernel/v_col                             size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.926014 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/mlp/wi_1/kernel/v_row                             size 768          shape (768,)                                   partition spec None
I1007 02:01:22.926042 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I1007 02:01:22.926070 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I1007 02:01:22.926099 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/mlp/wo/kernel/v_col                               size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.926126 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/mlp/wo/kernel/v_row                               size 768          shape (768,)                                   partition spec None
I1007 02:01:22.926154 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/pre_attention_layer_norm/scale/m                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.926185 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/pre_attention_layer_norm/scale/v                  size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.926214 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/pre_attention_layer_norm/scale/v_col              size 1            shape (1,)                                     partition spec None
I1007 02:01:22.926243 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/pre_attention_layer_norm/scale/v_row              size 1            shape (1,)                                     partition spec None
I1007 02:01:22.926271 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I1007 02:01:22.926301 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/pre_mlp_layer_norm/scale/v                        size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.926330 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.926357 140317719510080 utils.py:1364] Variable param_states/encoder/layers_11/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.926385 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/attention/key/kernel/m                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.926423 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/attention/key/kernel/v                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.926452 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/attention/key/kernel/v_col                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.926480 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/attention/key/kernel/v_row                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.926508 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/attention/out/kernel/m                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.926539 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/attention/out/kernel/v                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.926568 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/attention/out/kernel/v_col                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.926595 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/attention/out/kernel/v_row                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.926624 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/attention/query/kernel/m                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.926652 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/attention/query/kernel/v                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.926680 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/attention/query/kernel/v_col                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.926707 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/attention/query/kernel/v_row                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.926735 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/attention/value/kernel/m                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.926763 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/attention/value/kernel/v                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.926791 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/attention/value/kernel/v_col                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.926832 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/attention/value/kernel/v_row                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.926861 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.926894 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.926926 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/mlp/wi_0/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.926958 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/mlp/wi_0/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.926986 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.927014 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.927042 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/mlp/wi_1/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.927075 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/mlp/wi_1/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.927103 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.927132 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.927162 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/mlp/wo/kernel/v_col                                size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.927190 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/mlp/wo/kernel/v_row                                size 768          shape (768,)                                   partition spec None
I1007 02:01:22.927218 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/pre_attention_layer_norm/scale/m                   size 1            shape (1,)                                     partition spec None
I1007 02:01:22.927249 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/pre_attention_layer_norm/scale/v                   size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.927278 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/pre_attention_layer_norm/scale/v_col               size 1            shape (1,)                                     partition spec None
I1007 02:01:22.927307 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/pre_attention_layer_norm/scale/v_row               size 1            shape (1,)                                     partition spec None
I1007 02:01:22.927335 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.927364 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/pre_mlp_layer_norm/scale/v                         size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.927394 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.927421 140317719510080 utils.py:1364] Variable param_states/encoder/layers_2/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.927448 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/attention/key/kernel/m                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.927477 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/attention/key/kernel/v                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.927504 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/attention/key/kernel/v_col                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.927532 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/attention/key/kernel/v_row                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.927559 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/attention/out/kernel/m                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.927592 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/attention/out/kernel/v                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.927620 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/attention/out/kernel/v_col                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.927648 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/attention/out/kernel/v_row                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.927675 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/attention/query/kernel/m                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.927703 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/attention/query/kernel/v                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.927730 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/attention/query/kernel/v_col                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.927758 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/attention/query/kernel/v_row                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.927795 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/attention/value/kernel/m                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.927824 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/attention/value/kernel/v                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.927851 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/attention/value/kernel/v_col                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.927879 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/attention/value/kernel/v_row                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.927912 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.927941 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.927970 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/mlp/wi_0/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.927998 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/mlp/wi_0/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.928026 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.928054 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.928083 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/mlp/wi_1/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.928114 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/mlp/wi_1/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.928143 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.928172 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.928201 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/mlp/wo/kernel/v_col                                size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.928229 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/mlp/wo/kernel/v_row                                size 768          shape (768,)                                   partition spec None
I1007 02:01:22.928256 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/pre_attention_layer_norm/scale/m                   size 1            shape (1,)                                     partition spec None
I1007 02:01:22.928287 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/pre_attention_layer_norm/scale/v                   size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.928316 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/pre_attention_layer_norm/scale/v_col               size 1            shape (1,)                                     partition spec None
I1007 02:01:22.928343 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/pre_attention_layer_norm/scale/v_row               size 1            shape (1,)                                     partition spec None
I1007 02:01:22.928371 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.928401 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/pre_mlp_layer_norm/scale/v                         size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.928429 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.928457 140317719510080 utils.py:1364] Variable param_states/encoder/layers_3/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.928484 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/attention/key/kernel/m                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.928512 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/attention/key/kernel/v                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.928540 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/attention/key/kernel/v_col                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.928568 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/attention/key/kernel/v_row                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.928596 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/attention/out/kernel/m                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.928627 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/attention/out/kernel/v                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.928656 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/attention/out/kernel/v_col                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.928684 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/attention/out/kernel/v_row                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.928712 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/attention/query/kernel/m                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.928740 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/attention/query/kernel/v                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.928769 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/attention/query/kernel/v_col                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.928797 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/attention/query/kernel/v_row                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.928825 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/attention/value/kernel/m                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.928853 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/attention/value/kernel/v                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.928880 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/attention/value/kernel/v_col                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.928912 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/attention/value/kernel/v_row                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.928941 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.928969 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.928997 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/mlp/wi_0/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.929025 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/mlp/wi_0/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.929052 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.929080 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.929107 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/mlp/wi_1/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.929149 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/mlp/wi_1/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.929177 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.929206 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.929234 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/mlp/wo/kernel/v_col                                size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.929262 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/mlp/wo/kernel/v_row                                size 768          shape (768,)                                   partition spec None
I1007 02:01:22.929289 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/pre_attention_layer_norm/scale/m                   size 1            shape (1,)                                     partition spec None
I1007 02:01:22.929319 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/pre_attention_layer_norm/scale/v                   size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.929349 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/pre_attention_layer_norm/scale/v_col               size 1            shape (1,)                                     partition spec None
I1007 02:01:22.929378 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/pre_attention_layer_norm/scale/v_row               size 1            shape (1,)                                     partition spec None
I1007 02:01:22.929405 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.929435 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/pre_mlp_layer_norm/scale/v                         size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.929464 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.929492 140317719510080 utils.py:1364] Variable param_states/encoder/layers_4/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.929520 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/attention/key/kernel/m                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.929548 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/attention/key/kernel/v                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.929576 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/attention/key/kernel/v_col                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.929604 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/attention/key/kernel/v_row                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.929632 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/attention/out/kernel/m                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.929663 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/attention/out/kernel/v                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.929692 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/attention/out/kernel/v_col                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.929720 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/attention/out/kernel/v_row                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.929747 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/attention/query/kernel/m                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.929775 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/attention/query/kernel/v                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.929803 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/attention/query/kernel/v_col                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.929831 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/attention/query/kernel/v_row                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.929858 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/attention/value/kernel/m                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.929886 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/attention/value/kernel/v                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.929920 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/attention/value/kernel/v_col                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.929949 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/attention/value/kernel/v_row                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.929977 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.930005 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.930033 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/mlp/wi_0/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.930062 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/mlp/wi_0/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.930090 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.930122 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.930172 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/mlp/wi_1/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.930205 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/mlp/wi_1/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.930238 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.930272 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.930305 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/mlp/wo/kernel/v_col                                size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.930339 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/mlp/wo/kernel/v_row                                size 768          shape (768,)                                   partition spec None
I1007 02:01:22.930371 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/pre_attention_layer_norm/scale/m                   size 1            shape (1,)                                     partition spec None
I1007 02:01:22.930408 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/pre_attention_layer_norm/scale/v                   size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.930442 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/pre_attention_layer_norm/scale/v_col               size 1            shape (1,)                                     partition spec None
I1007 02:01:22.930475 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/pre_attention_layer_norm/scale/v_row               size 1            shape (1,)                                     partition spec None
I1007 02:01:22.930509 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.930544 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/pre_mlp_layer_norm/scale/v                         size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.930590 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.930624 140317719510080 utils.py:1364] Variable param_states/encoder/layers_5/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.930657 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/attention/key/kernel/m                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.930691 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/attention/key/kernel/v                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.930724 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/attention/key/kernel/v_col                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.930757 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/attention/key/kernel/v_row                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.930817 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/attention/out/kernel/m                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.930857 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/attention/out/kernel/v                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.930896 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/attention/out/kernel/v_col                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.930931 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/attention/out/kernel/v_row                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.930964 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/attention/query/kernel/m                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.930997 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/attention/query/kernel/v                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.931030 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/attention/query/kernel/v_col                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.931064 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/attention/query/kernel/v_row                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.931096 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/attention/value/kernel/m                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.931135 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/attention/value/kernel/v                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.931168 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/attention/value/kernel/v_col                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.931202 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/attention/value/kernel/v_row                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.931235 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.931268 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.931301 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/mlp/wi_0/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.931334 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/mlp/wi_0/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.931367 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.931401 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.931439 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/mlp/wi_1/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.931472 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/mlp/wi_1/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.931504 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.931538 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.931571 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/mlp/wo/kernel/v_col                                size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.931604 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/mlp/wo/kernel/v_row                                size 768          shape (768,)                                   partition spec None
I1007 02:01:22.931637 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/pre_attention_layer_norm/scale/m                   size 1            shape (1,)                                     partition spec None
I1007 02:01:22.931673 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/pre_attention_layer_norm/scale/v                   size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.931708 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/pre_attention_layer_norm/scale/v_col               size 1            shape (1,)                                     partition spec None
I1007 02:01:22.931742 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/pre_attention_layer_norm/scale/v_row               size 1            shape (1,)                                     partition spec None
I1007 02:01:22.931775 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.931810 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/pre_mlp_layer_norm/scale/v                         size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.931844 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.931877 140317719510080 utils.py:1364] Variable param_states/encoder/layers_6/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.931916 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/attention/key/kernel/m                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.931950 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/attention/key/kernel/v                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.931983 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/attention/key/kernel/v_col                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.932017 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/attention/key/kernel/v_row                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.932054 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/attention/out/kernel/m                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.932088 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/attention/out/kernel/v                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.932121 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/attention/out/kernel/v_col                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.932154 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/attention/out/kernel/v_row                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.932188 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/attention/query/kernel/m                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.932232 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/attention/query/kernel/v                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.932268 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/attention/query/kernel/v_col                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.932316 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/attention/query/kernel/v_row                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.932344 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/attention/value/kernel/m                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.932373 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/attention/value/kernel/v                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.932400 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/attention/value/kernel/v_col                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.932428 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/attention/value/kernel/v_row                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.932456 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.932484 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.932512 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/mlp/wi_0/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.932540 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/mlp/wi_0/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.932567 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.932595 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.932627 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/mlp/wi_1/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.932655 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/mlp/wi_1/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.932682 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.932710 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.932738 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/mlp/wo/kernel/v_col                                size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.932766 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/mlp/wo/kernel/v_row                                size 768          shape (768,)                                   partition spec None
I1007 02:01:22.932793 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/pre_attention_layer_norm/scale/m                   size 1            shape (1,)                                     partition spec None
I1007 02:01:22.932823 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/pre_attention_layer_norm/scale/v                   size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.932852 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/pre_attention_layer_norm/scale/v_col               size 1            shape (1,)                                     partition spec None
I1007 02:01:22.932880 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/pre_attention_layer_norm/scale/v_row               size 1            shape (1,)                                     partition spec None
I1007 02:01:22.932917 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.932950 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/pre_mlp_layer_norm/scale/v                         size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.932980 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.933008 140317719510080 utils.py:1364] Variable param_states/encoder/layers_7/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.933036 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/attention/key/kernel/m                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.933065 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/attention/key/kernel/v                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.933093 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/attention/key/kernel/v_col                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.933121 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/attention/key/kernel/v_row                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.933154 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/attention/out/kernel/m                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.933182 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/attention/out/kernel/v                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.933210 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/attention/out/kernel/v_col                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.933238 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/attention/out/kernel/v_row                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.933266 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/attention/query/kernel/m                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.933295 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/attention/query/kernel/v                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.933323 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/attention/query/kernel/v_col                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.933351 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/attention/query/kernel/v_row                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.933379 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/attention/value/kernel/m                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.933407 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/attention/value/kernel/v                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.933435 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/attention/value/kernel/v_col                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.933463 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/attention/value/kernel/v_row                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.933491 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.933519 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.933547 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/mlp/wi_0/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.933575 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/mlp/wi_0/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.933614 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.933644 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.933676 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/mlp/wi_1/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.933704 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/mlp/wi_1/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.933732 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.933761 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.933789 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/mlp/wo/kernel/v_col                                size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.933816 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/mlp/wo/kernel/v_row                                size 768          shape (768,)                                   partition spec None
I1007 02:01:22.933844 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/pre_attention_layer_norm/scale/m                   size 1            shape (1,)                                     partition spec None
I1007 02:01:22.933874 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/pre_attention_layer_norm/scale/v                   size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.933910 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/pre_attention_layer_norm/scale/v_col               size 1            shape (1,)                                     partition spec None
I1007 02:01:22.933938 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/pre_attention_layer_norm/scale/v_row               size 1            shape (1,)                                     partition spec None
I1007 02:01:22.933965 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.933995 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/pre_mlp_layer_norm/scale/v                         size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.934023 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.934051 140317719510080 utils.py:1364] Variable param_states/encoder/layers_8/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.934078 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/attention/key/kernel/m                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.934107 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/attention/key/kernel/v                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.934134 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/attention/key/kernel/v_col                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.934162 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/attention/key/kernel/v_row                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.934194 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/attention/out/kernel/m                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.934223 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/attention/out/kernel/v                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.934251 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/attention/out/kernel/v_col                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.934279 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/attention/out/kernel/v_row                         size 768          shape (768,)                                   partition spec None
I1007 02:01:22.934307 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/attention/query/kernel/m                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.934335 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/attention/query/kernel/v                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.934363 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/attention/query/kernel/v_col                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.934391 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/attention/query/kernel/v_row                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.934418 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/attention/value/kernel/m                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.934446 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/attention/value/kernel/v                           size 1            shape (1,)                                     partition spec None
I1007 02:01:22.934474 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/attention/value/kernel/v_col                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.934501 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/attention/value/kernel/v_row                       size 768          shape (768,)                                   partition spec None
I1007 02:01:22.934528 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.934556 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.934584 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/mlp/wi_0/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.934612 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/mlp/wi_0/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.934639 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.934667 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I1007 02:01:22.934699 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/mlp/wi_1/kernel/v_col                              size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.934727 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/mlp/wi_1/kernel/v_row                              size 768          shape (768,)                                   partition spec None
I1007 02:01:22.934754 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.934781 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I1007 02:01:22.934821 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/mlp/wo/kernel/v_col                                size 2048         shape (2048,)                                  partition spec None
I1007 02:01:22.934851 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/mlp/wo/kernel/v_row                                size 768          shape (768,)                                   partition spec None
I1007 02:01:22.934879 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/pre_attention_layer_norm/scale/m                   size 1            shape (1,)                                     partition spec None
I1007 02:01:22.934915 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/pre_attention_layer_norm/scale/v                   size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.934946 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/pre_attention_layer_norm/scale/v_col               size 1            shape (1,)                                     partition spec None
I1007 02:01:22.934987 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/pre_attention_layer_norm/scale/v_row               size 1            shape (1,)                                     partition spec None
I1007 02:01:22.935016 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I1007 02:01:22.935046 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/pre_mlp_layer_norm/scale/v                         size 768          shape (embed=768)                              partition spec (None,)
I1007 02:01:22.935075 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.935103 140317719510080 utils.py:1364] Variable param_states/encoder/layers_9/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I1007 02:01:22.935131 140317719510080 utils.py:1364] Variable param_states/encoder/relpos_bias/rel_embedding/m                                 size 1            shape (1,)                                     partition spec None
I1007 02:01:22.935162 140317719510080 utils.py:1364] Variable param_states/encoder/relpos_bias/rel_embedding/v                                 size 384          shape (heads=12, relpos_buckets=32)            partition spec ('model', None)
I1007 02:01:22.935193 140317719510080 utils.py:1364] Variable param_states/encoder/relpos_bias/rel_embedding/v_col                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.935222 140317719510080 utils.py:1364] Variable param_states/encoder/relpos_bias/rel_embedding/v_row                             size 1            shape (1,)                                     partition spec None
I1007 02:01:22.935254 140317719510080 utils.py:1364] Variable param_states/token_embedder/embedding/m                                          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.935285 140317719510080 utils.py:1364] Variable param_states/token_embedder/embedding/v                                          size 1            shape (1,)                                     partition spec None
I1007 02:01:22.935313 140317719510080 utils.py:1364] Variable param_states/token_embedder/embedding/v_col                                      size 32128        shape (32128,)                                 partition spec None
I1007 02:01:22.935342 140317719510080 utils.py:1364] Variable param_states/token_embedder/embedding/v_row                                      size 768          shape (768,)                                   partition spec None
I1007 02:01:22.935370 140317719510080 utils.py:1364] Variable step                                                                             size 1            shape ()                                       partition spec None
I1007 02:01:23.434377 140304280442624 logging_writer.py:64] [0] collection=train Got texts: {'config': "    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    import seqio\n    import t5.data.mixtures\n    from t5x import adafactor\n    from t5x.examples.t5 import network\n    from t5x import gin_utils\n    from t5x import models\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    import tasks\n    \n#### Macros:\n\n    BATCH_SIZE = 256\n    DROPOUT_RATE = 0.0\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = None\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'arabic_dataset'\n    MODEL = @models.EncoderDecoderModel()\n    MODEL_DIR = 'gs://sultan-t5x/arabict5_tinyy/'\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 114}\n    TRAIN_STEPS = 100000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = \\\n        @adafactor.standard_logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = None\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for models.EncoderDecoderModel:\n\n    models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n    models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.EncoderDecoderModel.module = @network.Transformer()\n    models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n    models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n    models.EncoderDecoderModel.z_loss = %Z_LOSS\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = None\n    utils.SaveCheckpointConfig.period = 1000\n    utils.SaveCheckpointConfig.save_dataset = False\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 100\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://sultan-t5x/spiece.model'\n    \n#### Parameters for network.T5Config:\n\n    network.T5Config.dropout_rate = %DROPOUT_RATE\n    network.T5Config.dtype = 'bfloat16'\n    network.T5Config.emb_dim = 768\n    network.T5Config.head_dim = 64\n    network.T5Config.logits_via_embedding = False\n    network.T5Config.mlp_activations = ('gelu', 'linear')\n    network.T5Config.mlp_dim = 2048\n    network.T5Config.num_decoder_layers = 12\n    network.T5Config.num_encoder_layers = 12\n    network.T5Config.num_heads = 12\n    network.T5Config.vocab_size = 32128\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = 2000\n    train_script.train.eval_steps = 20\n    train_script.train.infer_eval_dataset_cfg = None\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @partitioning.PjitPartitioner()\n    train_script.train.random_seed = %RANDOM_SEED\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n    train_script.train.trainer_cls = @trainer.Trainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for network.Transformer:\n\n    network.Transformer.config = @network.T5Config()"}.
I1007 02:01:23.575542 140304272049920 logging_writer.py:48] [0] collection=train timing/init_or_restore_seconds=9.45379
I1007 02:01:23.577123 140317719510080 train.py:570] Saving checkpoint before the training loop starts.
I1007 02:01:23.642703 140317719510080 checkpoints.py:781] Skipping save checkpoint for step 0 (directory gs://sultan-t5x/arabict5_tinyy/checkpoint_0 already exists)
I1007 02:01:23.642793 140317719510080 train.py:584] Starting training loop.
I1007 02:01:23.642864 140317719510080 train.py:611] Starting main loop over steps 0-100000
I1007 02:01:23.642921 140317719510080 train.py:618] Training with artificial "epochs" of 1000 steps.
I1007 02:01:23.642962 140317719510080 train.py:622] Compiling train loop.
W1007 02:01:28.465408 140317719510080 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:28.485978 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:28.512676 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:28.526279 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:28.539644 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:28.553622 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:28.578930 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:28.592932 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:28.616166 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:28.626980 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:28.637243 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:28.647372 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:28.661497 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:28.675062 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:28.688193 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:28.701428 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:28.715253 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:28.728671 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:28.742362 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:28.756407 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:28.770467 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:28.784204 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:28.797745 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:28.807794 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:28.817831 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:28.828233 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:28.841848 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:28.856197 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:28.870387 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:28.884304 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:28.897679 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:28.911340 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:28.925787 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:28.938981 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:28.952995 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:28.966447 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:28.980476 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:28.990509 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:29.000756 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:29.010986 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.024943 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:29.038443 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.052363 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.066598 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.080990 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:29.094954 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.108638 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.122433 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.136297 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.150532 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:29.164673 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:29.174744 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:29.184998 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:29.195261 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.209579 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:29.223579 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.237065 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.250885 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.265043 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:29.279733 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.294718 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.309416 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.324622 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.339804 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:29.355726 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:29.366555 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:29.667013 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:29.676092 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.689118 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:29.702694 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.716139 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.729742 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.743470 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:29.756770 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.770948 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.785048 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.798572 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.813077 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:29.827159 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:29.837448 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:29.846756 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:29.856295 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.869803 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:29.885241 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.899277 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.914164 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.927912 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:29.941541 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.955511 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.969370 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.983214 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:29.995980 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:30.008439 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:30.017654 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:30.026738 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:30.036400 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.049025 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:30.061549 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.074291 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.086795 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.099493 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:30.112461 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.125029 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.138972 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.151703 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.165395 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:30.177980 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:30.187473 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:30.196534 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:30.205448 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.217922 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:30.230397 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.243610 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.256875 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.269854 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:30.282727 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.295676 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.311148 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.325308 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.339194 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:30.353541 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:30.364088 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:30.374498 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:30.384902 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.399162 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:30.413506 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.427482 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.441878 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.455896 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:30.469830 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.484132 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.498302 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.512451 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.526783 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:30.540652 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:30.550973 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:30.561335 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:30.571614 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.586062 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:30.600199 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.614020 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.627120 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.640647 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:30.654421 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.667911 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.682000 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.695641 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.709248 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:30.722957 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:30.734434 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:30.744501 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:30.754559 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.768499 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:30.781927 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.796061 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.809708 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.824095 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:30.838356 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.854226 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.868669 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.883538 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.897755 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:30.911958 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:30.922240 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:30.932565 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:30.942891 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.958245 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:30.972869 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:30.986680 140317719510080 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.000828 140317719510080 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.026871 140317719510080 adafactor.py:358] Since rank of parameter decoder/relpos_bias/rel_embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W1007 02:01:31.043949 140317719510080 adafactor.py:358] Since rank of parameter encoder/encoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:31.054313 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_0/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.069591 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_0/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:31.084643 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_0/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.098863 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_0/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.113091 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.127252 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.140901 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:31.155201 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_0/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:31.166160 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:31.176730 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_1/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.191037 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_1/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:31.205400 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_1/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.219513 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_1/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.237090 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.251888 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.266416 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:31.280439 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_1/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:31.290767 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:31.301206 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_10/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.316125 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_10/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:31.330236 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_10/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.344530 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_10/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.358746 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.373149 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.387152 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:31.401494 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_10/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:31.412156 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:31.422708 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_11/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.436846 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_11/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:31.451103 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_11/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.465623 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_11/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.479712 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_11/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.494011 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_11/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.508123 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_11/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:31.522256 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_11/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:31.532728 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:31.543104 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_2/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.556995 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_2/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:31.571287 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_2/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.585553 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_2/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.599681 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.614470 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.628710 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:31.643241 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_2/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:31.653633 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:31.664263 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_3/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.678261 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_3/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:31.692490 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_3/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.706359 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_3/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.720395 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.734649 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.748438 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:31.763179 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_3/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:31.773532 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:31.783918 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_4/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.797946 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_4/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:31.812288 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_4/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.826421 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_4/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.840511 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.855047 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.869098 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:31.883805 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_4/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:31.894147 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:31.904511 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_5/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.919110 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_5/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:31.933474 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_5/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.947437 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_5/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.961847 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_5/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.975995 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_5/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:31.990113 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_5/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:32.004478 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_5/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:32.014974 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:32.025402 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_6/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:32.039255 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_6/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:32.053452 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_6/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:32.067956 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_6/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:32.082159 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:32.096460 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:32.110297 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:32.124458 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_6/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:32.134833 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:32.145242 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_7/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:32.159435 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_7/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:32.173698 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_7/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:32.187762 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_7/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:32.201932 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:32.216795 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:32.231111 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:32.245364 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_7/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:32.255794 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:32.266114 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_8/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:32.280200 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_8/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:32.294360 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_8/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:32.309062 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_8/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:32.323193 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:32.337021 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:32.350741 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:32.365510 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_8/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:32.375689 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:32.386049 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_9/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:32.399806 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_9/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:32.414016 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_9/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:32.428389 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_9/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:32.442870 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:32.457028 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W1007 02:01:32.470762 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W1007 02:01:32.484670 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_9/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:32.495236 140317719510080 adafactor.py:358] Since rank of parameter encoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W1007 02:01:32.506002 140317719510080 adafactor.py:358] Since rank of parameter encoder/relpos_bias/rel_embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W1007 02:01:32.516274 140317719510080 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
A few raw validation examples...
b'gs://sultan-t5x/arabic_text/ArWiki01.txt'
{'title': b'None', 'text': b'\xd9\x88\xd9\x84\xd9\x83\xd9\x86 \xd9\x8a\xd9\x85\xd9\x83\xd9\x86 \xd8\xa7\xd9\x84\xd8\xa7\xd9\x82\xd8\xaa\xd8\xb1\xd8\xa7\xd8\xa8 \xd9\x85\xd9\x86 \xd9\x86\xd8\xaa\xd8\xa7\xd8\xa6\xd8\xac \xd8\xaf\xd9\x82\xd9\x8a\xd9\x82\xd8\xa9 \xd8\xb9\xd9\x86 \xd8\xb7\xd8\xb1\xd9\x8a\xd9\x82 \xd8\xa5\xd8\xaf\xd8\xae\xd8\xa7\xd9\x84 "\xd9\x85\xd8\xb9\xd8\xa7\xd9\x85\xd9\x84 \xd9\x84\xd9\x84\xd8\xb4\xd9\x83\xd9\x84 " P \xd9\x81\xd9\x8a \xd8\xa7\xd9\x84\xd9\x85\xd8\xb9\xd8\xa7\xd8\xaf\xd9\x84\xd8\xa9 :'}
{'title': b'None', 'text': b'\xd9\x8a\xd8\xa8\xd9\x82\xd9\x89 \xd8\xa8\xd8\xb9\xd8\xaf\xd9\x87 345 \xd9\x8a\xd9\x88\xd9\x85\xd9\x8b\xd8\xa7 \xd9\x84\xd8\xa7\xd9\x86\xd8\xaa\xd9\x87\xd8\xa7\xd8\xa1 \xd8\xa7\xd9\x84\xd8\xb3\xd9\x86\xd8\xa9\xd8\x8c \xd8\xa3\xd9\x88 346 \xd9\x8a\xd9\x88\xd9\x85\xd9\x8b\xd8\xa7 \xd9\x81\xd9\x8a \xd8\xa7\xd9\x84\xd8\xb3\xd9\x86\xd9\x88\xd8\xa7\xd8\xaa \xd8\xa7\xd9\x84\xd9\x83\xd8\xa8\xd9\x8a\xd8\xb3\xd8\xa9.'}
{'title': b'None', 'text': b'\xd9\x85\xd8\xae\xd8\xb1\xd8\xac '}
{'title': b'None', 'text': b'\xd9\x88\xd8\xa7\xd9\x84\xd8\xae\xd8\xb1\xd9\x8a\xd8\xb7\xd8\xa9 \xd8\xa8\xd9\x87\xd8\xb0\xd8\xa7 \xd8\xa7\xd9\x84\xd8\xa7\xd8\xb9\xd8\xaa\xd8\xa8\xd8\xa7\xd8\xb1 \xd9\x82\xd8\xaf\xd9\x8a\xd9\x85\xd8\xa9 \xd9\x82\xd8\xaf\xd9\x85 \xd8\xad\xd8\xb6\xd8\xa7\xd8\xb1\xd8\xa9 \xd8\xa7\xd9\x84\xd8\xa5\xd9\x86\xd8\xb3\xd8\xa7\xd9\x86\xd8\x8c \xd9\x81\xd9\x85\xd9\x86\xd8\xb0 \xd8\xa7\xd9\x84\xd9\x82\xd8\xaf\xd9\x85 \xd8\xa7\xd8\xb3\xd8\xaa\xd8\xb9\xd8\xa7\xd9\x86 \xd8\xa7\xd9\x84\xd8\xa5\xd9\x86\xd8\xb3\xd8\xa7\xd9\x86 \xd8\xa8\xd8\xaa\xd9\x88\xd8\xb2\xd9\x8a\xd8\xb9 \xd8\xa7\xd9\x84\xd8\xb8\xd8\xa7\xd9\x87\xd8\xb1\xd8\xa7\xd8\xaa \xd8\xa7\xd9\x84\xd8\xb7\xd8\xa8\xd9\x8a\xd8\xb9\xd9\x8a\xd8\xa9 \xd9\x88\xd8\xa7\xd9\x84\xd8\xa8\xd8\xb4\xd8\xb1\xd9\x8a\xd8\xa9 \xd8\xa8\xd8\xa7\xd9\x84\xd9\x88\xd8\xb5\xd9\x81 \xd9\x88\xd8\xa7\xd9\x84\xd8\xb1\xd8\xb3\xd9\x85.'}
{'title': b'None', 'text': b'\xd9\x88\xd9\x87\xd9\x86\xd8\xa7\xd9\x83 \xd8\xa7\xd9\x84\xd8\xb9\xd8\xaf\xd9\x8a\xd8\xaf \xd9\x85\xd9\x86 \xd8\xa7\xd9\x84\xd9\x86\xd8\xb8\xd8\xb1\xd9\x8a\xd8\xa7\xd8\xaa \xd8\xa7\xd9\x84\xd8\xad\xd8\xaf\xd9\x8a\xd8\xab\xd8\xa9 \xd8\xa7\xd9\x84\xd8\xaa\xd9\x8a \xd8\xaa\xd8\xb3\xd8\xb9\xd9\x89 \xd9\x84\xd8\xaa\xd8\xa8\xd8\xb1\xd9\x8a\xd8\xb1 \xd9\x87\xd8\xb0\xd9\x87 \xd8\xa7\xd9\x84\xd9\x85\xd9\x85\xd8\xa7\xd8\xb1\xd8\xb3\xd8\xa9 \xd8\xa7\xd9\x84\xd8\xaa\xd9\x82\xd9\x84\xd9\x8a\xd8\xaf\xd9\x8a\xd8\xa9\xd8\x8c \xd9\x85\xd8\xab\xd9\x84 \xd8\xb6\xd8\xb9\xd9\x81 \xd8\xa3\xd9\x86\xd8\xb8\xd9\x85\xd8\xa9 \xd8\xa7\xd9\x84\xd9\x85\xd9\x86\xd8\xa7\xd8\xb9\xd8\xa9 \xd9\x84\xd8\xaf\xd9\x89 \xd8\xa7\xd9\x84\xd8\xa3\xd8\xb7\xd9\x81\xd8\xa7\xd9\x84\xd8\x8c \xd9\x88\xd8\xa8\xd8\xaf\xd8\xa1 \xd8\xa7\xd9\x84\xd8\xb1\xd8\xb6\xd8\xa7\xd8\xb9\xd8\xa9 \xd8\xa7\xd9\x84\xd8\xb7\xd8\xa8\xd9\x8a\xd8\xb9\xd9\x8a\xd8\xa9 \xd8\xaf\xd9\x88\xd9\x86 \xd8\xb9\xd9\x88\xd8\xa7\xd8\xa6\xd9\x82\xd8\x8c \xd9\x88\xd8\xa7\xd9\x84\xd8\xad\xd8\xa7\xd8\xac\xd8\xa9 \xd8\xa5\xd9\x84\xd9\x89 \xd9\x88\xd9\x82\xd8\xaa \xd9\x84\xd9\x84\xd8\xaa\xd8\xb1\xd8\xa7\xd8\xa8\xd8\xb7 \xd8\xa8\xd9\x8a\xd9\x86 \xd8\xa7\xd9\x84\xd9\x88\xd8\xa7\xd9\x84\xd8\xaf\xd9\x8a\xd9\x86 \xd9\x88\xd8\xa7\xd9\x84\xd8\xb7\xd9\x81\xd9\x84.'}
b'gs://sultan-t5x/arabic_text/ArWiki01.txt'
[RestoreCheckpointConfig(path='gs://sultan-t5x/arabict5_tinyy/', mode='latest', assignment_map=None, strict=True, fallback_to_scratch=False, dtype='float32', restore_dataset=False, checkpointer_cls=<class 't5x.checkpoints.Checkpointer'>, state_transformation_fns=[], checkpoint_manager_cls=<class 't5x.checkpoints.OrbaxCheckpointManagerInterface'>)]
Traceback (most recent call last):
  File "/home/big35manf/.local/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py", line 2531, in _cached_compilation
    xla_executable = compiler.compile_or_get_cached(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/big35manf/.local/lib/python3.11/site-packages/jax/_src/compiler.py", line 294, in compile_or_get_cached
    return backend_compile(backend, computation, compile_options,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/big35manf/.local/lib/python3.11/site-packages/jax/_src/profiler.py", line 314, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/big35manf/.local/lib/python3.11/site-packages/jax/_src/compiler.py", line 256, in backend_compile
    return backend.compile(built_c, compile_options=options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/big35manf/t5x/t5x/train.py", line 957, in <module>
    config_utils.run(main)
  File "/home/big35manf/t5x/t5x/config_utils.py", line 214, in run
    gin_utils.run(main)
  File "/home/big35manf/t5x/t5x/gin_utils.py", line 129, in run
    app.run(
  File "/home/big35manf/.local/lib/python3.11/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/big35manf/.local/lib/python3.11/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
             ^^^^^^^^^^
  File "/home/big35manf/t5x/t5x/train.py", line 891, in main
    _main(argv)
  File "/home/big35manf/t5x/t5x/train.py", line 952, in _main
    train_using_gin()
  File "/home/big35manf/.local/lib/python3.11/site-packages/gin/config.py", line 1582, in gin_wrapper
    return fn(*new_args, **new_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/big35manf/t5x/t5x/train.py", line 644, in train
    trainer.compile_train(dummy_batch)
  File "/home/big35manf/t5x/t5x/trainer.py", line 551, in compile_train
    self._compiled_train_step = self._partitioner.compile(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/big35manf/t5x/t5x/partitioning.py", line 998, in compile
    return partitioned_fn.lower(*args).compile()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/big35manf/.local/lib/python3.11/site-packages/jax/_src/stages.py", line 594, in compile
    self._lowering.compile(**kw),  # pytype: disable=wrong-keyword-args
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/big35manf/.local/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py", line 2276, in compile
    executable = UnloadedMeshExecutable.from_hlo(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/big35manf/.local/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py", line 2624, in from_hlo
    xla_executable, compile_options = _cached_compilation(
                                      ^^^^^^^^^^^^^^^^^^^^
  File "/home/big35manf/.local/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py", line 2528, in _cached_compilation
    with dispatch.log_elapsed_time(
  File "/usr/lib/python3.11/contextlib.py", line 141, in __exit__
    def __exit__(self, typ, value, traceback):

KeyboardInterrupt
Exception ignored in: <module 'threading' from '/usr/lib/python3.11/threading.py'>
Traceback (most recent call last):
  File "/usr/lib/python3.11/threading.py", line 1553, in _shutdown
    atexit_call()
  File "/usr/lib/python3.11/concurrent/futures/thread.py", line 31, in _python_exit
    t.join()
  File "/usr/lib/python3.11/threading.py", line 1112, in join
    self._wait_for_tstate_lock()
  File "/usr/lib/python3.11/threading.py", line 1132, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt: 
